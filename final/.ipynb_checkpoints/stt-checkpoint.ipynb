{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wxUbGCcSPcLj",
    "outputId": "db540a6f-6cce-425a-8602-160f6bbc59aa"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting openai-whisper\n",
      "  Downloading openai-whisper-20240930.tar.gz (800 kB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m800.5/800.5 kB\u001b[0m \u001b[31m25.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (0.60.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.0.2)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (2.6.0+cu124)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (4.67.1)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (10.7.0)\n",
      "Collecting tiktoken (from openai-whisper)\n",
      "  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from openai-whisper) (3.2.0)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba->openai-whisper) (0.43.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2024.11.6)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.11/dist-packages (from tiktoken->openai-whisper) (2.32.3)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.18.0)\n",
      "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (4.13.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (3.1.6)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2025.3.2)\n",
      "Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->openai-whisper)\n",
      "  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->openai-whisper)\n",
      "  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->openai-whisper)\n",
      "  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->openai-whisper)\n",
      "  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->openai-whisper)\n",
      "  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->openai-whisper)\n",
      "  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-curand-cu12==10.3.5.147 (from torch->openai-whisper)\n",
      "  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->openai-whisper)\n",
      "  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->openai-whisper)\n",
      "  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (0.6.2)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (2.21.5)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (12.4.127)\n",
      "Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->openai-whisper)\n",
      "  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->openai-whisper) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->openai-whisper) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.1.31)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->openai-whisper) (3.0.2)\n",
      "Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m119.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m91.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m60.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m14.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m64.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hBuilding wheels for collected packages: openai-whisper\n",
      "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for openai-whisper: filename=openai_whisper-20240930-py3-none-any.whl size=803404 sha256=241615e4f7f40ebee23e6729d2a65c0d8556d114346b38f6034d3e4d67d91c92\n",
      "  Stored in directory: /root/.cache/pip/wheels/2f/f2/ce/6eb23db4091d026238ce76703bd66da60b969d70bcc81d5d3a\n",
      "Successfully built openai-whisper\n",
      "Installing collected packages: nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, openai-whisper\n",
      "  Attempting uninstall: nvidia-nvjitlink-cu12\n",
      "    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
      "    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-curand-cu12\n",
      "    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
      "    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
      "      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
      "  Attempting uninstall: nvidia-cufft-cu12\n",
      "    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
      "    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
      "      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
      "  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
      "    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
      "    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
      "    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
      "    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
      "      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
      "  Attempting uninstall: nvidia-cublas-cu12\n",
      "    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
      "    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
      "      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
      "  Attempting uninstall: nvidia-cusparse-cu12\n",
      "    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
      "    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
      "      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
      "  Attempting uninstall: nvidia-cudnn-cu12\n",
      "    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
      "    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
      "      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
      "  Attempting uninstall: nvidia-cusolver-cu12\n",
      "    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
      "    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
      "      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
      "Successfully installed nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 openai-whisper-20240930 tiktoken-0.9.0\n",
      "Collecting ffmpeg\n",
      "  Downloading ffmpeg-1.4.tar.gz (5.1 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "Building wheels for collected packages: ffmpeg\n",
      "  Building wheel for ffmpeg (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Created wheel for ffmpeg: filename=ffmpeg-1.4-py3-none-any.whl size=6083 sha256=a3c25245e0846f3c8764fb7e45aa9264f2b497f11071dd37a9cc4322173d3506\n",
      "  Stored in directory: /root/.cache/pip/wheels/56/30/c5/576bdd729f3bc062d62a551be7fefd6ed2f761901568171e4e\n",
      "Successfully built ffmpeg\n",
      "Installing collected packages: ffmpeg\n",
      "Successfully installed ffmpeg-1.4\n"
     ]
    }
   ],
   "source": [
    "!pip install openai-whisper\n",
    "!pip install ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "u4qXzNv8r-v6",
    "outputId": "8917de1f-73b0-401e-e6fa-88b1a0020731"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 461M/461M [00:05<00:00, 91.0MiB/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 멜로기 예를 들어서는 어떤 형태들을 대하여 숫자 각도의 선형적인 패턴을 그대로 전달합니다. 그 양수 패턴으로 가니가 순주로 뭘 할 필요가 있어요? 없죠. 이것처럼 내가 사용하려고 하는 형태들에 대한 연결을... 내가 사용하려고 하는 형태들은 사실 값을 전달하고 그 값을 내보내는 건데 어미가 벗어나 버렸다면 외복되고 정리하자면 이런 거 없죠. 우리가 값을 하나 전달한다고 했었을 때... 통로가 하나 있는 거야. 이런 간격에 대한 통로가. 여기서 내가 물을 이 정도 높이로 내보내 줬대. 그러면 물의 값은 변화야 안 되나요? 물에... 그대로 가겠죠. 근데 제가 말하기에 물로 높이를 이 높이 높이로서 값을 줬다가 값은 거. 그러면 어떤 무게를 부러질까요? 오버한 만큼 흘러요. 이만큼이 벗어나서 통로로 가겠죠. 여기 땜이 아니에요. 땜 말고 통로로 가고 있어요. 이만큼이 막혀서 데이터가 원고내 형상과 만들어진 형상은 왜요? 외복된 형태가 발생을 알아야겠죠. 외복된 형태 때문에 사실상 다른 이상적인 결과의 출력이 변형돼 수록이 있다고 말하고 싶겠죠. 이것처럼 표기가 놓친 값은 내가 어떤 지표에 대한 여기 값을 전달했느냐 또는 어떻게 사용하는 데에 따라서 범인이 달라질 수 있잖아. 그렇지만 아예 첫 번째 예측으로 도출한 결과 값은 정답에 가까울 가능성이 높을까요? 뭘 가능성으로? 처음. 그렇죠. 확실히 아예 안 된 상태에서 아무리 초기 값을 잘 전달하더라도 뭘 가능성으로? 그러면 일반적으로 초기 출력을 로스가 크게 잡힙니다. 이렇게 크게 잡힙니다. 형태로서 넴풀들을 저기에서 가죽지들도 조금씩 골varen 방향으로 조정하고 로스를 감소하는 방향성을 계속해서 반복한다고 하는 점이 하자면 오차발생 수정. 오차발생 수정. 이걸 가지고 동작을 수기겠죠. 초반에는 별량이 커지잖아요. 커요. 로스가 클 것인 것 같아요. 그럼 그 다음 부터는 점점 완춘되고 중돈 되기 때문에 인상적인 정답을 찾을 가능성이 높겠죠. 그래서 충분한 횟수만큼 반복이 되었다고 한다면 이 로스에 대한 내용들에 최소화가 없어 가죽지 값을 산출할 수 있게 해서 이 최소화된 로스에는 애투어크에 대한 내용에서 결과, 나과는 타겟에 대해서 가장 가까운 출력을 만들어 내는 모델들을 이야기합니다. 동의하자면 밋런닝의 특징은 침을 거쳐 나가면서 점진적으로 어? 복잡한 표현이 만들어진다는 것과 이러한 점진적인 중간 표현이 공동으로 확술이 된다는 것 같아요. 이 각 계층의 의미들과 그 계층이 거쳐 나가면서 응집된 주요 기처가 도출되는 것으로 각 침의 상위층과 상위층의 표현에 대한 내용들로 명성을 만들어야 됩니다. 바로 이 두 가지의 특성에 대한 내용들이 조합해서 멋있는 어닝이 적극을 보다 인어닝이 훨씬 더 좋은 결과를 약식이 되었다고 표현을 드리겠습니다. 이 특징들로 이 신경 말을 모르지는 학습이라는 기조들을 더욱 더하게 이야기할 수 있는 거예요. 여기서 학습이라는 걸 이용해 하려면 그게 세 가지에 대한 내용의 정리를 가지고 있어야 되겠다. 첫 번째, 룩, 두 번째, 비만, 세 번째, 퀘셩, 결과에 대한 내용들은 소기가 준치라고도 말할 수 있지만 아웃북에 대한 특성들이 소기가 준지 값을 상정할 때도 액티벤션 홍전으로 경경되게 돼요. 공도도를 이야기할 수 있겠죠. 그래서 김런희 계락습에서 왜 액티벤션이 중요하느냐 라고 한다면 퀴드층에서 어떻게 가스를 전달하기만 하면 되는 거지만 출력층은 뭐야? 출력층에서 액티벤션 홍전이라고. 뭐라 했죠? 분류리. 뭐를 아시지? 내가 분류리 하는 일대로 들 건데 액티벤션 홍전은 회기를 만들 거예요. 그럼 제대로 롯스를 계산할 수 있죠? 없어요. 없겠죠? 얘 또한 일종의 아웃북은 결정되는 게 없다고 말하고 있을게요. 그래서 각각 정리해서 이야기하면 롯스 펌션은 모델이 식간 값과 실제 값 차이의 옷차를 측정하는 거야? 롯스 펌션 하는 거야? 이런 옷차가 옷차 나가는 방법으로는 옷차 역적발하고 있어야 합니다. 이 옷차 역적발을 담고 있는 펌션은 무슨 펌션 해보자? 옥기마이저 펌션. 제작한 거. 그리고 음원과의 개치마 개치마 모델과 모델에 대한 의미예요. 아웃북을 만들어놓은 녀석은 백페이저 펌션. 렌드팀은 인역시로와 총합을 출력시도로 변화하는 암부들을 지치간다고 표현할 수 있습니다. 이런 조친들에 따라서 저희가 추가적인 내용들로 농어접일 측면들을 정리해 보도록 할 거예요. 멋있는 녀석에 대한 내용들의 학습 기조들에서 정리했던 내용들에 이제 몇 가지 부조들의 학습 분을 이야기해 보도록 할 거예요. 뮤비 때 이 때의 내용들과 취약한 내용들이 있기 때문에 이 부분을 조금 간단하게 좀 유행하게 하겠습니다. 김너니는 무중기반에 왔습니다. 그렇죠. 심리반. 개신적인 표현대로 학습하는 부조다. 개친이 무선 되지 않는다고 하면 내가 뭔지 알았는 걸 보셔를 수 있다 없다. 없게 되겠죠. 바로 이 개친적 녀석. 우리가 멋있는 녀석의 표적이 개친기의 모델이 개친적으로 그 문제의 정답을 찾겠다고 하는 모델. 트리. 트리기혈. 트리기혈이 무슨 기회를 모델이? 모수, 비모수, 세미모수 기혈이? 비모수. 그렇죠. 비모수 기혈. 그래요. 특성을 가지고는 물론 되지만 개친적인 특성을 가지고 있죠. 그래서 모델만 싹 보고 보면 얘가 무슨 모델이다? 모수, 기혈, 모델이다. 하지만 개친적인 특성 때문에 무슨 답도도 가질까요? 비모수기혈을 특성도 가죠. 그래서 세미모수 특성까지 갖는다라고 표현을 하는 거고 그렇게 볼 수 있습니다. 그런데 이걸 왜 모수기혈이라고 부르냐면 학습과 과정상에는 세미모수 특성은 같겠지만 결론적으로 만들어진 모델은 뭘까요? 그냥 수학 공식 하나 끝이겠죠. 거기에 데이터는 결과가 바로 나오겠죠. 추가적인 연산에 대한 무슨 부분들을 이야기하는 건 다르다는 거죠. 사실있다. 입너리에 대한 여러분들께서 우리가 여러 가지 연속층을 가지고 있고 거기서 이 트레이닝에 대한 분들을 자동으로 하고 싶을 동작들을 이야기하는 게 아니라 사실 머신너리에 대한 접근하고 있는 한 개 또는 두 개의 데이터 표현칙을 학습시키는 경험성도 있어서 일종의 여러 부칙성을 한 번에 담는다고 보시면 아니잖아요. 그래서 머신너리 뇌의 이 학습 논리를 무슨 학습이라고 하는가? 야퇴낙스. 맞아요. 야퇴낙스를. 그래서 머신너리 뇌는 단축레이어미라고 이야기하시죠. 입게 사봐도 한 개의 친구 외친 분들로 덤밀 때 수밖에 없습니다. 입게 사봐도 한 개의 친구 외친 분들을 덤밀 때 수밖에 없습니다. 그래서 머신너리 뇌는 이 계층이 훨씬 더 깊게 잡혀있는 식경망이라고 말해요. 훨씬 더 깊게 때문에 디미라는 상태들에 대한 분들을 사용하는 것 같습니다. 이에 따라 입런닝의 학습은 기술적으로 데이터 표현을 학습하기 위한 단 한 개에 대한 대응에 처리란식을 이야기하고 그 가중치면, 패턴의 결과는 노출하고 노출한 결과에 대응되는 것을 이용해서 오차를 갱신한데 반복이라는 연산을 수행하는 거예요. 이 단순한 과정 그 두 가지의 반복이 뭐를 만들어집니다. 최소화제 손실을 내는 네트로크 타겟이 가장 가까운 출력을 만들어야 하는 모델을 형성해버리는 거예요. 사실 동작 알고리즘을 하셔도 됩니다. 학습 알고리즘을 놓고 보면 도구가 어려워요. 복잡해요. 아니요. 심플하게 말하면 예측감 만들고 오차 계산하고 그 다음 줄이는 방역으로. 갱신하고. 이걸 반복하는 분이 뭔가 굉장히 심오한 알고리즘을 아니요. 단순한 그러지만 그 이상적인 정량을 찾는 기법 그리고 모술적인 특성 이거를 가집으로 인해서 이 간단한 메타인집이 확장되면서 마술 같은 결과를 가져가야 합니다. 사실 한 개만 놓고 모자라고 한다면 크게 영향성과 이펙트는 없어요. 우리가 리뉴얼 유리에 대한 리니어 리얼모델들을 경상하고 썼죠. 기억하시잖아요. 근데 얘네들 한 형제들에 대한 유화전도 크게 바뀌는 게 있었어요. 리뉴얼 모델이요. 차이가 없었죠. 최소적인 법법이나. 그런데 이게 침이 깊어졌고 그러에 대해서 여러 정보들도 출례해요. 주요한 정답에 대하여 도달하기 위한 기법으로 정답을 도달하기 위한 이상적인 피쳐도출 이라는 작업들이 일어나면 하고 있는 훨씬 더 좋은 결과를 만들어냈다는 게 김너닝의 확실이라는 거예요. 정리하자면 김너닝의 특징은요. 침을 거치면서 점진적으로 더 복잡한 표현이 만들어진다. 이게 바로 뭐를 도출한다? 압축된 주요 피처를 도출한다. 바로 특징 추출에 계단한다. 라고 볼 수 있겠죠. 그러면 주요한 의미의 특징들이 만들어졌기 때문에 여기서 착각하면 안 되는 게 어? 왜 저 차원 근처가 단순해지니까 선으로 부를 수 있다면서요. 이거는 피처에 대한 정리로 그 피처 하나가 무슨 의미를 가셨던지 좀 더 복잡한 압축된 의미를 가셨다는 말이죠. 피처에 개수가 단순해지서 차원 공간이 막 100차원이었던 4차원, 3차원으로 경영됐다. 이렇게 이해하시면 되겠죠. 그러면 차원 개념에서는 단순해질 거지만 피처 개념에서 해도 돼요. 복잡해졌어요. 복잡해졌어요. 맞죠? 내용이 더 많아졌다고 하셨다면서요. 우리는 쩌만하라고 하지만 쩌미, 쩌미이 아니라 뭔가 압축된, 복잡한 주요 의미가 만들어졌다. 라고 볼 수 있겠죠. 그래서 복잡한 표현이 만들어진다는 것 그리고 정진적인 중간 표현에서 공동으로 학습되는 특성들을 기대하게 합니다. 사실 머신헌이의 최근 도연은 트렌드가 좀 바뀌고는 있지만 기본적인 트렌드가 변향성을 뛰었어요. 토끼의 제일 표적인 게 뭐야. 사이큘런드를 많이 쳤죠. 오늘 날도 많이 써요. 전체적으로도 많이 쓰고요. 그리고 거기에 대응되어 고추로서 이제 누구도 펜서프로로 역할하는 기본적인 프레임워크들을 많이 쳤어요. 이 시기 때가 약간 과록이 더 비시기 때도 있고 여러 가지 텐션제도 있는데 그다음 이제 성능적인 축도로서 XG고추로 좋더라 하면서 빙런이랑 싸우기 시작했었어요. 그때도 그러면서 캐나스라는 구조들을 진짜 알아봅니다. 캐나스가 뭐냐 하면 쉽게 코드를 짤 수 있는 구조래요. 그래서 옛날에는 저도 이제 교육을 할 때 교육 목적으로 많이 고르는 캐나스를 크게 통하지 않았어요. 그런데 얘가 버전이 3점 때로 올라가면서 호할성이 좋아지고 안정성이 좋아졌고 확당성이 좋아졌어요. 그래서 얘로 베이직한 구조가 만들었 다음에 나머지 것들로 저수준으로 연동하는 것들 뒷버트를 많이 확장시켰습니다. 정리하자면 저수준 ABI 텐션플로디아 캐라스가 텐션플로 특성의 보수준 ABI 라고 생각하시면 그럼 누가 만들기 지울까요? 캐라스 그런데 누굴 깔리만 하면 될까요? 텐션플로 텐션플로 안에 캐라스가 있다고 합니다. 원래는 옛날에 독립적이었는데 얘가 텐션플로 들어오면서 훨씬 더 좋아졌고 진지한 오로지 텐션플로만 쓸 수 있는 사실 텐션플로 봐요. 지금 2.0대거든요. 텐션플로가 1.0대에서 갑자기 2.0대로 바꾼 일 있어요. 이게 텐션플로 원래 기존 개발자들이 굉장히 빡쳤던 순간이에요. 우는 말이야 코드 빛이 완전히 달라져서 그래서 펌션 무조건 달라지고 이론이 달라지니까 새로운 구해야면 문제가 발생할 수 있는 이유가 있어요. 왜 그런가 천창기의 텐션플로디아 굉장히 푸르명적으로 코드가 되게 편해 어려워. 어려워 됐죠. 그러다 보니까 조금 원래 컴퓨터가 없어서 사람들이 입장에서는 그냥 쓰더라고요. 상관없어요. 근데 제가 이제 김동기 반등이 시작한 시기가 대표 쪽으로 원했어요. 비전에서 사진 대회 뭔가 다 다 뜯으리면서 대박에 사셨다 했잖아. 비전기어를 정보하는 사람들이 원래 컴퓨터에 정보하고 소프트웨어 그 사람들이 쓰려고 그러니까 텐상포보다 외화해 보다가 불편한 거예요. 그러다 그때 누가 된다냐면 파이터치 이 파이터치가 쓰기 편한 거예요. 그러니까 원래 처음으로 입혀야 되는 사람 있잖아요. 저는 그냥 파이터치 메소드 입히고 그냥 쓰기 편하겠죠. 그러니까 대고 텐상포로서 사람들이 또는 생친류 모이는 사람들이 다 파이터치고 하는 거예요. 그러니까 자기들도 없잖아요. 파이터치 전쟁 상이었어요. 그때도 살아가는 게 텐상포로서였는데 왜냐하면 에베스트에도 있고 카페도 있고 여러 가지 프레임이 많았어요. 지금도 태양하고 있고 정리세과로 있는데 그 상태에서 파이터치가 쓰니까 굉장히 좋죠. 그런데 여기 기억해야 될 거 누가 많이 좋아야 되죠. 어떤 연구자들? 뉴비 뉴비 그리고 그 뉴비 어떤 데니까 다른 사람 비전 맞아요. 파이터치의 프레임한크 패턴 뭐가 많냐면 비전 계열의 개인집 고대들이 많이 설계되어 있어요. 왜? 비전하는 사람들이 쓰기 때문에 그쪽 계열에서 뭔가 문제를 표현을 만들어 내겠다 라고 한다면 여러분 회사에 들어가서 내가 비전에 관련된 업무를 처리해야 된다 그러면 파이터치 있고 관련된 말하고 거의도 잘 만들어드리죠. 예열이 네 자 이 텐스터 또 나는 상태들을 원래 많이 쓰고 있었다. 했죠. 그래서 얘들은 일종의 뭐든이 말하느냐 원래 많이 쓰던 계열들은 그대로 가는데 그 베이직한 기념들은 텐스플루도 훨씬 더 많이 가지고 있어요. 게다가 어머머 친형말리한데 아래는 계열들은 아직까지 여전히 이슈가 없었잖아요. 그래서 텐스플루 쪽으로 많이 고정룡도 만들어져 있습니다. 지금 말하자면 우리 인포대에서 이제 하시면 자 그래도 똑같이 쓰는데 이 파이토치 또 한 번 갈까요? 저수준의 API 저수준의 API니까 뭐겠어요? 기계랑 더 가깝다. 그래 기계랑 가깝다라기보다는 그냥 프레이버프 관점에서 놓고 보면 오수준 보다는 조금 어렵다. 요런 생각도 해주시겠네요. 근데 지금 철이 속도로 놓고 누가 더 좋을까요? 오수준이죠. 저수준이죠. 저수준이죠. 네. 저수준 제가 굉장히 디테일하게 다루겠다 라고 얘기하자면 보� funktion�버ắn paper 이런 계열쪽에서 회사에 들어가면 자기들이 파이토치에 대한 용돌로 연구했다고 하더라도 회사 내에서는 파이토치 라이트는 사용하는 게 많아요. 또는 라이트 계열들. 해석으로도 메인의 컴퓨터들 쓸 때는 굉장히 메인이죠. 그런데 이메일이나 시스템 쓸 때도 돼요. 모델이 무거워면서 제대로 돌아가야 안 들어가요. 그렇죠. 이거도 라이트라는 의미는 문제를 합니다. 그런 것들처럼 닷담의 역할들과 부붐들이 있었다 정도로만 기억하시면 되겠어요. 그리고 지금 현재 아마존이라는 형태들의 기막으로 구만물소절을 공작하는 애들은 뭘 많이 썼느냐. 바로 mx나시라는 걸 많이 썼죠. 얘가 아마존 개발자들이 만든 프레이머크들인데 이것들도 한번 국내에도 좀 커지려고 하다가 지금 이제 변선사가 좀 빨라서 여전히 펜섭롤과 파이토치가 거의 1순위다. 이런 생각을 하시면 돼요. 그럼 이 둘의 큰 틀은 차이가 있다 없다. 없죠. 펜섭롤이 똑같습니다. 제가 말할까? 뭐가? 펜섭롤과 1점, 1점. 갑자기 뭘로 바꿨다? 2점 대로. 자 이런 원리들에 따라서 이 머신허링이라는 형태들은 사실상 각각의 연도에 따라서 업계 들에서 사용되는 거에 대표적인 형태들은 이제 딥러닝과 그레아디언, 부스틱, 이 두 가지들로 굉장히 치열한 전쟁이었다 라고 알고 있어요. 그런 형태들에서 각각 이제 서로 장단점에 대한 용들을 정립하고 이제 역할성을 나타내기 시작했다. 라고 볼 수 있고요. 그래서 대부분 빈간을 사용하는 페란스와 페란스의 상을 프레임워크의 펜섭롤을 많이 활용해서 활용하고 있습니다. 그래서 오늘 머신허링에 대한 용들은 두 가지 기술에 대한 용들을 인지하는 게 중요한데요. 대표적으로 무순 문들, 야스 락스에 대한 개념과 이어서 그라기언, 부스틱 머슨에 대한 용들의 지각 관련된 문제들을 인지하고 거기를 위해서 만든 딥러닝. 성계되는 기술을 이해하면 됩니다. 그래서 우리가 여러분들 머신허링 프로젝트 해봤잖아요. 그래서 여기 페란스가 어때요? 그라데이션 부스틱 기업을 했었죠. XG부스트도 이제 브레드 부스틱이고 라이트 GBM도 라이트 그라템드 부스틱이잖아요. 그래서 부스틱 개념에 대한 용들을 많이 활용하거나, 그리고 직접적으로 모델을 통해서 아웃콧을 만들어내고 거기서 모델을 공격하는 스테이킹 알보리지면에 대한 용들도 병행하는 거죠. 포워드 신경망은 여러 가지의 내용들을 조합하여 만들어지는 유런 문구들을 이야기합니다. 이 개침적인 구조들에 따라서 포니코넛 레이어로 연결이 되는 것이 이걸로다. 여기서 여러분 포워드 신경망이라고 이야기하면 일반적으로 고를 떠올려야 되죠? 앞으로. 완전히. 우리가 이런 주요 의미를 도출하기 위해서 무슨 전사는 합니다? 가중 합사 전사는 합니다. 피처들 더해서 제대를 도출하고 거기에 대응되는 결과감들을 선택합니다. 이 결과에 대한 용들을 해서 가중시는 특징을 추출할 때 영향이 큰 베이터를 선택하는 역할이다 라고 그럴 수 있어요. 내가 입력한 걸 모두 쓴다고 해서 당황에 따라서는 X예요. 그러면 이 쓸건지 안 쓸건지 아까 말했다 싶지 임너링의 장점이 뭐죠? 주요 피처를 도출하는 게 장점이잖아요. 이 주요 피처를 어떻게 도출하는지 바로 이 가중 합사는 또 도출하는 거예요. 그래서 피처들을 간의 관계성과 그 관계성에서 어떤 피처를 더 중요한지 판단하는 관점의 동작이라는 거죠. 이런 판단에 따라서 특징 추출에 가장 영향이 큰 베이터들의 가중지는 크게가 영향이 작은 데이터는 작은 날지치를 갖는다 라는 동작의 의미를 표현합니다. 사실상 이런 변화성들을 위해서 비선형 변화를 통해서 특징을 추출하는 액티베이션 펑셔람툴들이 필수적으로 필요한데요. 이 뉴런에서 사용하는 액티베이션 펑셔를 종류는 굉장히 다양합니다. 그중에 기본적으로 사용되는 액티베이션 펑셔리라고 할 수 있는 것이 렐루입니다. 이 렐루 계열들은 대표적으로 어느 데 있던 특성에서 굉장히 적값도가 높으냐 이미지 계열의 특성들의 적값도가 높아요. 일반적인 모델에 대해서도 적값도가 높습니다. 그래서 이 렐루라는 형태들에 대한 형태들은 일종의 뭐라 표현하냐 영보다 작은 영을 출력하는 구간선형왕수다 라고 이야기합니다. 구간선형왕수에 따라 우리가 직접적으로 동작하는 것은 뉴런 하나만 이야기하는 것이 아니라 뉴런의 집합과 전체적인 영역성을 이야기해서 나타내기 때문에 이제부터는 시람수와 백터왕수를 부분해야 합니다. 시람수라는 이 렐루의 펑셔네도 노출되는 형태의 왕수로서 입력의 크기가 극정 백터의 인값과 출력이 실수로 나타내는 형태들로 정의된 하나의 값을 시람수라고 표현합니다. 그러면 백터왕수라는 백터왕수는 일종의 입력의 크기가 일정 백터에 대한 내용들이고 출력 후원을 백토로 나온 구조들을 백터왕수라고 이야기하는 거예요. 그러면 이에 따라 내가 지금 조기의 입력에 대한 역의 백터들로 종단에 대한 실함수라고 한다면 일반적으로 어떤 형태들에 대한 역값들을 많이 이야기하느냐면 대상의 정답, 노속값의 결과들을 이야기할 때 이러한 실함수의 키워드들을 많이 이야기합니다. 정리하자면 유런은 실함수로 나타납니다. 그래서 유런을 여러개가 들어가면 뭐가 나와야 하나가 나오잖아요. 유런의 입력은 어때요? 여러개가 입력된 거잖아요. 지금 백터가 입력이 될 거에요. 그 백터 입력끼라서 뭐가 나올 거예요. 배류 하나가 은장을 하겠죠. 그리고 이걸 이제 개치음적으로 접근하면 이거 어때요? 아까 제가 유런 하나로 갔잖아요. 유런 하나는 실함주잖아요. 그럼 개치음. 개치음으로. 유런 하나가. 백터로 들어가서 하나로 남아요. 그래서 유런은 실함주에요. 그래서 가정치대들이 여러개 전달되었을 때 이 가정학살 통해서 미성형 활성함주로 매팅을 해서 실수를 출력합니다. 이 실함수들의 조화도 백터한수를 만들어 내는 거예요. 이게 개칭의 형성이에요. 정리하자면 뭐를 뽑는다. 백터를 입력해서 스칼락과 하나 등장하는 녀석은 실함주. 그게 유런이죠. 유런이. 요번에 대한 조합이 된 걸 개칭이라고 그러죠. 개칭의 인풋은 뭐죠? 유런 하나가 입력백터와 똑같잖아요. 유런의 입력이 뭐죠? 백터였죠. 그 백터가 입력해서 유런마다 배류가 나올 거잖아요. 그 개칭의 아웃풋은 뭐예요? 백터가 됐죠. 그래서 백터한수라고 합니다. 백터한수로서 간의 게 개칭이에요. 그래서 개칭은 입력도 백터고 출력도 백터인 백터한수에요. 그래서 개칭으로 이룰 때 각 유런은 이전 개칭의 출력을 백터 넘게 인정받고 거기에 따라서 각 유런의 출력들을 모아서 백터 넘게 출력합니다. 그래서 우리가 개칭다디를 묶을 때 제가 아까 말했죠. 유런이야기한테 원래 입력층, 퀴즈층, 출력층을 이야기하자고 했지만 계산의 한점에서는 우리가 어떻게 봐야 되겠죠? 입력과 출력을 한 세트로 가야 됩니다라고 했죠. 네. 백터 연산까지고 그래서 이때 입력 백터의 크기는 이전 개칭의 유��수와 같고 출력 백터의 크기는 현재 개칭의 유��수와 같다. 이 특징을 기억하셔야 되고. 그런 생각이 없어요. 제가 데이터를 입력받을 거예요. 첫 입력이죠. 입력층의 데이터 인스턴스가 퀴처로 15개 가지고 있다고 하자는 거죠. 그러면 제가 입력층에 만들어야 될 유언에 계신 몇 개예요? 자, 천천히 정리할게요. 데이터를 입력할 거예요. 인스턴스를 입력하겠죠. 그 인스턴스를 가지고 있는 퀴처가 몇 개다? 15개다 하고 가정할게요. 네. 그러면 내가 이렇게 모델을 현명할 거잖아요. 그럼 이 모델에서 내가 입력을 받아야 되잖아요. 그러면 저는 몇 개의 유언을 만들어줘야 될 거예요. 맞아요. 15개. 제가 말했죠. 입력층은 뭘 다? 입력앞이 그대로 입력가서 나와야 된다고 했잖아요. 그래서 입력은 이전 개칭의 입력, 즉 퀴처의 개수가 곧 입력에 대한 극복하는 값이겠죠. 거기 대비 내가 너를 줘야 돼요. 가정 받아줘야 되잖아요. 그거만큼. 그대로 받아야 되죠. 입력층입니다. 그대로 받아야 되잖아요. 그러면 유언에게 좀 더 어깨. 15개. 그렇죠. 여기 정리하려면 모델의 입력 유언의 개수들은 입력하는 데이터의 퀴처 개수와 일지하는다. 나는 말하고 좋아해. 괜찮다. 그럼 우리가 앞서 파나나, 파인애플, 하광라는 데이터들을 한번 본 적 있었잖아요. 그 데이터는 데이터 무조건 몇 창원이에요? 몇 창원이었죠? 네. 인스타 스 개수, 가로새로. 맞죠? 그런데 인스타 스 개수를 배지하면 몇 창원 정도예요? 2,000원. 2,000원 구조죠. 그럼 이 2,000원 구조를 그대로 넘는 데이터입니다. 여기서 증명을 드립니다. 우리가 지금 뭘 받아요? 백터 암수의 연사입니다. 백터 암수의 연사는 뭐가 들어와야 돼요? 백터가 들어와야 합니다. 그렇죠. 백터가 들어와야 합니다. 그러면 이 2,000원은 우리의 용어적으로 뭐라고 해요? 2,000원 텐서 또는 매트릭스 라고 해요. 이 매트릭스가 백터예요? 아니잖아요. 백터가 뭐예요? 임창웅 구조잖아요. 그럼 이 걸 뭘로 바꿔줘야 돼요? 백터로 바꿔줘야 되겠죠? 그래서 제가 이제는 뭐의 문자하고 하죠? 백터 암수의 연사는 그 사진들은 몇 개예요? 조기 피처는 만 개입니다. 백터 방정입니다. 그러면 당연히 흰역 유런의 입술은 몇 개예요? 만 개. 그렇죠. 이해가 되죠. 자, 그럼 이어지니까. 얘가 히든층에 유런의 입술가 6개예요. 첫 번째 입력이 유런 몇 개로 정리해서 할까? 우리의 15개죠. 15개 정리했죠? 히든층에 유런은 몇 개였죠? 6개라고 했죠? 그러면 그걸 받자마자 9부수로 나오는 데이터의 피처 개수는 몇 개예요? 아까 15개 넣어서 히든층을 거쳤잖아요. 히든층이 9부수는 새로운 데이터잖아요. 그 데이터로 피처가 몇 개가 있는지. 그렇죠. 바뀌는 거죠. 무거우의 개수로 히든주라는 개수예요. 그래서 히든층에 유런을 받아서 히든층에 유런을 받아서 히든층에 유런을 받아서 히든층에 유런을 받아서 그래서 현재 데이터의 크기는 이전 개친이 유런과 같아요. 근데 이게 이제 출력 데이터의 크기를 바뀌게 되는 거죠. 이 출력 데이터의 크기는 누구로 통해서 됐죠? 현재 바람 고고가 되겠지. 그럼 아까 입력 데이터가 몇 개였어요? 15개였죠? 입력 2번은 15개, 바다 특이기 15개 유진해져. 히든층에 제가 몇 개예요? 6개잖아요. 그러면 15개인 게 거치자마자 몇 개로 바뀐다. 6개짜리 바뀐다. 이 사이클의 눈쪽을 이겨주시나요? 과정상? 그 계층의 아웃풋뿔로 나타내면 되요. 그래서 이전 계층이 만약에 n개고 현재 계층이 n개 라고 하면 계층의 가중시는 경계가 풀리 커넥티로 레이어받고 있잖아요. 그러면 뭘 흔들려요? 어패주면 되요. 바이였스. 웨이트를 이야기하면 가로백덤바 세럼 뱉터를 이야기하는 영상들이 만들어지게 되는 거예요. 이어서 바이였스의 계수를 누구에게 수로 나타납니다. 출력으로 나타내면 되죠. 뱉터람수들의 합성함주가 바로 식량망예. 정리하자면 우리는 스카라에 대한 형들을 뱉터로 정리하고 뱉터를 실행수 연산을 통해서 결과를 도주하고 이 실행수의 결과를 집합으로 뱉터함수를 만들고 그 뱉터함수의 집합이 유럴레토그라. 라고 정리만 해주세요. 이러한 정리만 따라 신경남은 인력과 출력이 뱉터인 뱉터함수이면서 동시에 각 계층을 정리하는 뱉터함수를 순차적으로 실행하는 합정함수로서 정리할 수 있는 거죠. 이 모서리에서 보라돌아, 제 처음에 임동진흥에서 빈런이는 그냥 뭐 만드는 것 뿐이다. 멋있는 일과 똑같아요. 멋있는 일은 공식을 만들기에는 끝이겠죠. 그럼 이 모델 자체가 곧 뭐예요? 그냥 공식 한계라는 거예요. 그래서 Y라는 결과를 만들어내는 공식이지만 이 공식 자체가 어떤가 함수 안에 함수 안에 함수를 거쳐서 이 장갑의 정답을 또출하는 개념을 이야기하나. 이런 특성들에 따라서 정답을 맞추게 되더라면 기본적으로 어떤 특성을 갖게 되냐면 엄용 근사 정리에 대한 영향의 값들을 갖게 돼요. 신경망은 얼마나 많은 방법이 한 함수고 떠날 수 있을까? 라고 한다면 사실 이 신경망은 엔차원 공간의 임의의 연속 함수를 근사하는 그 기업의 문제를 해요. 여기서 말하는 엔차원이라는 건 뭐냐? 데이터가 가지고 있는 일정의 차원이라고 말하고 있습니다. 이 차원에 대한 내용들에서 데이터들을 최대한 쉽게 뱉게 시켜서 완벽한 정답을 찾게 되더라면 이 쪽의 근사치에 대한 정답을 찾을 수 있는 영향이 가지고 있다는 거죠. 그래서 임의의 연속 함수를 근사할 수 있다라는 것은 그림의 형태들처럼 우리가 클래스를 불려야 할 때 클래스 영역 간의 경계가 굉장히 복잡하더라도 연속적인 곡선으로 표현할 수 있게 되고 이어서 오른쪽처럼 인력과 출력과 관계성을 펌션으로 표현할 수 있게 되는 팩이 백단에서 복잡한 예측 곡선이라고 하더라도 크로는 그런 문제가 없다는 거예요. 그래서 대표적으로 지도학습계에서 말하는 문제, 분류와 팩이를 유로렛 도업을 통해서 1만 점 성료식보를 만들어낼 수 있다 라는 특징을 이야기하는 거죠. 정리하자면 요약해서 이렇게 볼 수 있습니다. 첫 번째, 각각 만족해야 되는 것들이세요. 첫 번째, 인력입니다. 인력은 반드시 어떤 형태여야 될까요? 인력이 형태. 백터. 그래야 모델을 넣어주죠. 이거 이렇게 표현할게요. 모델에 넣을 수 있는 각. 그러면 우리가 앞서 베이커 전철이나 뭐라고 했죠? 학습을 하기 위해서 데이터 준비하는 모든 자국들은 전철이라고 이야기했잖아요. 그러면 인력은 어떤 전철의 예고를 찍고도 뭐예요? 넣으려고. 맞아요. 모델에 넣을 수 있도록 계획을 정리해야 한죠. 유로렛 도업도 입장해서 이렇게 베이커 전철이라고 해야 되나요? 무슨 작업하는 거예요? 데이터의 백터. 아, 라고 말해요. 이어서 두 번째, 출력입니다. 출력은 어떤 형태들을 맞춰야 해요? 출력이 형태. 출력의 형태는 뭘 맞춰야 해요? 사용자가 원하는 결과인 것 같아요. 그렇죠. 결과. 우리 막 풀어내려야 할 문제가 뭔지와 똑같은 형태를 만들어야 해요. 내가 이런 확률을 예측해서 볼 것 같아요. 확률로 나오시면 돼요. 내가 여기 연속성의 숫자를 원한다. 연속성으로 나올 수 있도록 누구를 건드려야 해요. 출력이 활성화. 오늘 결정해야 합니다. 출력이 해야 한다는 둘을 잘못 결정하면 우리가 뭘 연산이 불가능하겠죠? 5차가 안 돼요. 5차 계산을 반드시 만들었죠? 그렇기 때문에 잘 지켜드렸어요. 세 번째, 은닛계층으로. 은닛계층에서 만족될 것은 뭐예요? 입값을 전달하는 게 준비 안겠죠? 그런데 여기에는 이번적으로 딥라오님의 생각하는 것은 단층을 생각하는 거예요. 다층을 생각하는 거예요. 다층을 생각하는 거예요? 그렇기 때문에 우리가 반드시 자층이 악수되려면 뭘 만족해야 하는지 했죠? 2분. 그래서 퍼셉트럴의 세 감수를 못 쓴다 했잖아요. 여기서 정보라듯이 은닛계층 활성화 함수가 중요해요. 왜? 무엇 때문에? 뭘을 낳으려 해야 되기 때문에. 미분 가능함수. 그렇죠. 미분 가능한 연속감수로 모델 구조가 정립이 되어야 하는가. 이게 열심히 하는 거예요. 마지막으로 네 번째에 대한 영역의 끝성들을 이야기하도록 할 거예요. 그럼 모델을 만들었고 모델이 학습할 수 있는 구조들에 대한 영역의 끝성들을 다 만족했잖아요. 여기서 모델을 휴닝해 주고 모델의 학습을 잘 될 수 있도록 구조를 바꿔 주겠죠. 이 개념을 기존의 배웠던 개념으로 이야기한다면 모델의 복잡도를 조절해 줄 거예요. 모델의 복잡도가 높으면 어떻다 했죠? 모델의 복잡도가 높다. 잘 맞추는데? 오버피티. 뭐가 맞히는 건가? 지금 높아요. 오버피티. 그렇죠. 과정과 오버피티. 자리랑에 다시 높죠? 네. 그럼 그것도 어른으로도 존재할 수 있는 것 같죠. 자, 두 번째. 오늘 이야기했던 것처럼 네트워크의 깊이가 깊어지면 깊어질수록 어떤 피처를 잘 찾을까 하신 것 없다 했어요. 깊어질수록? 네. 강조되는? 네. 주류 피처를 도출할 수 없다고 했죠. 그러면 약득 모델보다는 깊을수록 좋은 결과를 만들어야 가성됐다는 것이죠. 네. 이 둘 다 실질적으로 데이터에 대한 정보를 향상시키겠죠. 모델의 모델의 조절으로 네트워크의 크기를 조절할 수 있는데 이 크기의 조절은 대표적으로 두 가지 반적으로 구성할 수 있습니다. 하나는 깊이, 하나는 너비. 라고 해야 되죠. 자, 우리가 앞서 출력층에 대한 의미는 왜 이해를 대제하면 나머지 층들의 역할은 없든 역할이라고 해야 되죠. 전철이. 그렇죠. 전철이 역할. 그러면 우리가 앞서 그리운 것 같던 것처럼 첫 번째 예측은 어떤 종보들을 가지고 있었나요? 입력 고유의... 백 더. 자, 이렇게 표현하는 형태들을 이런 게 표현하겠습니다. 저수준 피처를... 저수준 피처를 가짓겠습니다. 단계도로 진행한 게 그냥 압축된 무슨 피처를 도추되요? 고수준 피처를? 그렇죠. 고수준 피처를 도추되게 되죠. 그러면 여기서 깊이는 앞서 말했다시피 얼마나 좀 더 압축할 거라는 관점이었다면 너비의 의미는 뭘까요? 얼마나 필 것이냐. 많은 거. 어느 정도 필 거라. 또 핵심적인 단어어진 두 장점이고요. 차원. 차원을 줄인다. 차원을 줄인다. 오히려 넓으면 늘리는 거 아니야? 피처를 들어가는 거. 유런하게 이렇게 또 가고 있는 건 뭐야. 특징을 도축하는 거. 그래서 제가 아까 개친이 앞단과 뒷단의 의미를 뭐라고 해야 돼? 저수준 피처를... 고수준 피처를... 고수준 피처라고 했죠. 그럼 여기서 너비를 들리느냐 하는 말을 뭘 들리느냐는... 피처를... 늘리겠다. 어떤 관점이? 사람이 보고 수정할 수 있도록? 그렇게 보기에는 김노림 접근 자체가 안 맞으니까 고고다란 다른 말이 같겠죠. 왜냐하면 이거는 너를 해석이 안 되는 거라고. 피처를 늘리는 건가? 여기서 중요한 건... 말했대로 전체적으로 늘린다니까 개친별로 늘려야 할 수 있는 게니까. 그러면 내가 첫 번째 개칭에서 이걸 늘렸어요. 그럼 어떤 의미가 있겠다는 이 부분? 피처를... 어떤 피처를? 이 중의 부분. 피처를 늘리는 건가? 피처를 안 늘리겠다는 거 같은데. 어떤 피처를? 입력이. 첫 번째 개칭은 저수준의 피처를 안 늘리겠다는 거. 얘가 되셨어. 내가 사람을 판단할 거야. 또는 내가 사람을 만들 거야. 사람을 그릴 거야 라고 했을 때. 사람이라는 걸 형성할 때. 얼굴, 다리, 몸. 이게 중요하죠. 근데 거기에. 나는 세부적인 디테일까지 생각하고 싶다고 한다면. 어떤 정도? 저수준의 용보로 늘리니까. 그럼 디테일한 정보가 또 반영하는 거 있죠? 즉 들어오는 양이 늘어난 거예요. 똑같은 데이터가 조금 더 디테일하게 오고 있다라는 거죠. 어느 수준에서? 다섯 개칭의 수준에서. 그러면 내가 주요 피처라고 하는 것들이 압축된 피처를 맞아요. 이 주요 피처를 어떤 만점? 정답을 해결하기 위한 반점의 피처를. 근데 데이터에 대한 정보를 안으로 조절할 수 있다는 거죠. 뭘 통해서? 너비. 정리하자면. 디피는 압축된 피처에 대한 압축 동조를 만들어내기 위해서. 모델의 디피를 쓸 수 있고. 너비는 그 수준의 주요 피처를 도출하겠다는 거의 양을. 늘리겠다는 게 너비의 확장성입니다. 라고 말해 드리겠습니다. 그러면. 디피가 늘어난다면. 8m의 양이 없거든요. 많아서요. 많아서요. 너비가 많아지면. 8m의 양은 없겠다는 거에요. 많아지면. 둘 다 어의관점입니다. 모델의 복작도를 상상시키는 데. 근데. 상상시키지만. 상상시키는 방향의 반점이 같아요. 달라요. 맞아요. 달라요. 디피는. 종단의 주요 피처를 어떻게. 조금 더 압축된 정보를 도출하겠느냐. 라는 만점이 아니었다면. 너비는. 그 수준의 주요 피처를 어느 정도 양으로 확장시킬 거냐. 라는 상태들을. 일단 딱. 나보고 싶은 말입니다. 기원적인 주요 피처를. 네. 이렇게 생각하시면. 여러분들 앞에 노트북이 있잖아요. 노트북에. 노트북이라는 정답을. 맞추고 싶어요. 맞추고 싶다. 할 때. 저 수준의 양으로. 라는 말은. 이. 노트북 크기에. 4각형 있죠. 내가 이거 하면. 피쳐야. 라고 하는 4각형을. 굉장히 작게 작게. 다. 그럼 디테일하고 다 보고 있죠. 네. 그러면. 디피를 널리겠다는 말은. 이 작게보다 모든 화소를. 그냥. 저만으로. 하고. 이 서로의 반점을. 이. 이 서로의 반점을 이해하면. 내가 모델 어떤 반점을. 저. 또. 올려야 되겠다는 걸. 조금 더. 유취해 볼 수 있어요. 자. 그런 반점에 따라서. 우리는. 푸러내는. 빛런이 알고 있는. 표정. 우야지기. 이런. 문제라고. 이야기할 수 있어요. 첫 번째는 무슨. 문제라. 분류. 맞아요. 분류. 분류.성을 하 Augen로 الط 70kel Izley 바디. 일단. 형태로 저의 전혀. Zuskin conferment. 그럼.� dua cinemas. rồi.�아. 얜 muscat. 이거. 따다다다다다다다다speaking. 용서.Qué scattering risciki las universes? 너무. 해�豚ans trabalhar. 패러보. 시키. steel. 힘. 구뜨. 그런데 배로듀 분포와 종규분포평평히 주세요. 아시죠? 네. 그 분포에 따라서 우리는 사건에 따른 배로듀 분포와 이양분포 같은 경우들은 어떤 목적성? 굴류 목적성입니다. 그리고 각각을 어떤 그 목적성의 분포에 맞게 데이터들을 정리받는 게 중요하다고 말해 볼 수 있겠죠. 자, 추가적인 이야기는 우리의 슈화타다 여섯, 다시 진행하도록 하겠습니다. 네. 네. 네. 네. 네. 네. 네. 아, 나 마크 계정 없어졌다. 왜? 마이크로소프트 인수할 때 개정을 안 바꿔가지고 아 진짜? 그래서 나 인수도 이거 나서 샀어 아 이 승리 잔여서 오할렘 뭐였더라? 모장 아 맞아 맞아 마이크... 마이크... 소프트 잘 인수했다 있냐? 어 이름도 비슷하고 그저 기기모... 어... 원래는 또리만 있다가 응 동물 유럽이야 아... 동물 없더라? 맞아 나도 이것이 하던 프로젝트 두 개 다 끝나면 마이크로프트 모드나 만들어 보려고 모드? 어 이겼는데 잘해 나는 아이디... 자바 코딩인데 자바 코딩? 어 모드를 만든다니까 그래서 아이디어가 좀 필요해 그니까 어떻게 할까요? 좋은데요? 아 꼭 이만큼 아 우리 자리 좀 어 어 어 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 아 تو 아 아 어 아 있다면 줘야 네 어? 그게 잡지? 이거? 이거? 이거? 왜 이렇게 못하고... 잘 못 봤는데... 아이고... 안 되겠다... 안 되겠다... 안 되겠다... 어? 안 되겠다... 안 되겠다... 인정 어? 오... 와우... 저... 이거지? 이거 사니까... 자기가 안 되겠다... 차이가 있었나? 차이가... 문화... 문화... 문화... 아... 이렇게... 역시... 내일 빠지는 건가? 역시... 다 하죠? 다 내렸는데... 전 오후에 나가야 돼요 아... 어? 나... 어? 나 연휴 때 먹어야 돼 약 빠져야지 어? 아, 그거... 양퇴 걷어봐. 다. 어? 그러면... 열어봐. 위쪽만. 준비 있는데. 여기 열어봐? 아니, 오른쪽, 오른쪽. 위쪽만. 한 칸만. 못 나오게. 나 잘 해보인 거야? 아니야, 아니야. 한 칸. 이렇게... 그... 아니 아니 한간만 잠깐만 어 아니야 이거 철창도 한간대 봐 잠깐만 그러면 이제 점프해서 가보자 위에 막아야지 어 그치 그런 다음에 포션 포션 일본에 포션 있거든 여기 가정 다음에 이건 나약한의 고기 한국 사람 정보는 맞아 그 다음에 그 다음에 그 떠들려요 보면은 한국 사람 아니 이거 던지 던져 봐 좀 좀비 안 돼 좀비 던지고 학원사가 되고 오클릭 어 누브지거리 치료되고 있어 이렇게 신기하네 이런 게 연질생각이지 어 생일이 아니라 그 마이크 타입 안에서 좀비 사태가 일어났는데 좀비 사태를 해결하려고 주민들이 어떻게든 연구를 한 거거든 좀 쓰면 돌아올 거야 여기 같이 주문인木고기 notified 보석 Canad 많아요 tasted 이런 게카롱OMAN 아 지금 잘못된 거 같은데, 이거 나와. 돌아왔어? 아니 아직 안 벗겼어. 아직? 이제 몸에서 백신이라 싸고 있는 거야? 백신이 백신이잖아. 아, 너 스킨도 지금 정리로 돼 있잖아. 어? 이게 지금 좀비 사이, 좀비 바이러스가 파진 거거든. 역시. 하하하. 아, 너 스킨도 지금 정리로 돼 있잖아. 어? 이게 지금 좀비 사이, 좀비 바이러스가 파진 거거든. 귀엽게. 하하하. 네, 네. 맞아요. 어, 그래서 이 기계가 다 아셨는데, 이게 지금 좀비 사이기.. 좀비 바이러스가 파진 거거든 귀엽게요 좀비가 사람을 만드네 약간 귀칼 봤음? 어 봤지 착한 언니 말이랑 여자랑 그런 게 있어 아아 이런 거 이런 말 있잖아 사람은 책을 만들고 책은 사람을 만든다 오오오 좀비는 사람을 만들고 사람은 좀비를 만든다 아니 좀비는 사람을 만들고 언제 변하지? 지금 깨도 되나 봐 지금 깨면 맞는데 하도 맞는데 그 상태지 이거 막 목», Hammer 아 belly 아파 머리 아프다고요? 아니요, 허리가... 얘 형, 아, 안으로 데리고 있습니까? 허리가 아플 수 있어요? 자, 우리 한번 우리 또, 허리를 주고 우리 두 똥을 좀 쏘고 싶다 이렇게 한번 더 끼고 자, 그렇게 볼게요 우리가 만들어낸 문제에 따라서는 피해소인 분포에 대한 년의 투성들을 생각을 해야 되겠죠 정대하자면 우리가 지금 모델에서 만들어낸 분포는 이진불류이기 때문에 일종의 110분 보급성에 따라서 데이터를 나누는 모습을 말하겠고 보여줄게겠죠 근데 직접적으로 데이터에 대한 년의 년에서 아홉 분포에 나가냐는 건 뭘 거예요? 주식값이겠죠? 그럼 그 주식값에 우리가 뭘로 왔거든요? 환경값으로 왔어야 되겠죠? 그때 필요한 출력에 대한 년의 액티베이션 범튼이 보다 시그 모델이게 되면 시그 모델을 종단에 묻혀주면 얘네들은 무슨 모델이 된다? 이진불류 모델이 된다 자, 이런 블루투성을에 따라서 출력 감수에 대한 년의 투성들은 아주 불른모델 같은 경우는 이런 카테고리 분포까지를 검토해서 모델을 정리하네요 그 카테고리 분포는 밸런을 분포을 이용해서 분포, 페이카네랑 사건이 발생한 확률들 뭐 디지털 리학을 띄워 앱을 연상까지도 이야기하는데 심플하게 말할게요 버금처럼 버금 때문에 치면 쳐와 자, 그럼 여기 중요한 게 있어요 우리가 시그 모델을 붙인다는 것은 유런의 종단을 붙이는 거예요 맞죠? 그럼 내가 이진불류기를 만들겠다고 할 거예요 그러면 모델 입장에 출력 유런은 반드시 몇 개야? 이진불류기를 만들었다는 거라고 하면 두 개요 아, 한 개 두 개부터는 이제 다 좀 불류려왔다 자, 그래서 이진불류에 대한 물러리야 그러면 유런의 개수수가 늘어나서 값을 풀어내겠다 제가 만약에 유런의 개수를 3개 정해서 클래스 물류기를 만들겠다고 할 거예요 그럼 저는 몇 개의 클래스를 맞출 수 있는 건 모델 입장입니다 이진불류기를 맞아요, 3개 그리 대신 종단을 하면 시그 모델을 붙이게 소프트 맥스에 붙이게 소프트 맥스 맞아요 소프트 맥스를 붙여야 돼요 왜? 3가지의 아웃품들 중에서 가장 높은 녀석 이게 정답할 거야? 라고 제시하는 거예요 네, 여기도 물어보니까 저는 고양이랑 계랑 사자랑 학습을 시켰어요 이 모델은 뭘 예측하는 모델이에요? 고양 사자 개 이 3가지의 물체를 맞추는 모델이 근데 제가 여기 물체를 넣어서 그럼 어떤 성장을 내보낸 거예요? 셋 중에 하나 맞아요, 셋 중에 하나 앞서 우리는 멋진 나리게 모델은 뭘을 만든다고 했어요? 하나의 공식은 안 된다고 했죠? 우리가 그대 공식에서 입력값이 잘못되면 못 풀겠죠? 네 근데 입력값을 그대로 넣기만 하면 무조건 뭘 만들어질 거예요 숫자는 나와요 맞아요, 숫자는 나와요 정답은 무조건 만들어집니다 입력이 이런 멋진 나리도 마찬가지로 우리가 입력값이 잘못되면 사실 동작이 안 돼요 맞죠? 네 입력을 잘못 오는데 값이 나온다고 이게 말해도 돼요? 안 돼요 안 되겠죠? 근데 입력을 주기에는 무조건 결과가 많아요 그 결과가 어떻다 하더라고요 틀렸다 하더라고요 나오게 됩니다 사실 여전히 비전 쪽에서 고색적으로 가지고 있는 이걸 개선하기 위해서 연구가 진행되고 있는 대로 오자면 내가 이미지를 세 종을 쓰시겼잖아요 근데 이게 아는 걸 입력하는 이게 아니다라고 하는 역치를 하고 싶잖아요 근데 여기 중요해요 모델의 학습은 어? 맞다만 학습한다고 했어요 맞다만 블루를 한다고 봤는데 참고지자면 3만 학습한다고 했어요 X X겠죠 아닌 U도 학습을 해서 그래서 내가 확률이 왜 좀 있는 거예요 여기 중요한 건 내가 어떤 값을 줘 볼 때 아니다라는 것들을 결정시켜야 되는데 성공은 없어요 그래도 학습이 됐을 때 안 되실 거예요 네, 그래서 이 빛나는 게 더 멋진 것 같아요 가지고 있는 문제점은 뭐냐고 오로지 순수하게 학습을 한 내용의 한정으로 문제입니다 그럼 학습된 적 없는 내용들을 전달한다는 건데 제대로 못 품다 보구나 네, 못 품다 그런데 전달은 내보낸다 안 내보낸다 내보낸다 이걸 철집기 때문에 내가 텍스트에서는 말자면 이슈들 보면 변화해지잖아요 그리고는 이슈 당뇨입니다 그것처럼 이러한 국진들에 대한 모델은 내 목적도 중요하지만 어떤 긴념값과 정확하게 내가 학습시킨 게 뭐냐 나를 것도 외침이 중요하다고 말해주세요 그런데 아클래스를 대결을 만들었다고 한다면 세계에 대한 의혹을 오직팀 무조건 철저히 대하되겠죠 그래서 여기서 등장하는 개념이 우리가 워낙 인코딩 끝났죠 워낙 인코딩 그리고 라벨 인코딩 오디덜 인코딩 이야기 해봤잖아요 맞나요? 자, 라벨 인코딩이라고 하는 작업과 오디덜 인코딩의 차이가 대해서 이야기해보면 오디덜 인코딩 같은 경우 연구 의미가 있어요 그런데 라벨 인코딩은 각각 모두가 숫자값들에 대한 의미를 구여하고 각들을 선택하는 거예요 정리하자면 이겁니다 내가 지금 숫자로 외에 보낼 건데 이 숫자로의 어떤 의미를 줄 거냐에 따라 석 프로그램의 방식이 다르다 두 번째 개치에 대한 내용들의 아웃퍼즐 유랑이 여러 개 있다라고 한다면 다중 클래스를 만들 때는 석 클래스를 떠나가 내가 이진불류류를 만들고 싶다라는 종단인 누런은 반드시 몇 개여야 한다 하나 하나여야 한다 그래야지 이진불류류로 만들 수 있다 그럼 만약에 다중 웨이블블류로 하고 싶다 라고 한다면 레이블 다중 레이블 불렀느냐 하면 아웃퍼즐이 있어라 그 중에 남사 안 면소 검은색 옷을 찍을 수도 있잖아 좀 둘 다 맞는 거 같아 따로 따로 다운다 개수막 제가 웨이프 한 개를 강조했냐면 다중 레이블을 불렀느냐 하면 될 때 유롤렛 조그의 액티벤션 펑션은 시그 모듈 그만한 걸까 유롤이 이 유롤의 정답이 맞다 라는 걸 내버려 근데 그는 세 개 그 뭘까 세 개 다중 웨이블블루 이런 정답에 대한 키워드들을 나타낼 수 있는 모델 특성이 있기 때문에 주의하셔야 돼요 뭘 만들든 분류기를 만들때 소프트백스 암수란 이 소프트백스 암수는 실수 베터를 뭘로 바꾼다? 환길 베터 환길 베터로 바꾼다 그래서 이 실수 베터의 강요소는 영회성부터 일사위에 법위로 변환되고 그 변화는 요소에 합을 반드시 모여야 되겠다 1위된 그렇죠 1위된 만점으로 만들어놓은 게 소프트백스 암수 그래서 여러 시인정방위 출력 개친에 대해서 실수 베터의 탈색오리 무부어에 대한 환근들로 변화할 때 사용하는 구조들이 소프트백스 암수에 대한 방식입니다 이러한 평소에 따라서 문제를 끌 수 있고 데이터들을 표현합니다 이번에는 회기에 대한 만점으로서 이야기하겠습니다 회기는 여러 국립변수와 종독변수와의 관계를 연속함주역대로 분석하는 데 그러면 관측항론의 독립변수와 종독변수를 나눠야 되겠죠 지금 XY로 나눠야 되겠죠 남은 다음에 뭐를 관계를 함수형대로 분석하는 거예요 그럼 이 관계성에 대한 내용들은 각각 어떤 정보들로 정리를 되려냐 대표적으로 데이터를 관측할 때 관측하는 오차 또는 실험오차는 무슨 분포로 정리를 된다? 그렇죠 앞서 우리가 분류라는 거는 분류로 관계한 양보의 내용들에 내다라고 한다면 회기패턴은 연속성에 있다며 연속성은 정기 분포를 이야기하잖아요 그 오차에 대한 정독들을 우리가 무슨 형태로 가우시한 분포를 기받으로 데이터를 대한 형회에 정립을 하는 거예요 정리하자면 회기 문제는 무슨 분포를 예측하는 모델이다? 강유부 그렇죠 가우시한 분포를 예측하는 모델이고 그래서 이 분포 안에 들어오는 정답에 대한 형들을 맞추는 겁니다 그래서 가우시한 분포는요 평균을 중심으로 대충적인 종무형으로 발생식의 확률을 가우시한 분포이라고 이야기하게요 그러면 종무형이죠 그럼 여기서 대표값은 누구 왔어? 과연? 평균이 제일 좋지 그렇죠 평균이 제일 좋지 하는 거 여기서 중요한 거예요 우리가 연속성 데이터 분포에서 평균이 난 게 뭐예요? 이 데이터 분포의 중요한 핵심값을 이야기하잖아요 그럼 이런 중요한 핵심값을 정립하는 거에 가우시한 분포로 정리가 되면 우리는 정답을 맞출 수 있어 알았죠? 그겠죠? 평균값 그래서 평균 요소로 연산되면 오차에 대한 연산도 우리가 뭘로 한다? 민기야 평균기야 연산대로 오차를 해야 되잖아 목적성이 가우션이 뭔데 그래서 그 장안의 예정이 뭐냐면 아웃풋으로 와이가 결정이 되었잖아요 그 와이 값은 정답이라는 그 정답은 누군가실까요? 아웃풋으로 평균값으로 가야 되고 그 대비가 있어요 오차 가우션을 구성할게요 그 밸류는 가우션을 보려고 조금 같이 보더라고요 가우션을 본격적으로 만든다면 내가 만든 추세선에 대한 연결에 사위점에서 발생될 수 있죠 회기라는 추세선에 대한 연결이 이상적인 백헌을 날려면 이유가 뭐냐면 바로 이 점을 대상해서 평균값을 기준으로 직선을 끊는 거기 때문이에요 그럼 이 백헌상 X에 대한 연결이 평균값이 틀러진다는 거 벗어난다는 건 직선이 이상하게 그렇죠 개인들의 영향을 크게 만들려는 녀석이 뭘까요 우리 머신헌인 개념을 한번 이야기한 적이 있는데요 이상치가 만약에 동포에 벗어난다는 얘가 어떻게 될까요 쓰러질겠죠 그럼 이상적인 직선을 끊을 수 있어요 없어요 정리하던 게 아니라 우리는 이상적인 추세선을 끊어야 돼요 그런데 벗어난 정보들을 어느 정도 파협해서 평균값에 직선을 끊었을 때 제일 좋은 예측에 대한 감시지만 그 예측에 따라서 발생하는 참전이 좋은 줄 알아요 그걸 우리가 뭐라고 잘 살아야 돼요 정대한 점이 사실 점 하나 다 뭘 하시니까 가우시한 공부를 가지고 그 전들마다 평균적으로는 그러니까 평균 가우시한 공부가 만들어지겠죠 그 평균 가우시한 공부가 최대한 이상적인 직단을 맞춰져 있다고 한다면 그러면 틀어질 가능성이 높아요 낮아요 맞죠 그래서 이 데이터들의 학습에 좋을까요 나쁠까요 정리를 해야죠 뭐 가우시한 공부 특성에 잘 맞게 오연히 정리해진다고 학습에 좋을까요 나쁘다는 좋겠죠 이 입지도 이 일명을 왜 이야기하냐 이게 바로 폐기 패턴의 모델이어서 모델을 최저화할 때 데이터에 대한 의원들을 정리를 하고 쏘는 정리를 해주는 로마라이즈를 쓰는 뒤나 라고도 말하고 있습니다 자 그래서 이러한 가우시한 공부의 평균을 예측하고 그 평균가 됐다 라서 이 점에서 추세사를 만들어야 하는 것을 이야기합니다 그럼 이제 이 추세성이 만들어졌으니까 이제 뭘 계산할 수 있습니까 회기 추세상 기준상 오차 맞아요 오차 그 오차를 뭐라고 하는 거잖아요 정별로 그 정별로 직선으로 떨어지는 걸 두 번째로 그 이 단체들을 다 도와서 뭘 계산할까요 평균가 맞죠 그래서 평균 오차 계산하는데 그냥 더 하는 무슨 문제였죠 살았어요 그렇죠 그래서 이제 부월 없애는 우승괴념 재고 그렇죠 재고 또는 절대 그래서 대표적으로 회기에 뭘 계산할 수 있습니까 이 점 오차를 그렇죠 그리고 차를 계산할 수 있는 로스 범차를 휘치게 하는 거예요 그러면 내가 회기 문제 풀고 싶어요 로스 뭘 쓰일까요 평균 그렇죠 평균 재고 보차 그렇죠 평균아 저 좋지만 재고 보차 또는 절대 거차 나와를 비받게 그리고 평균 재고 보차 일반적으로 써요 이유가 뭘까요 빛나는 게 저는 평균 재고 보차 써요 이유가 뭘까요 빛나는 백포르 파티이션이고 모델이 깊어질 때 요고 생각해요 자 우리가 재고 보차는 원래 가지고 있는 각색 데뷔야 어때요 그러니까 증폭대요 그죠 증폭대죠 허재적 그럼 오차를 크게 조용을 안 해요 근데 대칭이 깊어지면 이 계층에 가중치를 5천 원 하죠 그러니까 미분이 여러 번 조성되더라도 어느 정도 안정성 있는 결과들을 노출할 가능성이 생기겠죠 근데 여기서 두위해야 되고 있어요 그러면 우와 평균 재고 보차가 절대적으로 좋은 거냐 재고 보차기 때문에 뭐가 될까요 뭔가 틀어진 정도가 크다는 그게 뭘까 될까요 많이 빛나는 그렇죠 많이 커지겠죠 그러게 해서 이 재고 보차가 오차가 말도 안 되게 커져서 제대로 학습을 진행할 수가 없게 되는 거예요 그러면 그런 때는 뭘 갈까요 절대까 오차나 아이미스퀘어 같은 게임으로 바꿔줘야 되겠죠 자 그런 문포에 따라 정리하자면 회기모델은 가오시안 문포 20분리버린대는 때려도 이렇게 문포 다 좀 그랬으나 같은 경우는 다 문포 관점의 모델들의 공보를 만든다는 거죠 그래서 출력계층 같은 경우들은 오시안 문포는 평균인 애를 예측하는 계획적이겠죠 그래서 이 평균의 애를 요축하는 가점으로서 회기모델인 경우 예측된 평균과 분산이 바뀌게 되면 돼요 안 돼요 안 되겠죠 그 순간 다른 정도를 말하는 거잖아요 그래도 뭘 쓸까 종단을 안 되죠 그래서 리니어 본전을 쓰는 거예요 아무런 정도 없으면 뭘 쓸까 했죠 리니어 본전이죠 이유는 그냥 써야 되니까 가 아니라 보라서 일대 명동과 분산이 바뀌어버리면 의미가 달라져 버리기 때문에 제대로 정답을 못 한다는 거죠 그래서 회기팩터는 뭘 쓴다 리니어 본전을 액티베이션 본전으로 쓴다는 거예요 그래서 모델로 놓고 보고 그냥 뭐 써라 액티베이션 본전 안 쓰면 그게 똑같은 말이다 라고 할 수 있겠죠 순만의 신경만 하면 이런 것처럼 인력계층은 인력데이터를 뭘로 받은 건가요 백터 형태로 다 백터를 받아서 시납스로 쫓아와 개취는 백터를 백터함수는 괜찮아요 사실상 우리가 만든 류러 렉터컨 백터 감탈 라고 표현했잖아요 이것처럼 인력을 반드시 백터로 써야 돼요 이 백터로 내가 안 주면 제대로 본질을 풀 수 있다 없다 풀 수 있나요? 없어요 없어요 대표적으로 특히다 플리커넥퀴드 레이어넥 반드시 뭘로 써야 돼요 플리커넥퀴드 백터로 써야 돼요 자 우리가 조금 배웠던 인연들을 한번 들지 봐봅시다 먼저 우리가 기본점이 뽑아주시기 바랍니다 이 정도는 전부다 구조가 뭘로 만들어져있을 때 있죠 플리커넥퀴드 레이어넥 만들어져있다 있죠 얘들은 처음부터 인력받아서 반드시 뭘로 써야 돼요 백터로 써야 돼요 백터로 써야 돼요 백터로 써야 돼요 나중에 배우겠지만 시내넥무는 인력을 이미지로 인정합니다 왜? 이미지를 통해서 귀엽빛으로 놓출하는 것 같아요 자 근데 피처 전철이기는 기회가 돼요 그 다음에 종단은 제가 뭐라고 했어요 블루비치는 무슨 레이어라겠죠 출력 플리커넥퀴드 레이어넥 있잖아요 맞죠 그러면 제가 처음에 입력할 때 뭘로 주세요 시에는 이제 백터로 써요 이미지로 백터로 써요 그럼 이미지로 그럼 이미지로 이미지의 주요 피처를 뽑아내겠죠 근데 여전히 이미지일까요 백터로 써요 여전히 이미지입니다 이미지의 주요 피처를 뽑은 거예요 쉽게 말한다면 여기 사진을 딱 찍어져요 노트북을 알고 싶어요 그런데 노트북의 주요 피처를 보여요 모양 꼭지 부분 그 다음에 이 부분에 모니터가 날려야 하는 키보드랑 결합게 있는지 그런 부분에 노트북이야 이렇게 하잖아요 이 부분에 사진이죠 여전히 그 부분 모니터랑 키보드랑 연결된 이 부분이 같이 보통 있는지 거기는 사진이지 않았어 그럼 사진이에요 백터로 사진이지 알아요 그런데 제일 종단은 뭐라고 했어요 플리커넥퀴드 레이어라 했잖아요 나는 뭘로 입력해야 해요 이미지를 넣어도 돼요 그럼 종단은 분작을 해야 될까요 바꿔야 돼요 그쵸 뭘로 백터로 그렇죠 나중에 배우겠지만 CNN으로는 인력을 이미지로 가서 이미지로 나오다가 종단에 이제 내가 최종적인 주요 피처야 하고 결정되면 걔들을 몰라봤거든요 백터로 바꿔야 돼요 그럼 나중에 알았는데 연도 마찬가지예요 트랜스 코버도 마찬가지겠죠 전체적인 모델 구조도 나중에 배우겠지만 거의 다 보고 다 종단에 뭐가 붙어있냐면 플리커넥 퀴드 레이어드 그럼 나중에 앞에서 어떻게 처리했던가 우리가 무조건 뭘로 바꿔야 한다 백터 퀴드 레이어드가 괜찮아요 쉐이브는 어떻게 되요 첫 번째는 가로 인스턴스 두 번째는 세로 이렇게 쉐이브 해야 되겠습니다 그래서 우리가 이전에 사진 가지고 쉐이브 티옷을 넣어요 처음에 300장 백백 있었잖아요 근데 이거 모델에 넣으려고 했으니까 근데 뭐지? 안 썼지만 근데 오늘 백터 바꿔야 되니까 백터 바꿨잖아요 맞죠? 그때 어떻게 하세요? 오페스 인스턴스 그대로 바꿔보고 백터로 바꿔왔죠 그랬더니 뭐야? 옆에 앞에는 인스턴스 뒤에 한 개 존재하는 거죠 그래서 우리가 이걸 가지고 데이터 데이터 셋 하면서 학습했었잖아요 마찬가지입니다 독일하게 입력 데이터 크기가 M계 라고 한다면 백터라면 입력계측은 M계의 유언으로서 정의를 해야 돼요 그래서 내가 입력하려는 게스트가 뭐냐 몇 개냐에 따라서 데이터를 전달한다고 한다면 거기에 대한 되는 모습을 유럽을 맞춰줘야 되겠죠 이런 유럽을 대한민국 규칙석에 따라서 우리는 설계합니다 두 번째 은닉게측을 이제 설계할 거예요 첫 번째 입력계측은 뭐에 맞춰라 데이터의 특징수를 맞춰라 두 번째 히든층은 뭘 생각해야 됐나 활성한 줄을 생각해야 돼요 근데 액싱적인 기어들이 이야기했죠 은닉층에 백트메이션 펑스의 목적은 뭐예요 은닉층에 백트메이션 펑스 하는 거죠 그거로 좋아요 다음 걸 줄 건데요 이것만 생각해요 그러면 줄 건데 안 줄 건데 어떻게 줄래 라는 걸로 백트메이션 펑스는 백트메이션 펑스는 백트메이션 펑스는 백트메이션 펑스는 백트메이션いて S자 넌젤로ериgun returning który가ctl 경고miaSim js 원 fil Ql Ql modeling 그 율대를EM 시그 보이드를 어떻게 해요? 이런 식으로 했을 줄 알았죠? 네. 그래서 이걸 로지스틱에 대한 역함주로 만드는 거기 때문에 로지스틱 감수 라고도 모르죠? 네. 로지 감수? 그 역함중에서 로지스틱 감수? 그거 똑같이 시그 보이드라고 부릅니다. 시그 보이드 공식은 영어 시그 공식이 되어 있어요. 그럼 이 수식에 대한 영어의 노조 논리 이야기할 수 있겠죠? 그다음 탄젠타 아이폴리. 이 공식의 패턴들로 만들어져 있어요. 예전에는 이 공식 다 외워서 써야 했지만 우리 멸표 형 앞서 그냥 키보드만 기억합시다. 아, 이 그 과지가 보다. 시그 보이드 미안하구나. 그런데 이 공식은 지금 보이죠? 시그 보이드는 아홉, 푸슨, 영에서부터 1, 4의 예값이 나오나. 0.5를 기준으로 0과 1에 대한 기준성이 만들어진다. 자, 여기서 여러분들. 이런 2라고 만들어도 클래스 잠깐 볼까요? 여기 코드에서. 실수했던 말. 내가 실수로 코드가 여기 없거든요? 여기서 이게 아니라서. 유언투 한번 봅시다. 유언투 말고 싱글레이요. 싱글레이요로 우리 로스 확인섭 있잖아요. 로스 그래프 확인섭. 제일 마지막으로. 네. 그러기 보면 액티베이션에 대한 정도에서 우리가 예측봉전 하나 만들어봤잖아요. 기억나시죠? 네. 예측봉전 보관의 리턴 감소에 제가 뭐 했어요? 0.5 이상. 그렇죠. 0.5 이상이라고 써놓은 거 보이죠? 네. 이게 뭐를 의해하냐? 바로 시그 보이드 계열에 대한 정도로 우리가 아웃포즐 부분 2진 분류예요. 그럼 이 2진 분류에 대해서 나타낼 때 위에 보시면 액티베이션 공전도 지금 아까 우리가 구두로부터 갔던 이 공식 그대로죠. 뭐 어때요? 액티베이션 뭘까요? 액티베이션 제가 주심으로 쓰긴 했지만 이 스포넌셔리와 된 게 이, 이 이런 게요. 그리고 마이너스 제때리 때문에 제때라고 써놨죠? 이거랑 수식 똑같죠? 즉 여러분들이랑 저랑 만들어도 그 롤렛스 액티베이션 공룡의 시그 보이드 부분입니다. 근데 시그 보이드 부분을 다는 특징이 뭐죠? 출력라시 만들어드릴 때 0.5 이상이면 y가 시보여요. 1과 0으로 써놨다는 겁니다. 그래서 음수 패턴, 양수 패턴으로 보여주시면 얘가 되는 건가요? 그리고 0.5 이상이면 참, 0.5 이상은 거짓으로 내가 정립을 했다는 거겠죠. 키워도 너무 좋아, 이게 되셨나요? 자, 그런 특성들에 따라서 시그 보이드의 예를들에 대한 종이 아웃푯들을 직접적으로 동작시켜줄 수 있어요. 그래서 0.5 기준으로 양과 음을 정립할 수 있다. 자, 두 번째. 한짓도 해볼 겁니다. 해복 단짓때 같은 경우들은 범위가 어둡다. 마이너스 1에서 어둡다 1 사이에요. s 패턴은 동일한데 얘들은 중간 확실 어디에 있어? 0에 있어요. 그렇죠. 0에 있어. 0을 기준으로 양, 음을 나타낼 수 있고 이어서 출력에 의해 보이는 범위는 마이너스 1에서 1 사이에 범위를 이야기한다. 라고 말해볼 수 있겠죠. 내가 말할 게 아웃푯상, 와이에 음수의 의미가 필요해요. 출력의 음수의 의미가 전날이 돼야 해요. 그러면 누굴 써야 될까요? 그렇죠. 해복 단짓때 쓰니까 거짓말이겠죠. 근데 의미가 아예 없는 건 아니지만 양수가 더 중요해요. 그런 가점에서 누굴 써야 돼요? 시그노이드. 얘들은 좀 어떻게 사이에 음수를 이야기하는 겁니까? 근데 이런 생각이 안 들었죠? 시그노이드에서 양수들을 의미하고 이어서 양수의 개념들을 정리하는 거지만 x축을 작가 볼게요. x축에서 음수와 양수 둘 다 시그노이들의 의미가 있어 있어요. x축에서. 있죠? 의미 있잖아요. 이렇게 선형을 생각합니다. 선형 검행. 의미가 있죠? 그럼 이런 집교에서는 음수에 대한 내용들과 양수의 개념들을 같이 반영할 수 있겠죠. 그리고 단짓때는 뭘 또 어때요? 음수와 양수라는 정돈도 같이 병해하고 있고 얘는 y축에도 음수, 양수 개념을 같이 가지고 있죠. 이번에는 이제 렐루를 해볼 거예요. 렐루를 볼게요. 렐루는 얘는 뭐예요? 음수는 그냥 땅이에요. 그러다가 영음에도 없는 순간 그대로 선형 검행. 그리고 선형 검행. 약하면 약한대로 주고. 당하면 당하대로 주는 거예요. 지금 보기에는 차이는 뭘까? 지금 보기에도 또 약한대로 주는 것 같잖아요. 단계적으로 올라가잖아요. 어느 범위. 근데 그 범위가 한계점이 어디까요? 1까지. 0까지. 0에서 1 사이만 적응해야 돼요. 렐루를 넣어요. 작가가 크면 진짜 크게 전달될 거고 작음은 작게 전달될 거에요. 지금 제가 한 점을 뭐로 생각하냐죠? 히든치. 히든치는 목적이 고랐어요. 합소에 줄 거냐 말 거냐 마시기 싫다 했죠. 어떻게 줄 거냐. 아는 관점에서 렐루 계열과 지금 보기 계열을 이야기할 수 있는 거예요. 근데 렐루를 생각해봅시다. 렐루는 X 주계 관점이 너무 공개한데 음수의 의미가 전달이 돼요. 안 돼요. 맞아요. 안 되죠. 그러면 음수의 의미가 없기 때문에 기울기가 먹어야 될 거에요. 전달이 돼요. 즉 유론에서 나온 9개의 Output이 0이 돼요. 그다음 계층에 가중치가 있다 하더라도 뭐가 들어갈까요? 말인 더블 스프러스 괜찮아요. W가 몇 백이나 천이 있던 만에 무조건 0이 되어가 있어요. 그러면 그 유언의 의미는 의미가 뭐가 될까요? 죽어버리겠다. 의미가 하내없어져요. 이게 렐루 계열과 가지고 있는 문제점이에요. 자, 그리고 이런 특성들에 따라서 나타되지만 양수 쪽에는 굉장히 좋겠죠? X로 생각해볼게요. X는 축으로 놓고 볼 때 시그 보이드는 양수와 음수 양쪽으로 동일한 범인 한쪽 영향을 준다 라고 생각하면 맞는 말일까요? 아니요. 보시면 직선. 이거 손만 생각해요. 범인 보서란 거에요. 손만 생각해요. 딱 중간 사이즈만큼, 쪽 같은 머리 만큼. 그러면 양쪽으로 조금된다. 이렇게 변할 수 있게 되죠. 여기 기울이 계시나요? 그러면 렐루는 어때요? 어느 쪽만 정도요? 그렇죠. 그 금융소 쪽으로 고려하지 않아요. 제가 이걸 왜 이렇게 정립드리냐면 바로 뒤에서 배우고 대세적학 개념에서 초기 가중지 설정에 대해서 렐루와 시그모의 차이점을 인제하는 위해서 노작을 하는 겁니다. 생각해보면 범인으로 생각합시다. 우리가 숫자 크기까지 생각하진 않고 음 양을 1대1이라고 생각할게요. 숫자 1, 1. 그러면 시그모의 점은 어디 있는 거예요? 음수도 있고 양수도 있으니까 몇이에요? 2점. 맞나요? 숫자 범인 생각하지 말고 그냥 음수가 있다 이렇게 생각합시다. 이렇게 생각합시다. 그러면 양수 음수 둘 다 있을 때 얘는 2가 된 거예요. 그러면 렐루를 볼게요. 렐루도 음수가 있어요. 없죠. 근데 양수를 있어야 돼요. 있죠. 그럼 아까 있는 걸로 이런 생각 안 했잖아요. 그러면 얘는 몇이에요? 1이 되겠죠. 그러면 얘는 2가 되겠죠. 그러면 이 둘의 범인은 누가 더 넓을까요? 시그포이드가 제가 생각해야 될 범인이 넓겠죠. 나중에 배우겠지만 가중지 초기합이 볼 때 애플해시합 공듀는 렐루로 써드냐 시그포이드를 써드냐에 따라서 초기 가중지의 범인을 설정하는 초기이 범인 얘가 2배, 얘가 절반대요. 그런 기념들이 있는데 비슷하게 배울게요. 그럼 여기서 등장하는 이유는 무섭니다. 허용되는 숫자 범인이 차이가 있기 때문에 그렇구나. 그러면 렐루는 누구에요? 전체 데이터 관점에 절런만 신경 쓰는 거구나 라고 말해 올 수 있겠죠 키워드 본조가 얘기해 드렸나요? 근데 말했다시피 렐루가 처음 고환된 6개 목적이 뭐였어요? 얘가 어디서 처음 등장했죠? 이미지 맞죠? 이미지 이미지는 데이터가 뭐가 껴되세요? 영해서부터 양수밖에 없어요 얘는 응수를 고려할 필요가 있었다 없었기 때문에 처음으로 그렇게 고려하는 아이디로 였는데 문제는 얘 썩은 게 좋은 거예요 나란 쪽에 더 쓰겠죠 썩었던 게 응수가 아예 없는 의미가 없었잖아요 그 늦치기 때문에 의미를 가질 수도 있겠죠 그 응수가 의미를 가져도 늘어내들 죽어버리는 문제가 있어요 어? 큰일났어 줘야 되는 거에요 이게 없으면 지훈 중에 맞는데 줘야 되는 내용을 만들어서 딴 거죠 응수가 의미인 듯 합실하는 거에요 그 의미 있는 값을 다음 짓에 줘야 되는데 응수가 뜨자마자 그 값을 다음 짓에 줘야 돼요 그냥 안 줘요 크게 줄 필요는 없거든요 강안해 줄 필요는 없는데 적당한 게 줘야 돼요 얘는 아예 줄 수 있어 아예 못 줘요 못 주는 게 문제에요 그러니까 유론히 출발하고 확실히 진행이 안 되는 거죠 그럼 어떻게 하지? 라고 해서 등장한 연구기 리퀴레로 돼요 리퀴레로 한 번 봅시다 리퀴레로 보면 기울기가 지금 음량이 같은 게 달라요 다르죠? 이런 게 아니라 리퀴레로는 이 기울기는 이사하게 잡혔다고 이야기하면 이 형이 형 그럼 이제 음수 값을 전달할 수 있어야 되겠죠 사실은 나중에 좀 더 디테일하게 들어가면서 의미가 있겠지만 지금은 이렇게 기억하죠 음수에 영에 가까운 음수는 의미가 있다 이렇게 생각해요 여기 가까운 음수는 왜? 시그모이드를 보시면 돼요 시그모이드를 초반의 영향을 주죠 끝에는 영향을 주는 거예요 초반의 영향을 주자면 지고의 계획 그것처럼 초반 부분은 영향을 필요로 해요 모델을 전달시킬 때 근데 끝에 가면 크면 얘네들은 전달을 할 필요가 없어요 가치가 없어요 오히려 학습을 방해하지요 근데 리퀴레로는 맞아요 초반을 굉장히 적게 적용돼요 의미가 잘 전달 되는 것 같다 보이셔요 근데 워스트케이스로 만약에 음수로 엄청난 크기에 이렇게 값이 나왔어요 그럼 어떻게 될까요? 음수로 본전적으로 큰 건 의미가 없었잖아요 주면 안 된다고 했잖아요 근데 얘는 어떻게 죽어버릴까요? 이렇게 해 주죠 크게 줄 수도 있어요 맞아요 아무리 앞에는 조금 이렇게 잡히겠지만 뒤에 가선 없죠 큰 영향을 주시면 돼요 그만큼 영향을 줄 거예요 내가 학습을 잘 가려고 했거든요 그래서 잘못 했을 때 여기 가면 안 돼 하는 패너비텀으로서 따라서 갑자기 오세요 조금 벗어났을 때는 문제는 없어요 근데 내가 잘 가보인다고 너무 많이 벗어났어 하면서 확 긴 경험 저 지금 저기로 가야 되는데 정상적으로 할 수 있어요 없어요 저거 조각이 되는지 그런 문제를 가지렸어요 와 심각해졌죠 죽는 레드로는 해결했는데 학습이 오히려 방해가 돼요 오히려 이런 불안정한 데 그걸 해결하자 라는 아이디어에서 긴장하는 게 이제 이해를 전환하는 다른 CLRPL 영업 이런 개년들이 있다 이 정도로 기억합니다 그리고 우리가 선형성의 함수는 선형성의 개년밖에 모르지만 이 선형성에서 범위를 만들어낸다면 국자 뭐를 만들어도 되죠 비선형성을 만들 수 있겠죠 맥사물의 수업은 뭐냐 하느냐 1.9가 내 따라 그래픽 캡터를 다르게 만들어야 되죠 그랬더니 뭐인 것 같아요 비선형성의 문제가 만들어지겠죠 직선이 아닌지요 쉽게 말하면 직선이면 안 돼요 곡선이 필요해요 비선형성이 필요해요 그래서 렐루도 버려주세요 양수는 선형성의 음수는 제로가 비선형성 펑션을 만들었다고 이야기하죠 시그니처에서 비선형성에 대한 이 펑슌들을 말하잖아요 그러므로 이 비선형성을 맥사업을 통해서 구간별로 정리를 해주는 거예요 그래서 그 구간동으로 우리가 이런 종의 곡선의 함수들을 만드는 하나의 백도에서 펑선이나 이 정도만 기억해 주시면 되겠습니다 사실 디테일이라고 하려면 내용이 좀 많은데 우리가 지금 기억할 건 뭐가 뭐가 기억해달라고 했죠 뭐가 뭐 액티베이션 펑션 히든의 액티베이션 펑션으로는 몇 가지 계열이 있다 두 가지 계열이 있다 그 계열로 지그 보이드와 렐루가 있다 이 정도로 기억해 주시면 되고 렐루는 클리핑이 되기 때문에 시그 보이드는 여기 클리핑이 되기 때문에 어느 정도 범인의 크게 전달한 조조에는 감점을 낮추고 한다 그리고 연산 양이 작다 렐루는 연산은 단전하다 하지만 죽는 문제가 발생한다 이런 특징들에 따라 우리가 공작하려고 하는 액티베이션에 대한 내용들은 딱 사운들로 인지하는 게 필요합니다 대표적으로 퍼셉트롬의 액티베이션 무슨 누구였어요 스테이 암수죠 그래서 스테이 암수는 퍼셉트롬에서 제일 먼저 처음 인사했고 유론의 바로 방식을 모발해서 이거 신호 주겠다 안 주겠다 라는 것만 표현한 거예요 그런데 이에 따라서 이 동작들로 전달은 되지만 되돌아오는 감점에서 문제가 보입니다 그래서 시그 모이드 암수 같은 경우는 S자형태 암수로 계단 암수랑 똑같이 범위를 할 점이 됩니다 그래서 얘들이 여운 애서부터 인사해 보이려 합니다 그런데 지금 계단 암수는 이렇게 가이네트로 떨어지는 구간 때문에 뭘 할 수 없다 맞아요 미분을 할 수 없어서 시그 모이드는 미분을 할 수 있도록 하기 위한 형태들의 지금과 암수로 만든 거예요 그래서 시그 모이드 암수는 모든 구간에서 미분할 수 있고 증가 암수이기 때문에 미분 값이 항상 양수로 만든다는 특징을 합니다 그래서 액트레이션 펑션에 대한 분들의 팩터 번영에 의해하는 댄스 시그 모이드를 수 있게 되는 거예요 여기서 이제 추가적으로 관계연뜸 포화라는 개념을 조금 점겨볼 거예요 관계연뜸 포화 일종의 비율계팩터에 대한 정도 더 이상 움직임이 가치가 없다는 것들을 말하는데 이걸 관계연도 포화라고 이야기해요 시그 모이드는 펑션은요 암수의 끝부분에 미분 값이 여수로 포화된다 미분이 뭐라 했죠? 변화율 맞아요 변화율 그러면 그래프 시그 모이드는 펑션에 대한 이 구간상에는 변화율이 있어 있어요 있어요 있겠죠 선정적인 편식으로 입기고 있는 구간상은요 근데 극한으로 봤을 때는 얘들은 변화용성이 있어 없어요 없어요 개들의 의미가 없다는 거예요 그래서 기울기에 대한 정보를 전달하지 않습니다 이런 특징들을 통해서 암수의 포화란 인력값의 변화가 있다 하더라도 암수 값에 변화하지 않는 상태로 이래요 이런 포화특성에 따라서 데이터를 처리한다고 놓고 보면 어느 구간 바로 이 선정적인 구간의 의미만 데이터상 의미가 있다고 잡아주는 거예요 그럼 이걸 모서라로 숨면은 사실 필요성이 없는 의미다 라고 말해 볼 수 있게 됩니다 이어서 탄넨타의 공폴 닥치 표고들을 함속값의 법위가 몇이다? 그렇죠 마이너스 1에서 1, 4, 2에 대한 S인정 암수 뭐 시그모이드가 항상 뭐만 출력해요? 맞아요 양수만 출력해요 근데 얘랑 다르게 누구도 이 양수만 출력한다는 것 때문에 최적화가 비율적으로 진행되는 문제가 발생해요 어때? 음수가 필요한 점 경우 1대? 그러면 그걸 위해서 분장해서 보았는 게 한 잔 더 하이버블이 하이버블이 한 잔 더 라고 보시면 됩니다 이어서 렐루입니다 렐루는 큰 인력이 들어오면 폭발시키고 작은 인력이 들어오면 0으로 출력하라 무슨 일을 이야기해요 얘 또 마찬가지로 인력값이 뭐인 경우 M한 양수인 경우 M한 발사화에서 양수로 내보낸 것입니다 자 렐루의 특징을 조금 이야기하겠습니다 자 렐루는요 같이 한번 읽어볼까요? 시그모이드 계열보다 추렁과 학습 속도가 빨라지고 안정적으로 학습할 수 있다 이 이유로 대해서 말씀드립니다 아까 시그모이드 계열은 이 양 극단에 가면 무슨 현상이 일어나면 되죠 그라데on트 폭화가 일어나서 안 바뀐다니까요 그래서 1.9만을 벗어나면 이는 제대로 전달을 못 해요 그런데 렐루는 약하면 약한 대로 전달하고 강한 대로 전달을 하죠 그리고 비선 형성이 있겠네요 이 온도가 되겠죠 온도가 불구하고 쓸 수는 안 되잖아요 이라 특심들에 따라서 뭐 게다가 연산을 놓줄 때 스폰이셜로 분모 연산에 대한 고변산과 지수 연산을 해야 해요 렐루는 안 하면 되죠 그냥 주면 돼요 누구만 확인하면 되냐면 양순이 한 음냐 한 음냐 한 음냐 그럼 당연히 시그모이더보다 빨라요 빠를 수밖에 없죠 내 입을 안 벗어버리니까 이 투로우는 빠르기 때문에 고성상 굉장히 좋고 그 다음에 심지어 안정적으로 학습할 수 있다는 장점이 있죠 그러다 보니까 얘를 많이 끊어요 착각하면 안 되는데 렐루는 처음부터 이 안정성으로 폭조를 계속하게 만들었다가 아니라 이 정도에서 모델 승능 속도 올리려고 써봤는데 내가 좋아서 다른 쪽으로서 정행해서 흐른 중에 애티베이션 펑션도 하려고 하는 거예요 그러면 렐루에 대한 내용들에서는 다 좋다라고 이야기할 때 무슨 문제가 있어요 죽어 렐루라는 특징이 있어요 렐루는 특징 이 특징은 렐루는 빠르고 안정적으로 학습할 수 있어요 근데 다음과 같은 몇 가지의 단점을 가지고 있는 게 렐루입니다 그래서 오늘 날 일반적으로 인노린 모델을 만들겠다고 한다는 매직하게 사용되는 애티베이션 펑셔를 먹을까요 이런식으로 오늘 날 뭘 많이 사용하니까 매직하게 렐루 맞아 신고위도 뭐다 렐루를 많이 사용하니까 옛날에는 신고위밖에 없을 때 신고위를 썼는데 오늘 날에는 거의 대부분 디포트보다 렐루가 많잖아요 자 이게 렐루가 가지고 있는 한점은 렐루는 처음으로 또 구한 양수만 출력한 목적이라고 했죠 처음으로 또 하는 잘 만들지 이런 목적을 얘가 뭐 때문에 뭐의 목적으로 만들어도 되겠다 어떤 레이터가 또 이미지 레이터 목적으로 만들어도 되겠다 얘는 처음으로 또 음문수를 생각할 필요가 없기 때문에 진정을 안 쓴 거예요 오히려 이미지 입장에서는 음문수가 된장한 거니까 이미지 입장 제가 이미지가 옆에서 250만 됐던 데죠 우리 똑같이 소확적 연산을 하다 너는 뭐가 발생할 수 있어 왜 이때가 과외했을 때 그 음문수가 발생할 수 있고 있어요 근데 그거 이미지 관점에서 주요 피처가 아니예요 네 걸러줘야 되죠 그래서 제로를 즉 음문수가 발생하면 그냥 없애버리는 게 맞아 이게 일반적으로 말씀 됐잖아요 효율이 좋더니까 근데 좋으니까 쓰면 다른 문제에서는 다른 데 있어야 음문수가 의미를 가질 수 있잖아요 그럴 때가 없을 때가 어떤 문제가 발생하면 죽을래요 죽을래요가 발생하는 순간 학습이 진행되지 않습니다 왜냐 첫 번째 논도에서 렐루가 0을 내 바랐어요 그 다음에 내가 아무리 인혁받아도 그 유럽에 대한 이거는 의미천달이 돼야 돼요 안 돼요 안 됩니다 사라집니다 그러면 내가 첫 번째 주요 피처라고 줬던 게 합필이란 초기 가중심 어떤 시나리오를 말하는 거냐 그래서 하다가 미상적인 범선인데 음문수가 돼서 없어진 거 아무 상관 없어요 왜 꼭 이미지가 찾아서 주요 피처가 아니니까 없으니까 맞잖아요 맞죠 근데 이런 초기가 안 되는지 내가 결정하는 거 같은데 내가 결정하는 건 맞겠지만 완벽하게 정답을 맞게 결정했다고 말해도 돼요? 없어요 아니죠 정말 워스트케이스 첫 번째는 1년 뒤쩌어 정말 중요한 게 있었어요 근데 첫 번째 노동 가중치가 음문수였어요 아 그리고 만약에 양수여한테 1이었어요 근데 바이였스가 10이었어요 바이였스 10 연산을 했더니 결과가 뭐가 됐어요 빵 음문수가 됐어요 제프트가 음문수가 나왔는데 렐루 만났잖아요 렐루 만나면 어떻게 돼요? 음문수가 0이 돼요 0이 되죠 그 순간 내가 정말 중요하다고 생각할 때 그 피처가 어떻게 되죠? 없어져요 없어져요 그럼 내용 지금 들어 찬다 돼요? 안 돼요 주요 피처를 안 주면서 주요 피처를 도추라는 게 말이 돼요? 아니요 그렇죠 이 문제가 바로 주요 렐문수가 되요 핵심은 이제 학습을 하더라도 이제는 이제죠 얘가 전달된 적이 있어요 아니 근데 어차피 백폴박인은 고쳐지는 거 아니에요? 근데 처음부터 이미 유언이 죽었기 때문에 얘는 결과의 영향을 줬어요 안 줬어요 맞아요 안 줬기 때문에 얘는 변화는 이게 없어요 왜 이렇게가 고쳐질까요? 다이소가 고쳐질까요? 이거 안 고쳐져요 다른 거 없지 그러면 그건 유언 자체가 죽었기 때문에 제대로 해 주지 않도록 안 되고 그 돈줄이 결과의 영향 안 줬기 때문에 백폴박인은 좀 안 일어나겠죠 이게 안 고쳐져요 문제인데 안 고쳐지는 게 큰 문제죠 그러면 어떡해요 이거 잘못된 거잖아요 맞죠? 그래서 영향성 아예 죽어버리는 영향성을 있으면 안 되겠다 라고 하는 걸로 고앉는 게 필요합니다 이제 어떤 원리인지는 있었죠 죽을 내면 왜 발생하면 안 되는지 틀렸으면 고칠 수는 있어야 될 거잖아요 백폴박인의 목적이지 뭐냐면 완벽한 정답을 찾겠다는 아니라 틀리면 꼭 저거 조각 목적인데 틀려도 고치지를 못하는 거는 완전 잘못된 거에요 그래서 그걸 위해서 그 다음에 음주구만에 대한 용도도 사실 음수가 정답을 맞춰도 아니고 아니잖아요 의미적으로 잠깐 안 맞는 걸 얘 때문에 틀렸으면 고칠 수는 있게 해줘야 돼요 큰 값을 주는 게 좋을까요 짱값을 주는 게 좋을까요 맞아요 짱값 지금 컨셉은 뭐로 생각하면 양수는 옳다는 정답 음수는 틀린 정답 이라고 했을 때 그래도 값을 내보냈기 때문에 백폴박인의 목적이 고쳐질 거예요 안 고쳐질 거예요 고쳐질 거죠 잘못된 거니까 근데 영이 되면 순간 고칠 수 있어요 없어요 없어요 맞아요 그래서 조금만 전달하는 게 있어요 근데 아까 우리가 같이 정했잖아요 음수는 틀린 정답이에요 과도하게 틀렸기 때문에 과도하게 수정되어야 되는 거 맞겠지만 우리가 의미적으로 이거 혹시가 음수라고 하는 것들로 전달하는 게 좋겠죠 얘를 과도하게 놓지 않으면 틀린 거에 과도한 관점이 주르면서 나머지 거에 모든 정답의 오찬을 얘 때문에 발생했어 라고 해서 얘가 놓지 않더라고요 그러면 안 되잖아요 그래서 전체적으로 고칠 수 있도록 적극을 합니다 음수 구간에 기울기를 주면서 작은 그라디언트가 생겨서 학생도고도 고가 빨라집니다 그래서 다만 리퀴레로는 기울기가 오정되어 있기 때문에 최적의 성분을 낸다고 볼 수는 없어요 거기에 뿐만 아니라 기울기들에 대한 용들을 적용할 때 값이 커져버리면 제대로는 형태되면 고쳐지지 못하는 것인지도 버린다 그래서 좀 더 개선된 방법으로 피해률이 다른 것들도 존재하고 이해률이 이런 것들이 있다 정도로 생각해 주시면 되겠습니다 이해률입니다 이해률은 피해률이나 피해률에서 음수 구간이 직선으로 기울기가 작더라고 큰 음수 값이 들어오면 출력값을 반환해 버려요 그게 너무 크게 틀리게 만들어버린다는 거죠 그에 반해 이해률은 음수 기간에 대한 용들을 지수한 주로는 역동되어 있어서 좀 더 안정적인 형태들에 약속 기조를 만들 수 있다라고 생각하시면 되겠습니다 그래서 아주 큰 음수 값이 입력되더라도 함수 값이 커지지 않기 때문에 노이즈에 민감해지지 않는다 라고 생각해 주시면 되겠습니다 노이즈를 우리가 정답을 필요로 하는 정답에 대해선 저는 주요 피처가 전자들이 했잖아 일종의 노이즈를 어떻게 보면 하스의 의미 없는 정보나 쓸데없는 정보들이라든지 라고도 말하기도 했겠지만 이 노이즈가 무조건 나쁘다고 볼 수는 없어요 나중에 계속해서 노이즈가 뭔가의 의미를 가질 때도 많고 네일터에 하스의 안정화를 주는 경우가 되겠어요 근데 뭐든지 많으는 좋을까요 과하면 안 좋겠죠 네 과하면 안 좋겠죠 우리가 뭔가 코에 뭔가 들어와서 뱉어내야 되기 때문에 재채기가 나를죠 근데 이거는 굉장히 유명한 건데 재채기를 멈출 수가 있어 없어요 그럼 재채기 때문에 다른 일을 할 수가 있어 없어요 없겠죠 노이즈라는 건 하스의 안정성이 도움이 되는데 이게 짤과 지어 음주가 발생하는 건 틀린 정도인데 이게 과도하게 잡으면 얘 때문에 따로 할 수 있어 아까 그래서 제가 말했죠 음주가 틀린 정도인데 이게 크게 좋으면 특정 핏조 한 개만 다 잘못했고 나머지는 다 잘했으니까 나머지는 안 좋지 얘만 당어칠리 없는다는 거죠 얘만 영향이 너무 크게 맞는 거죠 그걸 좀 더 안정화 작업을 했습니다 이렇게 생각하시면 자 여섯, 몇, 삼, 수, 하트 경우는 그냥 일반적인 렐루샷을 뱉어보다는 성능이 딱 요 정도로만 기억하시고요 그리고 유닛 구조 같은 경우는 일종이 유런처럼 연결성인데 요거까지 알 필요는 없습니까 가볍게 인지하고 넘어가도록 하겠습니다 근데 맥스하우스에서 인지할 가능성의 한 가지가 좀 중요한 개념이 있어서 요구만 안 맞히고 기능의 현실을 가져오도록 할게요 자 우리가 맥스하우스라는 자체의 험순도 퀄시 펌션도 중요하게 한데 여기서 우리는 정립하려는 개념이 뭐냐면 선형을 통해서 비선형성에 대한 의미를 조금 부여해볼 거예요 우리가 직선이라고 이야기하는 백헌들의 선을 들으면 어때요 하나의 선을 의미하는 거죠 근데 이 선에 또 다른 선이 조금됨으로 비해서 직선의 백헌마를 갖다야 되잖아요 한가지죠 그리고 이 곡선의 의미를 하나의 주식으로 표현한다고 하셨을 때 굉장히 복잡한 주식을 만들어야 돼요 근데 이걸 복잡한 주식이 아니라 간단한 직선의 주식 두 개를 통해서 복잡한 주식의 모양의 형태를 만져낼 수 있게 되겠죠 이어서 각수히 전달될 때 직접적으로 두 가지의 경향성에 대한 내용들을 반영해준다고 하시면 허용적인 백헌들의 내용들에서 통합으로 보면 이 두 수식만 가지고도 보이는 뭐를 만들어주시죠 이 차곡선에 대한 정보들을 만들어드리겠어요 구간적으로 밖의 직선의 의미고 무조건 정으로 이야기하는 복잡한 차수식이라는 부분을 허용적인 백헌들의 내용의 문절로 나타내십니까 그래서 구간을 정립하면 허용 가지고도 이 허용성에 대한 내용의 경향을 정립할 수 있다 이 특징만 키워주시면 되겠습니다 사실은 블록 함수의 등장 등역을 비용한 게 앨룩이로 해요 그래서 이 각각의 특징들에 따라서 데이터의 배핑과 썸드를 이해한다 라고 표현해주시면 되겠습니다 그리고 스위치의 HMG도 있어요 얘가 언제냐면 구글 브레인에서 오토 MLE라는 걸 통해서 직접적으로 만든 레일로 기열의 애크메이션 펑션이에요 여러분들도 오토 MLE 가지고 주요 피쳐가 또 추라거나 가정 홍성수 분석을 다 돌려왔잖아요 그런 일종의 특성들을 이용해서 사람이 연달하지 만든 게 아니라 컴퓨터가 찾아준 이상적인 애크메이션 펑션을 자기들 모델에 쓴 거예요 그래서 이런 거 같구나 정도 그랬더니 모양을 보면 있잖아요 파란색 모양 어때요 제가 아까 말했죠 초반의 흉주는 의미는 있지만 큰 흉주는 사실 의미가 없다고 했죠 그래서 모델분석 어떻게 만들었느냐 하면 내려가요 여기는 사냥성을 그대로 가져갑니다 레일로처럼 근데 여기서 초반의 부분만 살짝 올룩하게 올라왔다가 그 다음에는 여기 수렴하도록 더위 만들어냅니다 이 원리들을 이용해서 2분을 하면 이런 식의 그래프를 그려지는 거예요 조금 더 데이터의 활용성이 벗어나더라도 안정성을 만들어 내고 거기에 변화성까지 줄 거기 때문에 훨씬 더 좋은 결과를 만들어낸다 나는 진짜로 존재합니다 자 이런 그릇들처럼 여러 애크메이션 펑션에 대한 오늘 같이 한번 이야기 해본 거고요 일단 지금 일단 10분이셨다가 여기서 다시 진행해보도록 하겠습니다 응 김하은 아, 진짜 너무 많아. 진짜 너무 많아. 와... 와... 하하마리... 아... 아... 아... 아... 아... 아... 아... 아... 아... 아... 하하... 하하... 하하... 아... 아... 50번 열심히 이렇게 부른데 아아 아아 아아 어? 너무 웃겨 왜 이렇게 웃는거야 여기다 아아 왜 웃었네 저희는 멤버가 웃겨 왜 이렇게 웃는거야 일단 탈당해야겠다 아 스트레스 맞네 어? 옆에 좋지 않습니다 아아 아 스트레스 맞네 다음은 저거 아아 그럴까? 한 2년고 응 지금 한 2년고 여기 있는거에요 아아 이거 왜 이렇게 웃는거야 ㅋㅋㅋㅋ ㅋㅋㅋㅋ 아아 이 형Green some more ㅋㅋㅋㅋ ㅋㅋㅋㅋ 이거 안니? 맞네 하... 있었다, 맞네 안니... 아, 꺼났네, 꺼났네 진짜, 얘는 진짜 죽었는데? 뭐야? 너 뭐하자, 너 총 감히 진짜 안 말하면 안 돼, 안니 안니... 일단, 셋에 바꿔 눈을 감아놨고 너 때 깃발을... 너 때 깃발을 안 낸다 안니, 안니, 안니 안니, 안니 안니, 안니 거의 덩어리 아, 새우나지, 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 새우나지 어휴... 그리고 aggi 좀 더 ignor에 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 아, 이거 진짜 너무 힘들어요. 그렇지. 많아지겠죠. 그 말은 모델의 크기를 통해서 확장되는 파라미터 양이 증가하면 모델 복잡도는 상승한다고 그러고 모델에서 주요 피처들을 잘 노출하거나 의미를 잘 외워왔다. 학습할까라고 말해주십시오. 그러면 생각해봅시다. 이미 학습된 모델에서 웨이프와 바이오스만을 이용해서 똑같은 모델 구조에 그 값을 그냥 구조하면 학습이 된 모델일까요? 안 된 모델일까요? 그냥 된 모델일까요? 말하지만 웨이프와 바이오스가 저장한 거랑 마찬가지예요. 그 웨이프와의 개념을 인지했다는 거잖아요. 그래서 웨이프와 바이오스로 이미 정답의 유치를 찾은 거잖아요. 그 위치만 전달해주면 나는 학습된 모델을 그리고 받았다고 그러죠. 그래서 웨이프와 바이오스로 웨이프와 바이오스가 그 모델 구조를 내가 만들지 않더라도 웨이프와 바이오스가 가지고 있다면 학습된 모델의 정보를 전달할 수 있게 되죠. 여기서 중요한 거예요. 웨이프와 바이오스. 우리가 찾는 그 미지스 이 미지스카스 얼마나 잘 전정하는 게 핵심이라고 말해주십시오. 너비는 개침별 유론의 수를 말하고 비피는 개칭의 수를 이야기합니다. 신경남의 크기를 정하는 공식이 있다. 없어요. 있으면 좋겠다. 왜일까요? 심플하게 말하면 어느 정도 비피가 좋은 거야. 어느 정도 너비가 좋은 거야. 알 수 있어요. 파괴되면 그걸 뭘로 파괴되면 로스트로 밖에 파괴되면 로스트로 밖에 파괴되면 로스트로 파괴된 모델의 성능이죠. 그래서 이 모델에서 이상적인 모델의 형태를 찾아낸다는 것은 굉장히 어려운 겁니다. 그런데 대표적으로 딥런기라는 녀석이 가는 특징이 있다. 데이터가 이상하면 우상팩턴에 퀸처를 뽑는다 했잖아요. 그러면 측정은 문제를 잘 해결하는 모델이 있다면 그 모델의 구조를 가지고 똑같은 기원의 문제를 잘 풀 수 있겠죠. 예를 들어서 이미지 카테고리 분류를 잘 하는 모델이 있어요. 그 모델 누군가가 잘 만들었어요. 그러면 내가 그 안에 데이터와 바이였스까지 완벽하게 한다라면 완벽하게 학습대 정보를 얻을 수 있겠죠. 그런데 모루토라고 아까 말했다시피 그 모델의 구조가 어떤 특징을 안 됐죠? 이미지 카테고리 분류를 잘 하는 모델의 구조라고 했죠. 그러면 내가 그 구조 그대로 만들어서 학습시키면 어떤 문제를 잘 풀 수 있는 것들 같죠. 나 잠깐요. 이미지 카테고리를 잘 할 수 있겠죠. 이것처럼 모델의 완벽한 동식은 없지만 이미기약습대 모델의 패턴들을 이용해 준다면 좀 더 좋은 결과의 패턴들을 확인하셔야 하는 거예요. 단순하게 말하면 사실 뷰러래 턱을 뭐의 조화도로 만든 모델일 뿐이에요. 뷰러래 조화도로 만든 뱉버 함수일 뿐이에요. 이 모델의 모양으로 여러가지 만들죠. CNN, CNN, RNM, TRESCOMMENT, BERT, GPT. 이런 비열들의 모델이 긴장하는 게 말 그대로 각각의 모델의 특성들을 어떤 형식인지 확인하는 거에서 알 수 있게 되는 모델이죠. 사실 모델의 크기라는 것은 완벽한 공식이 있다니까 없다. 그 말은 뭐 밖에 없어요. 실험적으로 찾을 수 밖에 있어요. 멋있는 모델에서 이상적이 파라딘터를 찾겠다고 했었던 기념이 뭐예요? 그리스서치, 그냥 기억나시죠? 그리스서치의 원리가 뭐였었어요? 다 찾아보는 거예요. 다 해보는 거죠. 어쩔 수 없이 딥러린기한 모델은 오늘 구조가 뭔가 이상적인 모델이 있다 이런 것들에 대해서는 없어요. 그런데 대표적으로 어떤 문제는 이런 모델 쓰면 좋더라 이런 모델 쓰면 좋더라 나는 개념은 있겠지만 대표에 대한 특성들을 파악하겠다고 할 때는 그걸 만들어봐야 해요. 그러면 만들어서 확인하는 만들어서 확인하는 이 개념 동전이 누구라고 하는 거예요? 그리스서치와 맨 덕서치와 같은 게 똑같죠. 그래서 신경광의 모델의 크기를 탐색할 때는 그리스서치나 맨 덕서치 같은 탐색 방법을 활용합니다. 그런데 이 탐색 방법에 대해서 이전에 한번 말씀드렸죠. 그리스서치는 어떤 뜻을 쓰느냐가 동의한 단역상에서 의미를 줄어들 수 있다면, 랜덤하게 그 몸 안에 만들어 낸다고 표현을 했잖아요. 이런 특성에 따라서 그리스서치는 파라미터 별로 구간을 정해서 등간격으로 값을 샘플링하는 방법을 그리스서치라고 이야기할게요. 등간격상에서 내가 의미 있는 조합이 없다고 하면 최고의 모델을 도출하면 쉽다 어렵다 어렵다고 말하고 있어요. 이어서 랜덤하게 값을 만들어 낸다는 탐색을 만들면 샘플링 방법이 나다고 볼 수 있어요. 그러면 다수의 값이 된 도구일 때 랜덤서치가 좋을까요? 그리스서치가 좋을까요? 이 부분은 수업시간 이야기했죠. 뭐가 좋다? 나는 모든도 있다. 랜덤서치가 더 좋다. 나는 모든도 있다. 이유는 우리가 실험적으로 해야 되기 때문에 똑같은 횟수로 한정을 짓는다고 하면, 누가 더 좋은 모델을 찾아낼 가능성도 없다. 랜덤서치가 좋은 가능성이 있다. 가능성 자체가 있는 거입니다. 이걸 그림보로 소피콘을 하겠습니다. 입값을 글로벌 맥시멘이라고 할 수 있습니다. 글로벌 맥시멘이 최형 선수입니다. 우리가 원하는 이상적인 모델, 글로벌 맥시멘을 만들어야 하는 모델이 좋잖아요. 모델을 무조로 만들어야 합니다. 그림버스는 이 간격을 기준으로 사람일 도로 모델의 형태를 만들었던 거죠. 그렇다고 성능집도가 여기, 여기, 여기, 세 가지가 작았어요. 지금 우리가 봤을 때 누가 성능이 제일 좋아해? 두 번째 모델이 이 모델이 베스트 모델이다. 라고 할 수 있겠죠. 왜? 세 번째 모델이. 근데, 우리는 전체적인 메이트업 한 점에서는 뭐예요? 여기가 진짜 제일 좋은 모델입니다. 제일 좋은 모델이. 아니죠. 그런데 랜덤서치가 있는 거에요. 얘는 가능성이 아니라 없어요. 종종권 좋다는 의미는 아니에요. 근데, 의미의 서치는 얘의 가능성이 있어 없어요. 지금 그리는. 근데, 랜덤서치는 어디를 뽑을지 모르기 때문에 여기를 도출할 가능성이 존재하죠. 그 차이를 이해하셔야 해요. 그래서 여러분들이 모델에 대하는 여러분들을 튜닝을 한다라고 하면 완벽한 모델을 도출하는 뭔가 이상적인 공식이 있다. 그런 건 없어요. 그대신 튜닝을 한다고 하면 좋은 것 같아요. 그래서, 튜닝을 한다고 하면 튜닝을 한다고 하면 좋은 정답을 찾아낼 가능성이 색이다라고 말하고 싶겠죠. 그리고 그 방법로는 어쩔 수 없이, 무슨 방법을 활용해야 한다? 무슨 방법? 탐색방. 을 활용해야 합니다. 이어서, 모델의 큰 기회 조정입니다. 모델이 무조건 크면 좋으던데 무조건 크면 좋으던데 그때그때 맞춰서 해야 해요. 그쵸. 목적성에 따라서 차이가 있어요. 무슨 말이냐면 악서 우리가 계층을 거쳐 나가다가 최종적인 결론이 만들어져겠죠. 그러면 제가 계층이 3개가 있어요. 우리가 방정식을 몇 번을 거쳐야 합니다. 3번을 거쳐야 합니다. 레인작진 만큼. 그러면 결과를 만들 때는 몇 방정식을 지나야 합니다. 똑같게. 그러면 학습이 완료된 모델이다라고 하더라도 이 거치는 방정식이 했으면 달라질 것 같을까요? 맞아요. 그럼 모델이 크다라는 말은 뭐에? 각세로 도출하는 시간이거든요. 예측값을 만드는 예측값을 만드는 시간이 걸린다는 거죠. 학습만의 시간이 걸린 게 아니에요. 예측값을 만드는 시간이 걸린다는 거죠. 그럼 내가 모델을 쓰고 싶어요. 그럼 모델이 큰 게 좋은 걸까요? 아니요. 오히려 늘려서 문제가거든요. 2번째는 하드웨어 스펙이 좋아요. 사실 여러분들이 컴퓨터가 신속하다보니까 컴퓨터를 뒷걸트로 장르는 경우가 많아요. 근데 컴퓨터는 생각보다 5,100 장비예요. 특히 쥐비옥까지 있으면 빛나는 게 굉장히 5,100 장비예요. 그럼 이 5,100 장비를 잘 돌아가는 모델이 내가 저 6,100 장비로 가자마자 돌아간다는 거 안 돌아갑니다. 그러면 내가 이 저 6,100 공식에서 내가 모델을 써야 되잖아요. 그 개념으로 이제 IT 개념에서 나 등록이 또는 여러분들 휴대폰, 최신의 한 번 나온 삼성 휴대폰이나 이런 거에서 강조하는 게 뭐예요. 오프라인드로 AI 할 수 있습니다 라는 말이 뭘 뜻하냐면 사실 이렇게 오지죠. 아니, 좋은 핏이 슈퍼 컴퓨터에 좋은 모델을 만들어 놓고 이 톤에만 내면 최고로 정답 안 지을 수 있잖아. 맞죠? 근데 이 톤에만 내면 최고의 순간 목표가 있거든. 팔을 수 있잖아. 사실 여러분 최신의 핏이 컴퓨터 자체에 습관을 쓰는 거예요. 자체적으로 장비되는 슈퍼 모델을 가지고 쓰는 거예요. 그 최신의 핏이 그 큰 모델을 내가 휴대폰도 올리면서 이게 말일 때 그 장비가 호영하지 못했죠. 이것처럼 모델의 크기라는 무조건 크다 결과를 줄 수도 있겠지만 목적성에 따라 안 만들 수 있다라는 거예요. 신경망에 부정할 때 굉장히 많이 사용하는 성대들로서 크기를 어떻게 하면 조정할까 하는 거를 고민성합니다. 그런데 아무것도 무예서 출발할 때 그러면 쉬운 것까지 어려울 것 같아요. 너무 어려울죠. 도대체 내가 지금 몇 층을 쌓아야 할지 어느 정도 크게 쌓아야 할지 근데 이전에 한 번만 해봅시다. 올해의 구조가 특정한 문제를 그때 좋은 계열의 모델이 이미 선행연구소에서 만들어진 모델이 있다 했죠. 최소한 최소한 모델을 그대로 가지고 오지 않더라도 모델 구조만을 참조할 수 있게 되겠죠. 그래서 신경망의 모델을 구성하는데 가장 많이 사용하는 표정이 방금 좀 하나가 어두우죠. 이미 성례에 검증된 거예요. 사실 여러분들, CNN이라고 하는 뭔가 새로운 모델 여러 번째가 아니라 그냥 똑같이 유럽을 들었고요. 그런데 계열을 가지고 이미지들로 하니까 좋더라 라는 결론이 이미 난 상태죠. 그럼 내가 이미지를 가지고 문제를 풀 거예요. 나중에 실습으로 되어 있지만 CNN 가지고 이미지 문제를 분류할 수 있어요. 그런데 내가 원하는 상점 좋은 성롱을 만들겠다고 한다면 CNN 모델을 써야 돼요. 그럼 그런 말점에서는 어때요? 레이터에 따라서 정영화되는 게 정해지죠. 그래서 성롱이 어린이도 검증된 모델을 뒷볼트 모델을 찾는 것. 그리고 나서 내 문제에 맞게 모델을 조정하는 뒷볼트 모델을 딱 정해놓고 나는 이것을 좀 깊어야 될 것 같은데 그럼 그때부터 출내에서 느리는 거예요. 또는 내가 레이터들이 완벽하게 패턴이 똑같아요. 그런데 나는 16대인가요. 학생 쓴 적 없는 데이터예요. 그럼 얘를 조금 더 추가적인 학습을 다루겠죠. 요 개념이 전의 학습입니다. 그럼요. 각각의 모델에 대한 특성들을 통해서 각 정보들에 대한 용의 전달을 예약해야 되요. 대표적으로 이러한 형태들을 2019년도에 지한된 EfficientNet이라는 뉴럴렌트와 그 존재예요. 이 뉴럴렌트는 깊이와 너비에 대한 용들은 기본적인 모델 구조나겠죠. 거기에 몇 가지 조정했냐 하면 해산돌하는 개념을 고려해야 해요. 그럼 몇 가지 고려가 된 거예요? 세 가지 고려가 된지 한 번. 얘가 지금 뭐랬지 제가 처음으로 이야기한 것처럼 이미지 관점에 대한 용들의 데이터를 가본되었죠. 그래서 이미지의 데이터 특성 때문에 해산돌하는 의미를 가질 수 있는 거예요. 그럼 데이터 도매인의 특성이기 때문에 사이를 이제 주시면 되겠습니다. 그래서 데이터 모델들을 볼게요. 그러면 지금 이제 이런 메이징 모델이 있는 거예요. 기본 모델이 어때요? 여러 가지 모델이 몇 개 측을 가지고 있는 모델이에요? 몇 개 측을 가지고 있는 모델이에요? 기본 모델. 네, 다섯. 그렇죠, 다섯 개 측 가지고 있는 모델이에요. 이 모델에서 관점을 세 가지로 오는 거예요. 첫 번째, 무슨 스케일링을 하겠다? 러비 스케일링이니까 뭐를 조정하겠다? 그렇죠, 모델 개 측의 러비. 러비를 늘리는 건 제가 어떤 의미하고 있었죠? 러비를 늘리는 건? 피처를 늘리는 거. 맞아요. 저도 있고 피처를 늘려봤는데 이렇게 생각하시다. 개 측면로 가질 수 있는 피처량을 늘리는 거. 러비의 조정. 그러면 러비를 늘리는 거는 사실 개 측면로 따라가야 하는 거겠죠. 전체적으로 모델 크기를 넓힌다는 관점 보다는 개 측면로 넓히는 걸 결정합니다. 이 러비스케일링의 종의 개 측면로 가지는 모델의 개 측의 너비를 증가시켜주겠다. 두 번째로는 뭘 해야 되나? 그렇죠, 좀 더 깊게 해보면 개념을 이해할게요. 여기서 우리가 아직 CNN의 반대옆이기 때문에 이 부분에 대해서는 그냥 간단하게 한번 들어보고 그때 또 다시 뜨면 좀 더 정립시겠죠. 이런 표현할게요. 이미지는 지업특기라는 게 필요해요. 그래서 제가 얼굴을 보고 싶으면 눈, 위치, 코, 위치 정해져 있죠. 이 위치를 이렇게 눈이라는 걸 위쪽 부분만 짜다 그러면 아래쪽에 보여 안 보여요. 안 보여요. 그래서 수용명령이라는 거 연정을 넓게 잡아주는 게 좋아요. 원래 숫자 3개도 끝낼 수 있는 거 9개에서 써야 되는 거거든요. 근데 이걸 똑같은 계절을 한 번 더 쌓았잖아요. 같은 개념의 알보리통을. 3, 3 해보니 얘가 5, 5, 5의 사이드를 쓴 거랑 똑같은 개념이거든요. 수식종으로 정리하면 3, 5, 5, 3, 9 나는 거죠. 바로 새로 왔을 때. 그러면 9도 하기 두잖아. 몇이에요? 18개를 쓰는 거겠죠. 근데 이걸 5, 5, 5 이렇게 사는 몇 개예요? 25개죠. 누가 숫자가 작아요? 18개 이거 숫자가 작죠. 그럼 연산량은 누가 더 빠를까요? 18개 더 빠른 사람. 근데 바라보던 영역은 똑같아요. 이런 수업적도 있는데. 그래서 똑같은 색깔은 어때요? 똑같은 계층? 2개, 3개 만들어주고 있죠. 이걸 수용용령 넓게 안 잡아주고 작업할 수 있는 거 같은 거 생각하면 되고. 그러면 그 목적으로 우리가 뭘로 안 되는 거예요. 제가 오늘의 깊이를 깊이 만들겠다. 컨셉이 뭐라 했죠? 압축된 주요 피처를 보겠다는 거겠죠. 정리하자면 내가 원래 3, 4, 2, 3 1개의 스탈라를 만들었는데 이 2개를 통해서 몇 5, 5, 5, 5을 5, 5, 5를 숫자 하나를 바꾼 거예요. 훨씬 더 넓은 몸줄을 작게 잡아줍니다. 압축한 거죠. 그런 게 점이 BP에 대한 스테이크. 이어서 해삼토 스테이딩 같은 경우는 차이가 있는데, 지금은 이런 기법이 있구나 정도로만 넘어갈 수 있어요. 왜냐하면 특수한 기필이 있는데 같은 용만 쓰는 기법이기 때문에 이어서, 그러면 얘들을 따로 따로 쓰는 거잖아요. 따로 쓰는 건데 조합해서 되는 거잖아요. 어떤 상황에는 해삼토도 높게 잡아주고 보면 이때만 보세요 보세요. 특정 상황에서만 해삼토 높게 잡아주고 나머지 똑같이 깊게 쌓았지만 안 좋죠? 선택적으로 쓰는 거죠. 심지어 넓게 하는 것도 여기는 그대로 유지해. 그런데 윗부분 종단에 넓게 해줘 그렇게 할 수도 있는 거겠죠. 모델의 무졸은 자유롭게 튼닝할 수 있어요. 그리고 너비를 깊게 넓게 했다고. 깊게 했다고. 무조건 그것만 정해서 써야 이런 의미가 아니라는 거예요. 그런데 너를 무해서 출발하면 쉬워요 어려워요. 너무 어렵죠. 그래서 뒷볼트 모델을 통해서 출발하고. 그리고 뒷볼트 모델로 좋은 성능을 만들어내기 위한 모델 튼닝을 한 마리 이렇게 기억해주세요. 그럼 대표적으로 우리가 기억하는 건가요. 첫 번째 D&N 구조. 우리가 이발적인 룰러 레트로 한 건 D&N 구조. D&N 구조가 바로 오늘 우리가 실습을 해볼 멀티드의 역할 색소는 구조가 제일 베이직한 D&N 구조가 라고 말할 수 있습니다. 두 번째는 C&N 구조. C&N 구조는 이미지에 대한 특성을 이야기할 때 이미지에 잘 뽑는 모델이고요. 그리고 안에는 별. 안에는 별은 시기열. 순차에 대한 정의 데이터들 주요 피처를 잘 뽑는 모델이고요. 처음으로도 이게 만들어져 있다. 아니죠. 누가 싫어하는 거니까 이게 그런 데이터 특성을 가리기 데이터들의 좋은 점을 다 쏴라. 그리고 혁신. 시콘션한 데이터들 혁신을 만든 철스포모. 위에는 또한 구성을 하니 더 좋아보이는 결머를 줘. 그러면 그 다음 구조는 철스포모로 쓰면 되겠다. 이런 식으로 노조가 만들어온다는 그래서 뒷노밍은 아직까지도 마찬가지지만 얘들은 완벽한 정답을 뭔가 만들어내는 공식이 형태로 높다는 없어요. 그래야지 선행적인 연구를 보면서 좋은 배우들의 모델이 있다면 그 모델을 차용해서 활용하는 기능이 좋습니다. 그래서 첫날에 제가 이름을 안 했었죠. 우리가 AI 파트 쪽에서는 우리 학교 고등학교 시절에 배우는 학문들을 보면서 아이고. 어디 쓰지 나는 모든 학물을 쓰는 게 그렇죠. AI는 왜 결머른 쪽으로 좋기만 하면 어떤 방법이 있습니다. 그래서 스케일링을 하면 따라서 지금 보시는 것처럼 이미지에 일하는 하나의 계획 같은 게 있는데 그 내외에 대해서 성능 지표들이 굉장히 좋아진 걸 볼 수가 있겠죠. 이 그래프들에서 주요한 의미가 있어요. 뭐냐 하면 새로 추고 뭐예요? 네. 성능이 올라간 겁니다. 여러분 다리 물어볼게요. 제일 끝에 있는 모델의 성능이 제일 좋으니까 이게 제일 좋은 모델이다. 이렇게 하면 정확도 관점의 제일 좋은 모델 음이라고 했다면 제일 마지막 모델을 칠 수 있겠죠. 근데 내가 패드리드 관점에서 써야 되는 모델이다 라고 하면 맨 마지막 모델에 쓰면 순간 쓸 수 있다 없다. 무거워요. 네. 무거워서 못 씁니다. 아예 예측이 불가능해요. 이것처럼 상황에 따라서 적절한 모델을 도출하기는 중요해요. 그래서 오늘날 이미 기학습 때 모델들 보면, 요로기업은 예를 들면 요로는 나무 모델도 존재하고 스물 모델도 존재하고 미들 모델도 존재하고 라지 모델도 존재하고 슬아지존 모델도 존재하고 그 모델은 제일 정확도 관점의 모델이다. 제일 큰 모델이 정확도 관점이 좋겠죠. 근데 그거를 내가 기본적인 휴대폰을 올리겠대요. 이거 말이 돼요? 마저 안 되는 거예요. 그리고 모델의 크기가 크기나 무조건 선형적으로 올라가나요. 여러분들 우리 전자제품은 약간 이런 느낌 굉장히 비슷한데 전자제품은 비싸면 좋은 걸 얻죠. 좋은 제품이잖아요. 그런데 하이겐트급으로 가면 돼요. 이게 중적과, TV옷 TV에서 중적과 라인으로 보면 몇 마리 되요? 한 5만원 선해서 10만원 선으로 잡히죠? 그러면 중과 라인이 한 20만원 선에서 50만원 선으로 잡히죠? 그럼 이제 고과 라인이 100만원에서 100만원 선으로 잡히죠? 그러면 하이겐트로 넘어가면 그때부터 단위가 어떻게 되냐면 200선이 된 것처럼 얘가 1000단위 헛단위 오잖아요. 그럼 TV가 어마무시하면 좋아질까요? 똑같은 포켓이 포켓이 8K예요. 2차예요. 이넘이 인가요? 이 이넘이 인거에 대한 내용들이 필요하다고 쓰ashi는 게 많겠지만 모델에 따라서 내 필요성에 따라서 맞추는 게 중요하겠죠. 여러분들은 오히려 이 이넘을 잘 인지하는 게 중요해요. 사실 개발 관점이 개발 관점에서 모델들이 쓰기 시작할 거거든요. 저는 여러분들은 제가 허리가 되서 말씀드리기 있잖아요. 저는 하드웨어 쪽으로 코드 양사인데요. 하드웨어 쪽으로 프로그램을 하는 사람은 무조건 뭐부터 확인하면 하드웨어 스펙부터 걱정되고 이 스펙에서 내가 할 수 있는 거고 할 수 없는 걸 선택한 다음에 코드를 짜요. 근데 소프트웨어는 그런 반점을 가지면 안 돼요. 자유로운 타보를 통해서 모든지 구경할 수 있다는 반점에서 접근을 해야 돼요. 그래야지 다른 사람을 생각하지 못해서 이상적인 소프트웨어를 만들어주기 때문에 근데 이게 뭘로 변경입니다. 하드웨어 스펙이 모델을 적용해야 되는, 여러분들은 이제 써야 되는 거예요. 회사를 들어갔을 때 또는 회사에서 이 모델을 써야 된대요. 그러면 회사 컴퓨터가 중요없어요. 그럼 뭔가 할 수 있을까요? 아이고, 아이고, 아이고. 근데, 회사 컴퓨터가 굉장히 좋아요. 그리고 내일 모델이 서버를 갈기 위해서 이 용자가 되게 본인이 좋기만 하면 쓸 수도 있게 해줄 거예요. 오히려 PC의 모델을 뒷-젓근거보다 뭐를 통신에 대한 룩을 뒷-젓근거보다 더 좋겠죠. 이것저것 목적성을 예한이 있습니다. 여러분들이 모델을 개발한다, 슬픈 이야기지만 오늘날 국내에서 모델을 직접적으로 만든다는 사람들이 거의 없어여요. 슬프지만. 그럼 오늘날에서는 개발 쪽은 어떻게 쓰고, 사용 쪽은 어떻게 쓰냐. 개발 쪽은요. 이미 잘 만들어져 있는 모델을 내 데이터에 맞게 최적화하는 목적을 합니다. 그래서 뒷-젓근거라서 개발 쪽으로 쓰는 사람들은 전의학습을 반드시 하고 하는 것 같아요. 그래서 미리 기학습 때 모델을 이용해서 내 거에 맞게 추가 퓨링하는 파인 퓨링 개념에 대한 년들을 꼭 알고 넘어가셔야 되고요. 사용만점이라면 내가 프로듀서의 문제의 목적에서 이 AI 모델을 어떻게 쓸 건지는 생각해야 돼요. 제가 군대에서 교육들을 진행했을 때 군 장조들의 교육을 진행했을 때 그 쪽의 군무원, 특사 조론의 부문, 그 고대생들. 물어봤던 게 뭐냐면 프로듀서 인젝션이라는 개념인데 일종의 원인은 요즘에 채팅끼대로 굉장히 많이 쓰잖아. 예를 통해서 조금 더 편하게 데이터에스 뵈을 관리하는 거에요. 예를 통해서 명령을만 인역하면 거기에 데이터를 관리하지 않겠죠. 데이터에스 뵈을 보여. 굉장히 중요한 얘기를 바꿔 있잖아. 그러면 이제 해킷하는 사람들, 뭘로 써. 최대한 돌아돌아서 그 속의 내용을 못 얻을 수 있게 하는 장면이겠죠. 이런 것들의 예정에 그런 것도 인젝션이라고 이야기하여 그거는 프로듀서 전문가가 잘 하는 게 아니라 오히려 문백적으로는 내용들을 잘 유출해서는 잘 해요. 그거를 어떻게 하면 디펜사일까. 이런 논조들을 이야기하거든요. 것처럼 내 목적성에 사용해야 될 모델과 그 모델을 어떻게 써야 될까. 그러면 문제가 무조건 발생하라겠죠. 내 표적으로 요즘에는 거의 대본 이 채팅비디 관련한 형태들의 만들어져 있는 라진 헥기지 모델을 이용해서 그 모델이 자체적으로 존재하라는 것들과는 해요. 우리나라에서 LLM 모델들을 연구하고 있는 기업들은 네이버나 SK나 삼성 L지 이것저것 4개업 말고는 불가능해요. 얘들은 어떻게 쓰느냐. 네이버 같은 경우는 자기들이 만든 LLM 모델을 솔루션기반에 스타트업들을 이용하겠죠. 얘들은 이 LLM 모델에 질문을 하겠죠. 질문을 연동을 할 건데 제공자가 질문을 하면 이 LLM 모델을 쓰는 게 아니잖아요. 그리고 순전으로 메인터가 공개되면 안 되겠죠. 그러면 그을 위해서 청분들을 한 게 더 만들어요. 보안 작업을 위해서. 이것처럼 목적성을 맞게 모델을 인지하고 사용하는 게 목적고 그다가 그 LLM 같은 경우는 메인 시스템에 대해서 구성을 한 게 되겠지만 이미 만들어져 있는 솔루션을 기관으로 뭔가 작업을 하겠죠. 내 반경에 맞게 어떤 목적에 맞게 모델을 쓸 건지는 도 모여하면서 접근을 해줘야 되겠죠. 이어서 우리가 앞서 각각의 요소들을 한다는 것들은 모델 반적이라고 한다면 이번에 나는 이제 시즈모이드 엉슈한의 대응기는 로스에 대해서 하라고 하더라고요. 사실 우리가 회의에 대한 대응은 로스는 없더라고요. 그냥 오차가 다 더 안 나오면 평균을 하면 계산할 수 있었잖아요. 그런데 블루는 뭐예요? 결과까지 뭘로 나와요? 황류로 나온다는 거. 이 황류로 맞나 틀리냐 하는 걸 생각을 해야 되겠죠. 그런데 우리가 시즈모이드 만점으로 생각을 해볼게요. 시즈모이드는 무슨 블루기를 만들 때 쓰는 녀석이에요. 이제 블루기를 만들겠죠. 그러면 발생할 수 있는 경우에 쓰는 뭐와 뭐가 있어요. 그렇죠. 여기가 지금 확건이 일어날 황류로가 일어나지 않을 황류로겠죠. 그럼 일어날 황류로 맞출 수도 있겠지만 틀릴 수도 있겠죠. 전혀 반대로 일어나지 않을 황류로 해서 뭐가 맞출 수도 있고 틀릴 수도 있겠죠. 이걸 다음번에 보면 되겠죠. 같이 봐야 되겠죠. 맞아요. 같이 봐야 되겠죠. 그래서 심블레이어 낳았던 것들을 하는 게 아니라 로스라는 부분을 잠깐 볼까요? 코드에서. 로스라고 쓰는 부분 보면 제가 예측과 와인을 봤죠. 그리고 실제과 와인을 비교하죠. 그런데 거기에 로그를 뭐 하면서 나타났는데 그 똑같은 수식이 반대쪽에 어떻게 되었어요? 1-Y 또는 1-예측값으로 써놓은 거 보이시죠? 이게 뭐냐? 바로 크로스 엔토비 로스를 포출합니다. 지금 보이듯 동시에 동무의 지수형이 있기 때문에 항소 양 끝부분에 그라겐트 포화가 발생하는 데 그런 사슬이 준다는 겁니다. 다만 지금 보이듯 동시에 출력 계층을 사용할 때 크로스 엔토비 로스를 이용한다면 1-Y 구간에서 그라겐트 포화를 다해서 지키지 않게 만들 수 있어요. 양격다리 말하기가 많은 이 포화를 방지해서 오차에 대한 경기를 계산할 수 있다는 거죠. 그래서 시그널 보이듯 동시에 지수형이 크로스 엔토비 로그 암소랑 상대하면서 소프트 플러스 암소 변형이 됩니다. 쉽게 말씀드리면 얘가 플러스, 얘가 오파이 얘가 나누기가 원유가 원유가 원유가 원유가 원유가 상추되니까 기본적인 내륙값 때문에 노출되겠죠. 이런 식의 연산설로 나타내는 소프트 플러스 암소를 기반해서 양수 구간에 그라겐트 포화를 방지시켜 지키죠. 이 크로스 엔토로프라는 형체들로 양쪽 값을 대하는 경우에 그라겐트 포화가 방지되기 때문에 전체적인 데빗을 노출할 수 있게 되겠죠. 그래서 보시면 마이너스 로그 레알 값들의 Y 값으로 어떤 주식이 있을 거예요. 이거를 수학적으로 다 정리할 필요는 없어요. 그냥 기계방지하고 주세요. 일어날 확률과 일어나지 않은 확률의 틀린 정도를 계산했다가 정리하자면 우리는 일어날 확률과 일어나지 않은 확률을 통해서 5차를 계산한다고 합니다. 정돈으로 생각하십시오. 거기서 우리가 지금 잊은 분류기 때문에 뭘로 분류냐고요. 단일의 플러스 엔토로프를 연산한 거겠죠. 그러면 여기에 다중 클래스 관점으로 적극되면서 다중 클래스 관점은 몇 개의 용서는 내가 하는 거예요. 클래스 게스트만큼 여러 개의 사건이 일어날 거냐 안 일어날 거냐 이렇게 생각하겠죠. 기원자가 얘기해 주셨나요. 그럼 크로스 엔토로 우리가 일어날 확률과 일어나지 않은 확률에 대한 영향으로 5차를 계산했다면 그걸 뭐해 주는 여러 개 다 계산했죠. 크로스 엔토로도 순자하겠죠. 그래서 대표적으로 여러분들의 기억에선 도표는 첫 번째 대반적인 크로스 엔토로. 근데 우리가 어떤 분류를 푸까다 알아봤는데 잊은 분류를 푸까다 하면 마이너 리 크로스 엔토로피 라는 도수를 적용해 주시면 그때 보여요. 이어서 다중 클래스 분류가 필요한 그냥 엔토로픽이 아니라 카테고리컬 크로스 엔토로픽 를 비교해 주시는 거예요. 개의 대단히고 월요일 연산한다. 5차를 계산한다. 그럼 이렇게 여기서 회의기에는 디폴트로 로스펀빵션은 모스자. 그미스테어 X. 블루는 잊은 블루라고 한다면 모스자. 역시 5년에 크로스 엔토로픽 마이너 리 크로스 엔토로픽 다중 클래스 분류다. 모스자. 카테고리컬 크로스 엔토로픽 를 이용해서 5차를 계산한다. 그래서 보시면 지금 본 걸 J세파 라고 이야기하고 있잖아요. 이게 뭐예요. 복잡한 주차. 그래서 로스펀빵션을 통해서 로브드 플러스를 통해서 로그들이 연산을 하는 거예요. 이런 연산을 통해 이상적인 정답을 도출하겠다는 것들로 활용합니다. 양수만을 활용하는 이런 활성 암주에 대한 연구 으로 시즌을 도출하는데 이런 경향성들을 이용해서 각각 진동 패턴들을 안성화 지켜주는 렐루에 대한 연구를 예약해야 합니다. 액티베이션 펌션에 대한 연구를 조금 이야기하고 있겠는데요. 여기서 이제부터는 이제는 액티베이션 펌설도 미분을 해야 한다. 알고 있잖아. 그럼 이 미분성에 대한 연구로 미분이 가는과 불가능에 대해서 조금 이야기하고 있습니다. 미분이 불가능한 액티베이션 펌션. 식연방에서 학습을 하려면 미분이 가능한 데트레이션 펌서를 사용해야 합니다. 하지만 렐루 같은 구간 선영방술은 구간이 바뀌는 지점에는 미분이 된다고 합니다. 안 됩니다. 식연방을 근사 방법으로 학습을 표현하기 때문에 약간의 미분 호용에도 결과에 있는 영향을 준다 아닙니다. 맞아요. 그래서 이렇게 이 두간에서 어? 이거 문제된 거 아니야? 변화 이렇게 잘 안 되는데 라코가 더 내의 영향이 100%다? 아니라는 거죠. 어느 정도 호용에 준비를 합니다. 그래서 렐루 펌서를 문제될 게 없는 거. 그 대신 스테방술은 안 되겠죠. 스테방술은 그냥 없어지지 않아요. 그렇기 때문에 하려고 못 하는 것 같아요. 자, 이어서 이 식연방의 입력 데이터가 들어왔을 때 어떤 출력을 만들어야 할지는 정하는 규칙을 감수적인 관계로 표현을 합니다. 마치 이 부품들이 조립되어서 복잡한 기계를 구성하는 것처럼 식연방의 요소는 감수들을 어떻게 맵핑하는지 따라 바로 따라 부를 수 있어요. 그래서 아까 우리가 용어적으로 뭐 했어요. 시암수로 백터 암주로 만들고 백터 암주로 또 다른 백터 암주로 만들어서 유럴렛도업을 정리한다고 했죠. 그런데 이 적일을 왜, 우리가 전체적으로 뭘 필요가 있을까요? 오케이, 쩌. 최소한 바지를 제대로 만들어 놓는다면 전체적으로 조언을 조언 말해주는 거잖아요. 가중화성과 활성암수의 연결성으로 유럴을 구성하고 유럴을 모아서 개칭을 구성하며 개칭이 모여서 식연방을 구성하는 형태로서 훨씬 더 복잡하지만 이걸 단순화된 개념으로 복잡한 식연방을 만들어낼 수 있다는 걸로 이러한 유럴렛도업은 개칭 구조 자체가 식연방 함수적 맵핑 관계를 만들어냅니다. 자, 그러면 지금 이러한 만점에서 제가 데이터 인스턴스랑 하나 전달을 따로 데이터 인스턴스가 4가지의 피처들을 전달한 거죠. 이 4가지의 피처들을 통해서 내가 정답을 도출하고 싶다고 예약을 해요. 거기에서 일종의 인디켓 측에 악습을 했더니 이런 식의 가정치를 통해서 이상적인 정답을 만들어서 결과가 도출되면 혹에 따라 모델이 만들어서 따라서 말해 볼 수 있는 거잖아요. 이러한 하나의 형태들로서 우리는 뭘 찾는 게 최우선입니다. 이상적인 모델, 최적화된 모델을 해볼 수 있는데요. 이러한 최적화만점에 대한 내용들로 이거더들을 이야기해 볼 거예요. 사실 방정식을 통해서 회를 정확하게 구한다. 이게 현실적으로 정답이 있으면 완벽한 정답을 나오는 공식이 존재하겠죠. 그런데 일반적인 방안에서 우리는 모든 형태들을 해야 하는 의미를 다 봐야 하는데 유한한 정도로서 결과를 나타낸다는 것은 완벽한 정답을 많이 줄 수 있다. 없겠죠. 쉽게 말씀드리면 무한한 게잇과 집합을 완벽하게 맞힐 정답을 주고 싶은데 나는 몇 개밖에 없다. 유한한 게 수밖에 없는 거예요. 그걸 가지고 맞추겠다 그러면 완벽한 정답을 찾을 수 있어요. 없겠죠. 그래서 근사적으로 회를 구하는 방법을 접근한다는 거죠. 다양한 재학적들을 만족하면서 목적함수를 최대화하거나 최소화한 회를 반복적으로 조금씩 접근한 방법으로 이야기하는 게 최적화다. 이 최적화에서 인지하는 굉장히 중요한 것은 최소화 문제와 최대한 문제의 관계성을 이해하는 게 중요해요. 최소화 문제로 정의할지 최대한 문제로 정의할지는 문제를 잘 표현할 수 있는 방법으로 선택하는 게 중요합니다. 이거는 최세한 문제라는 거죠. 이렇게 설명하는 거죠. 많이 맞추는 거 다른 목적은 없죠. 최대한 많이 맞추는 목적을 합숙하면 최계값으로서 합숙을 하는 거겠죠. 최소화의 관점에 대한 문제는 뭐야. 최대한 적게 틀리는 관점을 이야기하는 게 좋죠. 그래서 몸의의 안정태로 이야기하면 최소화 문제로 정의할지 최대한 문제로 정의할지는 최소화 문제는 최대한 문제는 어떤 형태들다. 1.1봉자의 양면이 어떨까. 그러면 우리가 분류 문제로 생각을 합시다. 분류 문제의 어큐로시는 뭘 추려 하는 게 제일 좋다고 어큐로시는 1일 잘 맞추는 거고 100% 맞추는 거고 근데 이걸 차관적으로 설명을 해볼게요. 작을 수 없죠. 이것처럼 뭘 기준으로 잡느냐에 따라서 논조가 다르다는 거예요. 동차례 양면성이라서 얘들 다치가요. 하나가 정기되면 하나는 사실 따라옵니다. 정기됩니다. 그래서 일반적으로 주식적인 연산이 쉬운 개념으로 적그라는 거. 제일 쉬운 개념은 어느 정도로 틀리냐가 최소화하는 반점에서 접근 안정대를 쓰는 거예요. 오차를 최소화도록 학습하면 정답을 최대치로 만들 수 있다 없다. 그럼 적기 있다. 같이 따라오는 거에요. 그래서 주식적으로 이야기하면 우리는 학습의 최적화 문제는 8m가 최대한 결과값에서 최소화가 될 수 있는 문제점을 만들어낸다가 이 목표점에 대응되는 것으로 주식 연산들을 만들어내는 거. 이게 오차의 대항이에요. 오차가 최소화대로 가는 주식이었으니 그러면 블루 발점에 최소화하는 문제입니다. 아까 블루에 오차를 계산한 게 뭘 통해서 계산한가 했죠? 프로센트 블루 잔차는 이렇게 해. 그래서 블루 같은 경우들은 오차가서 노출하는 프로센트 퀄리티의 값을 이용해서 최소치료가 되는 거. 에 대한을. 그래서 우리가 이 최적화는 저는 학생분들한테 이야기할 때 이 그림을 꼭 가지고 오거든요. 이 그림이 진짜 설명하기 제일 좋아서 이걸로 얘기해봐요. 우리가 매번 단순하게 그리면 늦었지만 실질적으로는 데이터를 통해 초평화를 만들면 여러 웨이터들을 가지고 차원 초평화를 만들었습니다. 로스 공간. 근데 결론적으로 말하면 제가 지금 뭘로 접근하고 있어요. 오차로서 접근하고 로스부터. 우리가 원하는 거예요. 이 만들어진 초평화에서 어느 위치를 잡고 싶어요. 오차가 제일 적은... 제일 낮은 곳. 그러면 여기서 굴리든 여기서 굴리든 우리는 어디든 어디를 가야 되겠죠. 이 장소로 가야 되겠죠. 근데 굴리다가 빨리 떨어지면 정말 좋겠죠. 근데 굴리다가 이쪽으로 떨어져서 뭔가 골짜이 같은 거 들어봐 버렸죠. 여기서 못 나오실 수 있겠죠. 기훈이가 쟤라고 하면 얘가 올렸어요. 로콜리 미니웜을 맞히고 그랬죠. 그래서 여러분들이 많이 헷갈리거나 할 때 이 근육을 떨어졌으면 좋겠어요. 그러면 다 뭔가 좌우로 양 방향으로 상하로 굉장히 복잡한 모양이 만들어져 있겠구나. 근데 우리는 최종적인 목표는 글로벌 미니웜을 고치러겠다. 그래서 앞서 우리 오전을 해야 될 것 처럼 이 글로벌 미니웜을 갈 수 있는 이 언덕 같은 산에서 내가 갈 수 있는 건 모르겠어요. 두 가지 방법으로 옥기 마이저가 발전을 했다 했죠. 첫 번째, 속도. 두 번째, 정확도관적으로 적극내려야 되겠죠. 속도관점이라는 건 뭘까요? 이쪽에 가속도방측을 적극나는 거예요. 바로 직전에 굉장히 빨랐어요. 그러다가 갑자기 제로가 됐어요. 그럼 거기서 먹으면 출면 될까요? 기울기가 제로가 되는 구간은 정답을 찾은 글로벌 미니웜 글로벌 미니웜은 수도 있지만 뭘 수도 있어요. 로컬 미니웜은 수도 있죠. 이 상태에서 내가 분명히 다음 스테일에서 여기에 기울기가 제로가 되는 순간이 왔어요. 그 순간을 딱 멈춰버려요. 원래 경사먹은 거잖아요. 그러니까 고너 순간 기울기로 속도를 결정한 게 있으니까 근데 아까 부분은 굉장히 가파라잖아요. 그 상태에서 내가 다음 스테일이 오더라고 이전 속도를 반영합니다. 지금 추가되는 속도를 몇일? 0이 있지만 과거 속도가 있으니 그걸 적용해 주는 거겠죠. 이걸 그대로 적용하면 될 것 같아요. 현실에 생각하면 돼요. 우리가 어때요? 앞에 가속도가 어떻게 되냐 속도가 좀 줄지만 앞에 속도가 그 원리를 이용하면 제가 말하는 이곳에 받을까 하면 올라갔다고 다시 내려오겠죠. 그러면 이 정도면 근데 지금 말씀이라고 하세요. 여기 제로가 됐지만 조금 더 움직이기만 하면 내려와요. 살짝 밀어주는 거잖아요. 밀어주면 내려와요. 다시 가슴을 띄워주시면 돼요. 이게 바로 몸의 팜인화면 가속도 없지. 그래서 오브젝트에 대한 영향을 찾든 아니면 중요한 글러블링을 찾을 때 하는 기법의 첫 번째. 속도를 향상시키자. 가속도 고치를 활용해서 속도를 유지하는 게 이게 모멘텀이라는 느낌입니다. 두 번째. 아까 정확도라는 것들을 이야기했잖아요. 정확도. 그러면 지금 현재 제가 이 언덕에 있어요. 지금 언덕이 보니까 약간 일직서로 쭉 되어 있잖아요. 일직서로. 일직서로는 맞죠? 일직서로는 제가 일직서로만 간다면 기울기가 어때요? 완만하죠. 이 장소. 지금 손가락으로 보이는 부분. 보이시나요? 그린에서 완만하잖아요. 그러면 천천히 맞겠죠? 근데 옆으로 조금만 더 어떻게 해요? 그쪽은 골장이잖아요. 그러면 급격하게 변화할 수 있을 건데 학습소도가 금방 계속될 건데 내가 그대로 그냥 일직서로만 가버리면 굉장히 천천히 왔어요. 그래서 일고 흔드는 거. 그럼 어떻게 해요? 막대기를. 앞일 때는 분명히 완만했는데 옆에서 살짝 뜨니까 어때요? 뚝 떨어지는 부분이죠. 그러면 급하게 내려갈 수 있게. 이 두 가지만 쪽으로 학습을 진행시키는 거예요. 별량에 대한 의미. 이 개념을 나중에 배우겠지만 학습률 조정이라고 해야 합니다. 학습률 조정은 여러 가지 입법이니까 그 중에 나중에 제가 도제한 것들에 대해서 제가 도제한 것들에 대해서는 적응적 학습미를 일하는 개념은 꼭 기억하셨습니다. 학습률 조정은 많습니다. 그 개념으로 우리가 디테일하게 다를 겁니다. 신경만에 학습레를 적응을 뭘까요? 목적? 우리 계속 강전이 뭐예요? 목적이. 글로벌 리듬업이 도착하겠다가 목적이다라는 거예요. 이 목적에 따라서 신경만의 로스펑션은 차원이 매우 높고 복잡한 모양을 하고 있어요. 이런 모양이 뭐예요? 모자마자 머리가 아프잖아요. 지금 뚜렷하게 여기 보여서 보이는 거지. 이거 그냥 풍문을 본다면 어디가 제일 라즈든지 찾을 수 있어요? 못 찾겠죠? 제가 이렇게 다를 보고 이 부분이 없다고 볼게요. 이 부분은 추위에 제일 라즈든지 어디 있거든? 5초, 10초, 10초. 이것처럼 복잡한 모양을 하고 이런 건 사실 최적하고 어려웠습니다. 그리고 이런 복잡한 형태 때문에 로스펑션은 하나의 글로벌 리듬업과 함께 수많은 로컬 리듬업을 가지고 있어요. 내가 지금 로컬 리듬업을 한 개가 아니라 여러 개가 있는데, 어느 장소에 빠질 따로 딱 집중하지 않는다는 거예요. 글로벌 리듬업에 대한 분들은 함수 전체에서 가장 낮은 곳인데 로컬 리듬업 같은 건 그냥 제로로서 만들어지는 위치라서 사실상 로컬 리듬업은 로스펑션의 수위 만 있습니다. 오마모시한 양이 많이 생겨요. 뿐만 아니라 이기만으로 이야기했지만 맥시멘도 마찬가지입니다. 높은 위치가 걸릴 수 있겠죠? 여기서는 이렇게 몇 일요? 그렇죠. 그럼 멈출 거죠. 이것처럼 이러는 구조들에 대한 유지인들을 안장점이라고 불러요. 안장 모양이 어떤 냐면 옆에서 부분으로 파여 있어요. 여기가 타야 됩니다. 위에서 보면 말에 골고루 나타내려는 이런 식으로 이렇게. 그러면 높은 지점에서 기운이 제로로 되어 있는 거죠. 낮은 지점에서 기운이 제로로 되어 있는 거죠. 이걸 묶어서 안장점이 됩니다. 이란 쪽 성대를 따라서 최적화 알보리즘은 사실 종교가 많다 했죠. 인너리의 신경망에 대한 영웅들의 기본적인 논조는 미분이에요. 미분을 하기만 하면 오차를 계산할 수 있다는 거예요. 왜? 그래서 대표적인 미분 방식이 1차 미분, 1.5차 미분 2차 미분에 대한 영웅들로 최적화 개념이 존재되래요. 내용만큼 경사견법, 중위욕법 퍼레, 유통방법, 내부선명 이런 것들이 있는데, 복잡하게 디테일을 하지 말고, 우리는 뭐만 기억하자. 그 내시, 1차 미분 건 기억하자. 그래서 나중에 1차 미분 건 같이 이야기할 거예요. 얘네들 또한 남사까지입니다. 첫 번째, 기본기시전 그라델 디센트. 알잖아요. 뭐예요? 기본적인 경사견법, 그라델 디센트는 그냥 경사를 내리는 영웅들이죠. 거기에 확률적이고, 뭐 하는 거예요? 샤프하는 거, 그 다음 아까 말했죠. 멈추면 다 속도 버치고 싶으면 있다 했죠. 오멘터, 그리고 아다드리브, 얘가 뭐냐는 적응적 합슨이를 조금 시킨 경사견법기법, 그리고 안쪽, 얘는 적응적 합슨률에 대한 기법을 좀 더 강화시킨 적응적 합슨이. 조기값도 잡아주는 건데, 그 다음에 이제 아다미를 하면, 이 모멘터와 아다드리브 가지고 있는 적응적 합슨률을 개선시킨 아다미입니다. 그래서, 두 가지 방향으로 스백 경사견법을 만들어내는데 거기에 뭐까지? 톡톡까지 반영해서 조금씩. 그 다음에, 뒤에 또 배우는 적응적 합슨률이 내 테르프 모멘터 미랄이 존재하되요. 이 내 테르프 모멘터는 오버슈팅을 감지하기 위한 것인데 그걸 어디에도 아다미도 적응시켜서 나담이라고 부르는 옵티바이저 지는 존재해요. 그리고 옵티바이저 또 더 있겠죠. 자, 그것도 제가 표정리해 놨을 나중에 교재내에서 보면서 이야기할게요. 자, 그런 것들처럼, 물론 중으로 말하는 우리는 뭘 찾겠다는 목적이에요. 경사 하강법을 이용해서, 글로벌 미니멀 찾겠다, 이것만 기억하는데요. 그 대신, 무슨 제약점이 많다. 로컬 미니멀에는 장애물이 많다. 우리 이 장애물을 극복하기 위한 기법들로 여러 희망할 절 쓴다. 이 계단으로, 제작적인 장리를 잡아보도록 하겠습니다. 신경망에서 사용되는 경사 하강법을 이용하거든요. 우선적으로 백프로방지에 대한 일들과 눈치 가실 것 같습니다. 꽃차가 계산이 돼야 돼요. 그래서 목적함줄이 기반해서 만들어진 꽃차를 이용해서 웨이트에 대한 경신을 놓추라고 합니다. 그 웨이트에 해당되는 부분들을 추정하는 기능들을 지금 안에 경사하는 법에 대한 일들로 이야기하여 있습니다. 사실, 파란색도의 업데이트는 경사하는 법을 이용해서 가중치를 업데이트하면서 먼저 로스 펑션에 대한 가중치를 입은하고 업데이트 시기에 따라서 각각의 스태프를 변화시키는 것을 이야기합니다. 그래서 동일하게 정리해서 이야기하자면 과도, 웨이트, 미래, 웨이트, 형태의 배턴들을 이 때 적용되는 학습률 기울기값로 정사하는 법을 한다. 연쇄업체는 통해서 미분을 예상합니다. 백파라바케이션. 그래서 이 미분값은 합성함께 미분을 표현할 때 합성함께 시대유 순서에 따라 각각 미분을 오픈합니다. 단계적으로 미분을 하면 된다는 것입니다. 체류를 통해서 미분에 대해서 도출되어 있는 수식을 미분하고 수식을 미분하고 수식을 미분하고 최종적인 결과에 대한 정도로 목적감수들을 정립할 수 있게 되어있습니다. 이러한 연사를 통해서 고쳐나가려고 하는 목적성이 굉장히 중요하다고 말하고 있습니다. 여기까지는 제일 첫 번째 웨이트 각점에서는 구조가 달라질 게 있을까요? 구조 말이라면 악출력에서부터 제일 첫 번째 1번 레이어까지 와요. 그런데 그 레이어가 웨이트가 몇 개예요. 여러 개죠. 그리고 웨이트 앞에까지 역으로 갔잖아요. 우리가 입력으로서 충전과 가면 이전 계층에 출력이 그 다음 계층에 입력이 되잖아요. 역으로 돌아갈 때는 마찬가지겠죠. 이전 계층에 출력이 이후 계층에 입력이 되겠죠. 그러면 그 앞에까지의 봉식은 달라질 게 있었어요. 제일 우리 계층에 달라질 게 없죠. 그래서 이 중요합니다. 백프로포기에선 단계로 이미 도출된 결과들을 오페주기만 하면 되거든요. 그러면 학계층에 가면 도출되면 그 다음 계층이 그 계층에서만 웨이트1, 웨이트2, 웨이트3 따로 계산해주면 정답을 계산할 수 있다라는 걸겠죠. 이 원리가 백프로포기에서나 백신이에요. 그래서 첫 번째 계층에서 이거 연산됐으면 다음 주에서 또 연산할 필요가 있던데 없어요. 그 이 계층에 속도 미분했어요. 만들어졌죠. 그다음 여기 연계되고 있는 모든 애들은 이거 그대로 쓰잖아요. 그대로 쓰기만 하면 되잖아요. 이런 특성에 따라서 신경망의 경상 방법은 로스에 대해서 그대로 쓴다. 정도로 밀착연기볼를 하겠습니다. 자 이어서 분시였다가 다시 반 수냉을 해보도록 하겠습니다. 2. 3. 4. 5. 6. 7. 8. 9. 10. 11. 12. 13. 14. 15. 16. 17. 18. 19. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 20. 이거 좀 개인적인 거라서 네? 좀 개인적으로 좀 심각해서요 멘탈이 좀 아픕니다 네? 상황이 좀 어려워서 수업은 괜찮은데 개인적인 상황이 좀 많이 어려워서 이 부분은 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 좀 더 심각해서 지금 막 역파력으로 미국값들의 계단에도 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 더 좀 1이잖아요. 1이잖아요. 1이라고 적겠죠. 뭐예요? 모르게 하면 되죠. 제일 위에 보면 디즈위치에 보면 1은 모르게 하면 되죠. 층숫. 그렇죠. 층숫. 몇 번째 층. 첫 번째 층. 웨이크인데 엠 엠 번째. 웨이크. 라오마 레오. 이걸 보자마자 우리는 인력층은 몇 개의 유론을 가졌다고 하라고 합니다. 인력층은 몇 개의 유론을 합니다. 인력층은 n. n. n이거든요. n. n이거든요. 그러면 이제 출력층의 기술은 그 첫 번째 레오의 출력층의 기술은 n. n. 유론을 가졌다. 나온 말에 볼 수 있겠죠. 이 계층에서 이제 조금 더 열고 올까요. 가장 먼저 5차를 노출할 수 있는 거는 어디로 노출할 수 있겠죠. 출력 위치에 노출할 수 있겠죠. 그럼 이 소식상에서 예약을 해보면 그러면 우리가 노출을 해야 하나 노출을 해야 하는 건 y라는 값은 이게 칠값이 이랬죠. 그리고 칠값에 대한 연구로 y를 기점할 수 있겠죠. 그러면 y를 칠값이라고 이야기할 수 있고요. y 프라임을 칠값이라고 표현을 할 거예요. 그러면 이 뺄 셈이 곧 모르는 게 하는 거예요. 맞아요. 5차죠. 그런데 한계죠. 그러니까 시그마로써 i는 1에서 6 더 x까지 라고 표현을 할게요. 그러면 이 연산에 대한 내용들을 표현하게 되는 거겠죠. 그런데 우리가 이 회기라는 100원에 대한 5차를 이야기한다면 평균에 대한 내용들을 계산하겠죠. 그때 노출하기 위해서 제고 5차를 계산하잖아요. 여기까지는 기원을 이야기해 주셨나요? 좋아요. 그 상태에서 내가 x개수 이걸 깔릴 수 있겠습니까? 그 계수는 마냥하지 않으세요. 그리고 5차에 대한 제고 7차를 연산해 주려고 하겠죠. 그런데 매번 열어본 적은 누난도 있잖아요. 제가 추신대에 보면 꼭 2분의 1이 하나 있었잖아요. 네. 맞죠? 이유가 뭐냐. 이 2분의 1이 없어도 돼요. 그런데 붙이는 이유는요. 바로 이 1대로. 이 5차를, 제고 5차를 비분하잖아요. 그럼 이 1라는 것이 어떻게 될까요? 내려와서. 내려와서 곱샘하고 그 다음에 수식으로써 y에 대한 내용도 나열 되겠죠. 그러면 이 1라는 숫자는 연산을 계속 들어갈 때문에 오히려 수식을 복잡하게 만들어야겠죠. 우리의 목적은 완벽한 정답을 맞추겠다고 아니라 근사에 대한 정답을 맞추겠다고 목적이고요. 그래서 제고비 없어지면 훨씬 더 계산이 용이하게 되죠. 그래서 연산할 때 2분의 1을 붙여주는 거예요. 그리고 제가 이제 데이터들을 이야기할게요. 이 데이터가 전체적으로 x라는 기계를 쓰고 있기 때문에 제가 k라고 좀 할게요. k, k에 데이터 인스터드가 있다라는 거예요. 그러면 이 집교에 대한 영국이 2분의 k, k를 연산을 준비해야죠. 그러면 win square L. 어떤 영국이 되죠? 그러면 일단 시그마에 i의 y는 이로서부터 k까지 j, i, i 또 인력을 주면 이게 출력에 대한 영역에 목적함수다. 라고 정리할 수 있겠죠. 이 목적함수에서 누구로 미본을 할 수 있어요? y로 미본을 하면 미본이 가능하죠. 그래서 chain을 이반에 y로 미본을 하는 거예요. 이것 좀으로 이야기해볼게요. 그러면 이렇게 보겠습니다. 예측값과 실적값에 대한 영율로 연산을 해서 왜냐하면 미본값에 대한 영율로 차수들을 이야기하는 거니까 예측으로 넘어가게 되겠죠. 미본들로. 그러면 fm에 대한 영율들이 때문에 이렇게 정리해오면 y에 대한 영율들의 제고 minus e y y5 이라고 볼 수 있겠죠. 그리고 y5에 제고 이라는 걸로 연산 형태들을 이야기하고 있잖아요. 여기까지 키워드 얘기하셨나요? 정리한 거니까. 이걸 이제 미본할 거예요. 미분한 자라로 물어보면 각자 값에서 미본을 해서 여기에 이제 이거겠죠. 전체의 m. em에 이런 식으로 주식이 나야 되겠죠. 여기서 미본을 하는 순간 y를 기준으로 미본을 하면 예의 y는 않겠죠. y 값은 그대로 있고 나머지들은 minus e y5를 보고 y'3이기 때문에 지워지겠죠. y-e y5이라는 것들이 값을 만들어지겠죠. 이수식에 대한 영율로 연계해서 공유한다면 m. 이런 식으로 결과가 하나가 만들어질 수 있겠죠. 이 결과 값을 이제 보여주면 돼요. 대인만 하면 결과가 도출할 수 있게 되겠죠. 첫 번째 이 결과가 만들어진 분으로 위해서 종단의 결과들 화색해 줄 수 있게 되는 거예요. 그리고 이제 두 번째로 써 이야기하겠습니다. 그러면 이미 결과에 대한 영율를 뽑았기 때문에 상관이 있어라. 없겠죠. 마지막에 고패주면 되잖아요. 그 값을. 그럼 이제 두 번째 예측에 대한 영율로 인가도 해봅시다. 그럼 아까 말했듯이 우리는 j이라는 거에 일본의 형태들에서 weight로 미문을 하겠다는 건데 이걸 이제 전기해서 chain으로로 만드는 거잖아요. Chain으로로 만들어서 이야기할 거기 때문에 j에다가 y로 미문했던 것처럼 y를 올려주고 y의 수식이 뭐예요. activation function에 z 넣은 거잖아요. 맞죠. 이걸 이 시그멍이공선로 예를 들어 어떻게 할 점을 드리면 이렇게 볼 수 있겠죠. 이걸 z로 미문을 하는 거예요. 그럼 뭔가 미문을 하겠죠. 그럼 미문을 한 번 더 한 번 더 만들어지겠죠. 미문에도 당연히 결과까지 만들어 진다는 거잖아요. 자, 그라디오에서 z뜨는 뭐예요. z뜨는 wxu wxu wxb 라는 공식이죠. 이 공식을 내가 뭘로 써. 그 상황에만 미문을 해주면 되지 않아요. 맞나요? 근데 여기서 중요한 게 이 z뜨는 공식을 그렇지만 여기 뭐가 적혀 있어요. 재고비 아니라 그렇죠. 이건 헷갈리면 안 돼요. 왜 그러냐면 u-life-work에서는 구조들에 대한 개층적으로 하시기 때문에 이게 많이 헷갈리죠. 주식으로 들 때 이거 재고비 아니예요. 잠깐. 우리 처음부터 말했죠. 지수 중에 보는 게 재고비 아니라 뭘 의미가 있나. 데이어 측으로 되게 한다. 이 유머는 두 개 측으로 만들어드리던 이 키트 측을 하는 이 미문이에요. 그래서 그러면 이때 두 번째 측의 인력을 무슨 값이에요? 두 번째 측의 인력. 다음 의인. 네, 다음 의인. 다음의 출력으로. 그거보다는 전 레이어츠의 출력이라고 볼 수 있겠죠. 그래서 웨이뜨 측, 첫 번째 측에 대한 내용의 가설한 숟가락. 이 남남라고 라고 말할 수 있겠죠. 이게 무슨 말이냐면 h는 뭐예요? 액티베이션 펑션의 z2번을 빌리냐는 얘도 액티베이션 펑션의 시그모이드라고 가정하면 아웃보즈 대한민국이 약간일 수 있겠죠. 그렇게 해서 못 추네요. 그럼 이제 그다음 h는 뭐예요? 그.. 그.. 그.. 그 다음에 z 똑같죠? 웨이뜨, x, plus 3인게 그럼 미문을 하겠죠. 여기가 미문이 이기어드셨나요? 그러면 이골 쪽까지는 어느 측 6월 레이어로 보면 6월 계층에 대한 관계의 의혹일 때 첫 번째 측에서 전달된 내용들 이 구조까지 6월이 여러 개 있다 하더라도 이 6월에 연결되어 플리카 느낌 레이어리 때문에 전부다 영향성을 하자. 가죠. 근데 수식적으로 놓고 보면 연독적으로 거쳐온 거 보기 때문에 이 내용이 바뀔 일이 있어요. 왜냐하면 1번 6월, 2번 6월, 3번 6월, 4번 6월이 이때 그때 존재하는 웨이뜨를 배제하고 나머지 웨이뜨가 똑같아요. 그렇죠. 똑같다. 이게 행시네요. 백포방의 행신을 바르고 내가 지금 웨이뜨 고칠 건데 그 웨이뜨가 전층까지는 바로 그 웨이 직전까지는 모두가 똑같은 결과가 가능하네요. 이게 행시네요. 행시네요. 원래 주로라면 제가 유론을 볼게요. 첫 번째, 아웃프, 두 번째, 아웃프, 세 번째에 대한 결과가 이렇게 4개 있다가 가정해볼게요. 그러면 이 선에 따라서 최소로 잡을게요. 그럼 여기서 뭔가 연산을 해야 되겠죠. 여기도 연산을 해야 되겠죠. 맞나요? 최소 잡았다. 그러면 첫 번째, 두 번째에 대한 연산을 해서 결과가 X가 만들어줬겠죠. 두 번째는 다시 이 첫 번째, 두 번째 연산에서 X2가 만들어져겠죠. 근데 생각이 없으면 또 연산을 필요가 있다. 없다는 겁니다. 이해하죠. 그러면 여기까지 연산을 되었잖아요. 이건 그대로 유지하고 그다음에 각각 요소들만 연산하면 연산 양이 없네요. 슬벳적. 이게 100% 파괴 전에 핵심올리. 막연당에 값을 전달한다, 오차에 전달한다, 이런 역전파에 맞고 오차에 역전파에 대한 비건적인 베이지게이냐면 보시는 것과 마찬가지로 전층에 뭘을 찾는 건 공통. 맞아요. 공통. 그래서 이러한 공통 부분에 대한 연산들 통해서 결과들을 만들어 내는 것 이 1차가기 목적이고 그러면 레이어치로 특정 레이어치까지 갈 수만 있다는 그 다음 부터는 뭔가 웅한 연산입니다. 그 해당 웨이트에 대한 연살들만 연산에 추면 그대로 활용할 수 있다는 특징을 가질 수 있게 할 수 있습니다. 벌레특성 이대한 기능을 이해드리셨나요? 이걸로 역전파에 로스트에 대비 유문한 그리고 출력무론은 아웃둑 입안에 전역 유문값들로 가중치들을 업데이트 그리고 업데이트 대는 유문값은 누구로 전달하는 거예요? 은닉층으로 전달하죠. 은닉층에 대해서 액티로이션봉실에 대한 형들 가중학전으로 일어들기 때문에 별도의 리본으로 수행을 합니다. 그래서 당연히 들어오면 소개의 리본에 대한 형의 아웃둑 그리고 각 성결로 아웃둑에 대한 형의 리본인데 이 파란 색깔 보면 되고요. 셋다 파란 색깔 값은 차이가 있어야 되고요. 맞아요. 목소리가 이게 공정으로 대선해서 도출. 일을 도출한 결과 값들로 아시그룹 및 선의 리본들의 연결을 지어줍니다. 그 가중학산에 대한 대한되는 모두로서 아웃둑을 동작시켜준다고 합니다. 동일하게 세 가지 안개의 요소로 대화하시면서 최종적인 웨이트 값들 끼리만 연산에 변신다는 정기를 만들어낼 수 있도록 합니다. 유론관점에서 보면 몇 점파 알고 있는 활동 암술에 대한 논리를 혼신 암술을 통해서 계산하는 원리를 요기해요. 이 계산에 따라서 그 장소 후에 이분들만 적용해 준다라면 통합적으로 할 필요가 있던데 없었다. 그래서 역전파 알고 있는 단계는 이전 걸 하면 그 다음 스텝에서는 얘를 하여흥하기만 하면 연산을 해결할 수 있는 논리입니다. 연산을 해결할 수 있는 논리입니다. 연산을 해결할 수 있는 논리입니다. 연산을 해결할 수 있는 논리입니다. 연산을 해결할 수 있는 논리입니다. 연산을 해결할 수 있는 논리입니다. 연산을 해결할 수 있는 논리입니다. 연산을 해결할 수 있는 논리입니다. 연산을 해결해야 하는 건 당연한 건데 모든 경우마다 다일 필요가 없다고 합니다. 나는 것이 백프로파리이션의 핵심 정례입니다. 개층단계 리본을 만들어내는 실제 그런식이 뭘로 계산한다? 백프로파리이션 배틀스 배틀스 모르잖아. 그래서 백프로파리이션 연산 해서 핵열� 연산을 하는 것입니다. 여전판도 호워디 연산을 하는 것 같이 핵열� 연산을 해서 역으로 뒤로로 하는 것입니다. 이 구조뜻섬에 따라서 연산을 하다 하고 그래서 만들어진 이 백터에 대한 표현서로 유론해서 백프로 표현해서 가슴치가 아디언티 백터를 로표하다면 개층의 가슴치의 리본은 자급위한 행격이라고 이야기하는 매트리스에 대한 형의 아면 수백도 도함조 행결로 연산하는 것입니다. 키네비스 쪽에서 자급위한 연산이 굉장히 중요해요. 엔딩키네비스 연산을 위해서 반드시 본인이 해야 되는데 이 개념이 그냥 쉽게 말하면 과정을 거쳐서 문문한 세종적 결롯 같은 그리고 활성암수 같은 경우는 당연히 아프다는 이야기 했던 것 마찬가지로 애티벤션 검출대학분들이 이용해서 각각의 관측 데이터들을 활용한다 공부로 문제해 주면 되겠습니다. 자, 그러면 우리가 간단하게 나마 모델에 대한 퀴스업들을 이용하면서 활용을 할 건데요. 이를 위해서 일단 부손적으로 모자국을 할께요. 이거는 오늘 쓰지 않을까요. 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 이제 і의 강 precipitation 사 recomend 자,же 여전히 빅러링의 환형 세팅은 우리가 스톱을 위해서는 로컬 세팅을 인식하게 하지만 업무조고 쓸 때는 어떤 환경을 쓰되죠? 도컬한 환경의 작업을 한다 하고 있습니다. 이 프레이머크들이 오늘날 윈도우에 대한 환성은 안 좋아요. 그래서 어떻게 써야 되냐면 윈도우 기반에 대한 동작이 돼야 돼요. 그럼 윈도우가 주변 부스틱을 써야 되잖아요. 그럴 필요가 없어요. 왜냐하면 윈도우 세팅이 너무 오면서 왜 윈도우에 환을 안 하냐면 다체적으로요. WS라고 이야기하는 윈도우 환경에 대한 부족을 만드는 데요. 여러분들이 안 모르겠지만 안에 내부에 인형성이 되어 있는 상태입니다. 그래서 도컬하는 형태들을 이용해서 WS를 설정하는 데요. 여러분들이 쉽게 활용할 기수도 있을 거예요. 지금 우리 수업시간에 그거로 처리하지 않을 거예요. 그렇기 때문에 우전 세팅으로 잡았냐면 그냥 PIB 인스턴트 펜서플로 잡아서 이게 뭘 까는 건 펜서플로 CPU라는 설정입니다. 사실 GPU 세팅을 잡아주려고 한다면 대표적으로 베이직은 코드라인은 여기에 후다 라는 펜으로 설정을 하는 거예요. 그런데 이거 설정 안 돼요. 왜냐하면 여러분들이랑도라 지금 그래픽카드 세팅에 대한 설정은 아니기 때문에 없어요. 또는 그래픽카드가 외장 그래픽으로 있다라고 한다는 연결이 없으면 연동이 안 되는 경우가 많습니다. 그리고 그건 아니라 이 내부에 형태들을 대한민국들을 확인하려고 나타나면 이 정도의 차이가 있어요. 자 그러면 이제 설치가 진행이 되었을 거예요. 이 형태에서 이제 동작을 씻고 놓을 겁니다. 화디는 해보도록 할게요. 그래서 인포도. 펜서플로. 나는 누가 불러와 지는지 하네요. 불러와지나요? 여기서 인형 다른 부분이 아니에요. 그리고 화디는 위해서 펜서까지만 인형하시고 탭 불러요. 화디 목적이에요. 이거 떠요? 펜서플로. 혹시 안 드시는 분 있나요? 아 전 여기까지 잘 떠요 펜서플로. 그러면 기본적으로 펜서플로 환경을 해야 하는 체대는 잘 된 것 같아요. 아무튼 말해서 우리는 펜서플로를 쓸까 하겠지만 메인은 뭘로 써. 그래서 이걸 알기 위해서 캐라스라는 걸 쓸 거라고 했죠. 펜서플로를 서치하면요. 이 안에 캐라스라는 게 자동으로 설치합니다. 펜서플로 R를 캐라스합니다. 그래서 똑같이 캐라스 KDR만 입력하시고 탭 한번 넣으면 돼요. 그러면 뜨나요? 자 그러면 일단 우리가 내일 가볍게 모델들에 대한 공부를 할 때 가요할 수 있는 기본적인 복건이 만들어졌으니 이전부터는 이 백 프로파리이션에 대한 공부를 할 겁니다. 우리가 앞서 무졸레이어를 만들었죠. 싱글 레이어를 만들어서 모델에 대한 공작들로 결과들을 도출해 봤었죠. 맞나요? 그런데 이 싱글 레이어로는 사실상 엑소하리게 하는 문제를 쓸 수 있어요. 없어요. 맞아요. 없죠. 그러면 이런 목적성에서 우리가 엑소하리의 동작들을 구현하는 한점에서 유로의 형태들을 단계적으로 한번 구현해 볼 거예요. 그 대신 내용을 이해하는 게 목적이기 때문에 제가 일부러 플래스드로도 있다는 의견을 안 할 거예요. 단계적으로 생각합시다. 자 그럼 첫 번째 우리가 예측을 할 거잖아요. 그러면 자 엑소하리로 이야기하고 싶어요. 먼저 우리가 박스를 가지고 동작 치기고 활용을 할 거잖아요. 그래서 이 의견을 할 때 표정으로 사용되는 것들이 넌프라이로. 넌프라이로를 이용해서 각들을 기장한 거나 도출해 줄 겁니다. 그러면 모델에 대한 능력들에서 필요한 몇 가지 조건들이 있었잖아요. 첫 번째 인력이 있어야 지금 모델을 도출할 수 있겠죠. 자 엑소하리에 대한 능력과 출력을 한 번 만들어봅시다. 엑소와 y를 만들어볼 거예요. 자 그러면 x를 만들기에 앞서 우리는 어려유를 만들어줄 거고요. 엑소하리의 이름을 뭐죠? 0110 그렇죠. 인력의 형태들에서 보자한 인력으로서 데이터들을 알아서 할 거에요. 그러면 x가 안 끌어진 거죠? 자 그리고 이어서 y를 만들어봅시다. y는 0, 1, 1, 0. 이라고 알아봅시다. 그래서 이렇게 정립할게요. 일용용이도 해서 데이터 준비. 데이터가 있기 때문에 이제 딱 승을 살펼 수 있겠죠. 이 날이면 데이터가 중요하잖아요. 자 그러면 이제 알아서 생각하죠. 이미 우리가 이혼적으로 알고 있어요. 우리는 x돌을 끌기 위해서는 단칭 레이어를 끌 수 있답니다. 맞아요. 없다는 의미가 없죠. 단칭 레이어를 끌어야 합니다. 그러면 우리가 예측을 도출하기 위해서라고 한다면 반드시 필요한 문준 거일까요? 예측을 하려면 뭐가 반드시 필요해요? 초기가 좀 지. 그렇죠. 초기가 좀 지. 자 그래서 이제는 원로 예측과 도출을 위한 예측과 필요한 거에서 도전도로 캐럭을 했습니다. 보기, 아줌치. 라는 걸 설정해볼 거에요. 그리고 이어서 두 번째로 예측을 만들겠다라고 하고 초기가 아줌치를 결정하려고 하는데 여기서 무조건 뭐가 있어야 아줌치를 결정합니다. 결과 예측값과 진실제값. 있잖아. 과... 예측에서 뽑으려고 뭐가 있어야 해요. 어 가죽기 있잖아요. 근데 가죽기 몇 개 만들었네. 인력 데이터만. 모드만큼. 왜 거기서 말해? 모드. 모델이 없는데 결과를 내주거나 말이 안 되겠죠. 자 그래서 이제 따뜻하게 할 거에요. 모델을 준비할 거에요. 모델의 평론. 우리는 레이어 측은 몇 개로 만들겠다 했어요. 출력 인력층, 히든층, 출력층. 최소족본으로 세 개에 대한 명의 이런 게 측면이 있어야 되겠죠. 탁세어로 뽑으면 두 개의 레이어 측은 맞는다. 나도 표현할 수 있겠죠. 그러면 유론의 기술 정기표죠. 첫 번째. 인력 유론의 기술입니다. 인력 유론의 기술은 몇 개를 줘야 될까요? 4개요. 4개 주는데요? 아 2개요. 2개 줘야 돼요. 왜 2개 줘요? true or false. 인력 유론의 기술? 비트가 2개라서. 맞아요. 2천 원인데요. 이게 중요해요. 자 여기서 모델의 형상에서 여러분들이 정할 수 없는 값을 낳죠. 모델의 형상은요. 정할 수 있는 값을 낳죠. 정할 수 있는 값을 낳죠. 정할 수 있는 값을 낳죠. 정할 수 있는 값을 낳죠. 정할 수 있는 값을 낳죠. 정할 수 있는 값을 낳죠. 정할 수 있는 값을 낳죠. 정할 수 있는 값을 낳죠. 정할 수 있는 값을 낳죠. 정할 수 있는 값을 낳죠. 정할 수 있는 값을 낳죠. 정할 수 있는 값을 낳죠. 정할 수 있는 값을 낳죠. 정할 수 있는 값을 낳죠. 이걸 어떻게 잘 보는데요. 정할 수 없도요. 그러면 내가 기초에 5개 자리해 주면 5개 정리하는 말이요. 그럴 수도 있겠지만 원론적으로 말하면 2일 통해서 결정되는데요. A. 지금 A에다가 몇 개의 기초를 가지고 있어요. 그렇죠. 그래서 입력 위로는. S는 입력 A. 그렇죠. 그 입력이 수만큼 남아요. 지금 입력이 있는데 X는 입력이 몇 개의 자리에요. 그렇죠. 그렇기 때문에 입력 위로는 반드시 2개예요. 이어서 두 번째. 출력 위로는. S는. S는. 하겟 데이트에 따라 결정. 그럼 지금 계단. Y이 몇 개에요? 4개에요. 4개지만 몇 개로 맞나요? 1개. 왜냐하면 이 스펀스. 하겟에 이 스펀스가 중요한 게 아니라. 하겟이 노나고 있는. 이렇게 수만큼으로 보았어요. 무슨 말인지 모르셨나요? 그런데 제가 이걸 하겟 데이트를 따라서 표현한 이유가 있어요. 나중에 율루라는 얘기할 때. 제가 방금 드릴 거예요. 지금은 우리가 결과로 내보내면. 답은 몇 개? 그렇죠. 답이 몇 개. 그래서 이렇게 변합니다. A4A. 답. 세다. A4B. 세다. 네. 그러면 여기서 물어볼게요. 우리가 출력할 위로는. 출력 위로는 몇 개에요? 그렇죠. 한 개에요. 그러면. 이거는. 위로는. 이렇게 출력과. 위로는. 위로는. 위로를 보내는 것. 어떤 차이점이 있을까요? 그래서 정리하면. 인력 위로를 예수하고. 출력 위로를 예수하고. 내가 정할 수 있다고. 없습니다. 그러면. 인력 위로를 예수하고. 출력 위로를 예수하고. 눈물을 참여서. 데이트를 따라서. 히든 위로를 예수하고. 히든 위로를 예수하고. 히든 위로를 예수하고. 누가 예수하고. 그러면 여러분께 물어볼게요. 제가 히든 레이어에. 위로를 예수하고 싶어요. 몇 기온 정도인가요? 이 자리 ppt에서. 내가 x2, x3. 두 개 두 개 있습니다. 두 개로 해볼까요? 또 하나요. 그러면. 두 개로서. 징해보도록 하겠습니다. 이게 바로. 일종의 뭘을 결정한 거다. 모델의 형상을 결정했다. 라고 말할 수 있겠죠. 그러면. 예측값을 준비합시다. 여기서 우리가 필요한 예측값은. 보호가 있어요. 예측값. 그게 가동심이에요. 어떤 가정이죠? 초기사 좋지. 왜? 이 별로 가는 가정이지. 라고 말해줄 수가 있겠죠. 이 어느 정도는 이해 되셨나요? 그런데. 조금 더 쉽게 이야기하면. 메이트와 바이였스를 결정해야 하는 거잖아요. 저는 별도 바이였스에 대한. 인사는 쉽게 빼지 않기. 그러니까. WS를 해준다. 이렇게 봐도 되겠죠. 그러니까. 그래서. W. W. 뭐라고 이야기하겠어요? 첫 번째 계칙에서 전달하는 것들에 대한. 웨이트들을 이야기할 건데. 이 웨이트가. 굉장히. 스탈라가스. 이를 통해서. 여기. 첫 번째 레이어에. 가지고 있어야. 웨이트의 개수는. 첫 번째 레이어에. 가지고 있어야. 웨이트의 개수. 첫 번째 레이어에. 가지고 있어야. 웨이트의 개수. 레게요. 왜 레겠죠? 입꼽하기. 뭐의 입꼽하기? 입력과 히든. 입력과 히든. 윤런의 입꼽이 있는지. 거기에 선을 안경을 안해서. 이렇게. 그래서. 보정교수를 만들어줄 거예요. 자. 넌터하게. 랜덤으로. 랜덤한. 보정교수를 만들었을 거예요. 그리고. 몇 개. 쉐입으로. 입력. 윤런의. 개수에서부터. 히든. 윤런의. 개수만큼. 끌어만들 거예요. 자. 먼저 말씀 드릴게요. 이. 랜덤 모듈의. 랜덤의. 값. 레소드는요. 뭘 만드냐면. 0에서부터. 1살 이 위에. 랜덤한 주자를 만들어요. 자. 여기서요. 제가 무슨 작업을 할까요? 제가. 랜덤한 주자를 만들어요. 랜덤한 주자를 만들어요. 랜덤한 주자를 만들어요. 자. 여기서요. 제가 무슨 작업을 할 거냐면. 곱하기. 이. 해. 를. 할 거예요. 그러면 이제. 보미가 어떻게 돼요? 0에서부터. 2살 이 위에. 그리고 여기에. 마이너스 1에. 해. 해. 해. 해. 해. 그럼 어떻게 되겠어요? 그쵸. 마이너스 1에서부터. 1살 이 위에. 보미가. 랜덤한. 가중치를 만들겠다. 라는 기회가 되겠죠. 자. 지금은. 이거와의 양념으로 설정할 거예요. 자. 그럼 두 번째. 두 번째 레이언은 어떻게 설정해야 할까요? 두 번째 레이언이 왜 해야 할까요? 이 곱하기 1. 그렇죠. 이 곱하기 1로서 만들어야겠죠. 그 2는 누구의 육아? 히든. 그렇죠. 히든 육아를 해야 할까요? 그리고 1을. 1은. 아웃. 훈련 육아를 해야 할까요? 어디서? 오추를 해야 할까요? 그럼 정리하려면. w1의 취임을 한 번. 그리고. w2의 취임을. 보시는 것 같은 것으로. 웨이트에 대한 형들이. 레트리스 위치상으로 구조가 맞게 만들어져 있는 거죠. 히든가 얘기하셨나요? 자. 그리고. 이번에는. x에 대한 형의 취임을. 여기서 뭔가의 극치성이 보이지 않아요? 음. 이걸 좀 더 이야기 쉽게. 제가 이런 표기를 할게요. 네. 네. 네. 네. 네. 네. 네. 네. 네. 네. 네. 네. 네. 네. 네. 네. 네. 네. 어떻게 해주세요? 이게. 내장이 가능하게끔 되어 있어요. 그래서. 중요한 기능입니다. 우리가 내장을 할 때. 내장이 성립하려면 어떻게 되어야 해요? 가운데 길이 맞아야 해요. 그래서. 그래서. 그래서. 그래서. 그래서. 그래서. 그래서. 그래서. 그래서. 그래서. 그래서. 그래서. 그래서. 그래서. 그래서. 이 부분 puree 이�cul 무대. 김 Даة's politique이�긴. slide 3raisie 와 네ready. 월거.ierte 아우ensor. 자. 보석dar to heong. descri 주야 소풍쟁이 hall Jewón Osis. Unfinter Rome sponsored by perseverance Hiper 마흐가 narcotine Essential Music 세상에. 오. oughtie band 그러면 첫 번째 레이어드의 입분, 볼소스의 대한동부는 뭐가 증가한 거예요? 입력증 두 개의 입청 X가 증가하자, 그는 입력증이라고 말해보세요 그리고 첫 번째 입분 X가 증가하자, 그는 입력증이라고 말해보세요 입력증에 들어와서 레이어드와 연상된 아웃북이 1번 개 측에 출력을 1번 개 측에 출력을 어떻게 연상해야 해요? 네, 그래서 입력과 레이어드의 입력과 첫 번째 가중치의 백터 연상에서 내장을 하죠 골뱅이 입력증을 하죠 그러면 이 일을 통해서 예정에 대한 분들이 만들어졌으니 벌과의 입표를 도출했겠죠 네 그러면 이제 이 유언에서 아웃북이 4번 개에 대한 입력이 뭐가 또 있어요? 액티베이션 액션 이 회식으로 우리가 화면에 만들어졌을 것입니다 이 회식으로 우리가 입력을 만들어보겠습니다 지금 보이드 자체도 있어요 액티베이션을 잊지 마, 우리가 액티베이션을 통해서 표현해서 더 먹으러 갈게요 그러면 액티베이션, 통션, 입력과 입력이 뭐가 있는지 궁금할 것 같아요 Z가 입력이 좀 좋을 거죠? 네 그리고 짝 1, 나는 1, plus mp, exp, L minus z 나는 것을 입력하면 여기에 두 동의 오소들이 나타내드리겠죠? 네, 여기 중요한 게 있죠 우리가 연산이라고 하면 뭘 생각해야 돼요? 우선 순위를 생각해야 돼요 맞나요? 그러면 운모로서 나타내드리기 때문에 이런 식으로 연산이 느껴져야 되겠죠? 이걸 우리는 뭘로? 지금 보이드 발소만 그런데 100% 파괴하시면 안 하면 모두 비군무이 해야 된대요 액티베이션, 통션도 비군무이 해야 되겠죠? 네 그래서 뭘 해야 될까요? 이 액티베이션, 통션을 의문한 값에 대하는 경우도 이야기할 거예요 액티베이션, 통션입니다 그런데 의문을 한 액티베이션, 통션을 이야기할 거예요 그래서 이걸 의문하면 그냥 수식이 정리되어 있어요 이걸 아주 수식 전기야를 하면 나오긴 한데 지금 그냥 따라 쓰는 걸로 이야기해 봅시다 자, 그래서 리턴 x, x1, 그리고 1, minus x 이게 지금 오이들의 의문 안단 x으로 의문을 하면 이 의문 결과까지 나와요 이게 도완수예요 그럼 이제 도완수 값이니까 이거를 모아요 지금 오이들의 한수인데 이거의 활성화 한수인데 이 활성화 한수, u이 뭐예요? 도완수 의문한 거예요 자, 그러면 이제 액티베이션, 통션이 만들었으니 이제 첫 번째 아웃북들에 맞이하는 경우도 있어야 하지만 우리가 결과가 도출할 수 있을 거잖아요 네 키워드가 예를 들었죠 자, 그럼 그대로 전달하겠습니다 자, z1에 대한 의문을 어디에 넣을 거예요? 액티베이션, 통션 그렇죠, 액티베이션, 통션에 넣어주면 나오는 아웃북 자체가 뭐예요? 0, 네 이게 일종의 z가 활성화 u, u 오늘 한 개 첫 번째 계층에서 기능을 안 했죠 이 계층으로 이제 동작을 하는 게 일참적으로 뭐가 만들어진 거예요 이 첫 번째 계층에 아웃북이 만들어진 거죠 그럼 이어서 이제 두 번째 이번 계층으로 들어갑니다 자, 이번 계층은 사실상 동작 쪽으로 뽑으면 이게 뭐예요? 은닉에서 어디로 가요? 술력으로 가는 계층입니다 우리가 레이어 2개만 해야 될 거에요 그럼 은닉 쪽에서 데이터들에 대한 의물로 동작을 하고 나타내는 게 있어야 되겠죠 그럼 전달 받은 거에 따라서 이 두 번째 계층에서 뭐예요? 하는 기회예요 z2 z2를 못 들어요 z2, 근데 핀력이 누구예요? h1 h1과 누구? w2 w2를 연상하는 그 말은 히든쯤 술력과 어? 술력 술력 가든취 예정 을 하게 되었네요 여기서도 똑같이 에트레이션 펑셔를 거치게 되죠 그러면 여기서 에트레이션 펑션은 어떤 목적이에요? 여기에는 결과 중요하다는 결과 술력을 미한 그래서 얘는 활성화 안수가 많지만 술력은 활성화 용어적으로 뭐라고 했어요? 에트레이션 펑셔를 합니다 근데 아웃포트를 뽑아내는 에트레이션 펑션에서 술력 안수로 표현을 한다는 거죠? 네 그러면 얘는 뭐라고도 있어요 아웃포트 이라고 알고도 있겠죠? 열 L 아웃 에트레이션 펑션을 인형을 L1이라고 표현했습니다 1,2에서 레이어 침에 대한 용들을 동작을 해서 결과를 도출해 준 거죠 어떻게 해주면 되죠? w2입니다 그래야지 결과를 하시면 되죠 정리하자면 술력 전 나는 것으로 동작하는 형태리죠 피오는 너무 좋아해지셨나요? 여기까지는? 네 그럼 정답 잘 맞추고 있는 거예요? 아니요 못 맞추는 게 당연한 거죠 왜요? 조기가 중치라서 이 연사는 다시는 든든 파 연사인데 그런데 못 든든 파 연사는 초기 정답이 아닙니다 그럼 이제 중점 파 연사 했으니 얘의 결과는 사실 어떤 탑시라고 해요? 저랑 숫자가 달라도 잠깐 앞서 다른 게 당연한 거예요 웬더로 돌렸어요 예측값을 뽑았으니 이제 뭘 이렇게 상할 수 있어요? 이제 오차를 대상할 수 있겠죠? 이제 오차를 대상할 수 있습니까? 어떤 오차를 대상하는 거예요? 이런 건요 출력에 대한 영향의 에러 값들을 예상한다 하고 출력 에러는 어떻게 예상한데요? 오차 값으로 노출해주면 되는 겁니다 그래서 출력을 하겠습니다 일부러 연산에 대한 영향들을 좀 편성으로 하기 위해서 출력 값 여기에 칫각세상 가일을 보여주면 할게요 에, 언더바, 아웃 세기, 와이라고 친하겠습니다 이게 어떤 값을 도출한 거예요? 출력 오차를 도출한 거죠 여기 출력 오차에 대한 영율로 여기에 대한 영유를 수렴하겠다 나오고 싶은 게 되겠죠? 언론 쪽으로는 이게 맞아요 에, 언더바, 오인, 이가 맞아요 이 두 마리 늦게 졌어요 왜냐하면 와, 공포력이 아닌데 그러면 무슨 일 없죠? 뒤에 다 들었죠? 그러면 이쪽으로 만든 거겠죠? 어차피 양쪽은 차이가 없어요 그런데 우리가 이야기 시키세요 이거로 정리해서 그랬습니다 오차가 만들어졌으니 이제부터는 뭘 해야 되죠? 오차가 만들어졌으니 이걸 가지고 이제 역전 파일 첫 번째 이야기 였죠? 우리가 이야기는 도출 된 이 오차는 출력 오차를 이야기하는 거죠 그렇게 도출 된 출력 오차 값이 위험해서 어떻게 해야 되죠? 출력 오차를 전달해야 되죠 출력 오차를 전달로써 어디로 줄 거냐면 바로 출력단을 줘야 되죠 그러면 우리는 이 출력단에서 나타내는 뭘을 도출해야 된다? 오차를 계산 오차를 도출해서 미문해서 각질 전달해야 되겠죠? 네 그럼 예측 값에 대기해서 전달해야 되는데 생각을 해볼까요? 출력단을 아웃트둘이 뭐예요? 출력단을 아웃트둘이 액티네이션의 Z 값이잖아요 액티네이션의 H1 값이잖아요 이걸 역으로 액티네이션을 뭐해야 돼요? 역함수들 그렇죠 역함수들 비록 2분에 해야 되죠? 네 역함수들 자리 바꾸는 거 같으면 받는 거 아니니까 2분에 해야 되겠죠? 2분간에 전해서 2분간에 전해서 2분간에 전해서 그러면 출력에 대한 여러분들 출력침 아웃층의 레이어에서 가는 2분 값 액티네이션 펑션인데 2분으로 그리고 이때 입력되는 부분이라면 바로 출력값 아웃 레이어드 이게 뭐예요? 레이어드 5 레이어드 5 레이어드 5 레이어드 5 레이어드 5 이게 이제 역으로 아웃둘 들어온 거에 뭘 지난 거예요? 액티네이션 펑션 역으로 지난 거잖아요 그럼 뭐가 좋지? Z2가 좋지 않겠지? 거기에 뭘 같이 줘야 돼 오차 오차가 역전판인 거잖아요 여기에 출력층에 대한 이 공값들로 거기에 복값이 모분값 출력 오차 값 이게 출력 오차 값을 전달을 놓으세요 두 번째 이제 뭘 놓으려면 히든 오차를 놓으세요 히든 측에 오차 히든 측에 오차는 이 여부로 전달될 값을 기반해서 연산해야 되겠죠? 근데 아까 말한 것처럼 이 역으로 전달될 값이 이쪽으로 말했어요 입력이라고 했잖아요 그 입력이랑 누구라는 건가요? 웨이트 그래야 X라고 합니다 Z를 웨이트로 역으로 연산해야지 X가 되잖아요 그러면 뛰어볼게요 행렬은 중요한 게 여기 자리가 중요해요 X를 넣어서 웨이트라고 내장했잖아요 웨이트가 어디 있어요? 뒤에 있고 근데 이걸 거꾸로 오는 거잖아요 그럼 입력을 어디에 당하는 거예요? 빛도 안에서 당하는 거예요 앞에서 들어온 게 아니라 방향을 두 번 해야 돼요 앞으로 생각하시자 히든 오차는 약속 들어오는 입력이 누구예요? 히든 오차가 있잖아요 입력을 출력 오차에 전달 값 이 출력 오차에 전달 값을 웨이트로 받아야 되는 거예요 그래서 똑같은 논출로 받아주는 거에 값을 연달해 줘야 되는 거예요 그래서 입값을 다시는 예정할 거예요 누구로? 웨이트로 근데 이때의 웨이트는 누구를 말하는 거예요? 이때의 웨이트는 투어의 웨이트 그래서 투어의 웨이트 근데 이걸 조금 설명하기 위해서 제가 한 가지에 가만 더 추려서 이야기해 올게요 실행을 하면 아웃풋에 대한 연결이 만들어지겠죠 이 아웃풋을 쉐입을 확인되면 얘가 어떻게 되어있냐면 입력과 대비 4콘 마이 2돼요 4콘 마이 1 그리고 웨이트2를 볼게요 웨이트2에 쉐입을 확인되면 이게 어떻게 되었어요? 2콘 마이 2 2콘 마이 2 그러면 내가 입력 대비 웨이트를 거쳐 나가는 걸로 내장을 하고 싶다라면 휴충이 맞아요? 틀려요 안 맞아요 그럼 뭘로 할게요? 트랜스 포장 정환을 해야 돼요 그래서 누구를 정환하느냐면 웨이트를 전치해 볼 거예요 그래서 점 t라고 전환을 할 거예요 여기서 점 t 전환을 주면 이제 내부 부족을 맡겨요 그래서 이 형태들로 히든 오차를 도출할 수 있어요 여기 연산의 결과 값이 곧 뭐다? 히든 오차라는 그다음에 내용은 똑같아요 이번에는 이제 누구를 전달해야 돼요? 히든 오차를 전달해야 되겠죠? 그러면 또까지 도왕수에 누구를 뽑할 거예요 히든 에러를 뽑아주고요 이때 전달자를 틀려보고 누구야 이 히든 레이어의 아웃푸 이라운드는 h1 값에 대한 용들을 전달을 받아야 돼요 액티베이션 펑셔널에 대한 아웃푸입니다 그래서 액티베이션 펑셔널에 대한 용의 아웃푸들 역함으로 역으로 도왕수랑 연결해서 오차들을 전달해 주는 거예요 그러면 이게 누구의 히든 레이어의 미분값이 되고요 그럼 정리하자면 지금 오차가 몇 개가 도출됐어요 미분값이 두 개가 도출됐죠 이 두 개로 웨이트를 경신해 주는 거예요 그러면 w2와 w1을 못 칠 거예요 minus equal minus equal 값으로 전달을 받을 거예요 인정으로서는 런이베이트 값을 0.25서 설정할게요 거기에 꼭 한 미분값이 전달해야 되는 거잖아요 맞나요? 그럼 그 미분값을 위해서 이제 미분값에 내려내려야 하는 걸로 연산을 해 줄 거예요 앞서 히든 오차에 대한 용도 있었잖아요 그리고 여기에 h1이라는 값을 전치해서 내장해 줄 거예요 그러면 옆으로 똑같은 패턴으로 경신할 거예요 그 대신 여기서 달라지는 거는 이제 아웃컴 패턴에 대한 용도 있죠 이 히든에 대한 미분값과 전달 받는 인력의 고통당 레이어드 인력이요? 레이어드 인력 이로써 전달해 주면 이게 웨이트의 경신을 이야기할 거예요 이건 정상한 거 그럼 이 구조들로 경신을 주고 개로박값을 대한 용도를 웨이트로 고쳐 준다 나는 걸로 이제 파트드를 쪼개고 자, 이번 시간을 여기서 제가 중간에 들었고 제가 마지막 시간을 여러분들 안 뺏겠다고 하기 때문에 조금 시간이 오버려야 했다는 양이 부탁드리고요 그래서 여기까지 해서 구성이 만들어졌을 때 내일을 보면 우리가 이걸 통하고 묶어서 돌릴 수 있는 구조들을 한 번 해볼 것 같아요 이걸 통해서 이제 오차였던 파에 레이어치인에 대한 경련들을 가볍게 하는 지폭을 치간들이 없다고 보시면 되겠습니다 여러분들 뒤졌다가 마지막 시간 진행한 시간 또 오시면 되겠습니다 다른은 바로 보여드리도록 하겠습니다\n"
     ]
    }
   ],
   "source": [
    "import whisper\n",
    "\n",
    "model = whisper.load_model(\"small\", device=\"cuda\")  # << GPU 사용\n",
    "result = model.transcribe(\"/content/4월 29일 오후.m4a\")  # 파일 경로 맞춰줘야 해\n",
    "print(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sS9Xqyu3Trpq"
   },
   "outputs": [],
   "source": [
    "with open(\"/content/result.txt\", \"w\") as f:\n",
    "    f.write(result[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wfFIwjWVTvCB"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
